{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":301510,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":257504,"modelId":278802},{"sourceId":304328,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":259730,"modelId":280904}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-24T08:40:35.444251Z","iopub.execute_input":"2025-03-24T08:40:35.444572Z","iopub.status.idle":"2025-03-24T08:40:35.449371Z","shell.execute_reply.started":"2025-03-24T08:40:35.444550Z","shell.execute_reply":"2025-03-24T08:40:35.448337Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport math\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\nclass GPTConfig:\n    block_size: int = 1024 # max sequence length\n    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n    n_layer: int = 12 # number of layers\n    n_head: int = 12 # number of heads\n    n_embd: int = 768 # embedding dimension\n\n\nclass GPT(nn.Module):\n    def __init__(self, config):\n        self.config = config\n\n        self.transformer = nn.ModuleDict(dict(\n                wte = nn.Embedding(config.vocab_size, config.n_embd),\n                wpe = nn.Embedding(config.block_size, config.n_embd),\n                h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n                ln_f = nn.LayerNorm(config.n_embd),\n            ))\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            std = 0.02\n            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n                std *= (2 * self.config.n_layer) ** -0.5\n            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\nclass MLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n        self.gelu    = nn.GELU(approximate='tanh')\n        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n        self.c_proj.NANOGPT_SCALE_INIT = 1\n\n    def forward(self):\n        x = self.c_fc(x)\n        x = self.gelu(x)\n        x = self.c_proj(x)\n        return x\n\nclass Block(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.linear_1 = nn.LayerNorm(config.n_embed)\n        self.attn = CausalSelfAttention(config)\n        self.linear_2 = nn.LayerNorm(config.n_embd)\n        self.mlp = MLP(config)\n\n    def forward(self, x):\n        x = x + self.attn(self.linear_1(x))\n        x = x + self.mlp(self.linear_2(x))\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T08:40:35.457526Z","iopub.execute_input":"2025-03-24T08:40:35.457749Z","iopub.status.idle":"2025-03-24T08:40:35.468197Z","shell.execute_reply.started":"2025-03-24T08:40:35.457731Z","shell.execute_reply":"2025-03-24T08:40:35.467532Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip install tiktoken\nimport tiktoken\ntiktoken.__version__","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T08:40:35.485237Z","iopub.execute_input":"2025-03-24T08:40:35.485423Z","iopub.status.idle":"2025-03-24T08:40:35.490048Z","shell.execute_reply.started":"2025-03-24T08:40:35.485406Z","shell.execute_reply":"2025-03-24T08:40:35.489428Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# https://tiktokenizer.vercel.app/?model=codellama%2FCodeLlama-7b-hf\n\ntokenizer = tiktoken.get_encoding(\"cl100k_base\")\n\ntext = (\"''' below is the python code for sum of two number''' def add_two(a, b):\\n    return a+b\")\ntokenizer.n_vocab","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T08:40:35.503185Z","iopub.execute_input":"2025-03-24T08:40:35.503439Z","iopub.status.idle":"2025-03-24T08:40:36.296227Z","shell.execute_reply.started":"2025-03-24T08:40:35.503418Z","shell.execute_reply":"2025-03-24T08:40:36.295414Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer.n_vocab","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T08:40:36.297372Z","iopub.execute_input":"2025-03-24T08:40:36.297729Z","iopub.status.idle":"2025-03-24T08:40:36.302846Z","shell.execute_reply.started":"2025-03-24T08:40:36.297704Z","shell.execute_reply":"2025-03-24T08:40:36.301913Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer.decode([19317,\n 3770,\n 374,\n 279,\n 10344,\n 2082,\n 369,\n 2694,\n 315,\n 1403,\n 1396,\n 19317,\n 711,\n 923,\n 24120,\n 2948,\n 11,\n 293,\n 997,\n 262,\n 471,\n 264,\n 36193])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T08:40:36.304664Z","iopub.execute_input":"2025-03-24T08:40:36.304912Z","iopub.status.idle":"2025-03-24T08:40:36.318926Z","shell.execute_reply.started":"2025-03-24T08:40:36.304892Z","shell.execute_reply":"2025-03-24T08:40:36.318144Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# embeddings\nimport torch\nimport torch.nn as nn\n\nvocab_size = 10\noutput_dim = 3 # 256\n\ntorch.manual_seed(23)\nembedding_layer = torch.nn.Embedding(vocab_size, output_dim)\nembedding_layer.weight","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T14:33:13.184554Z","iopub.status.idle":"2025-03-25T14:33:13.184933Z","shell.execute_reply":"2025-03-25T14:33:13.184779Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"nn.Linear(10,3).weight","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T08:40:36.336390Z","iopub.execute_input":"2025-03-24T08:40:36.336718Z","iopub.status.idle":"2025-03-24T08:40:36.345111Z","shell.execute_reply.started":"2025-03-24T08:40:36.336690Z","shell.execute_reply":"2025-03-24T08:40:36.344257Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"nn.Linear(10,3).weight","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T08:40:36.345972Z","iopub.execute_input":"2025-03-24T08:40:36.346210Z","iopub.status.idle":"2025-03-24T08:40:36.359296Z","shell.execute_reply.started":"2025-03-24T08:40:36.346192Z","shell.execute_reply":"2025-03-24T08:40:36.358460Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pos_embedding_layer = torch.nn.Embedding(context_length, output_dim) # (512, )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T08:40:36.360168Z","iopub.execute_input":"2025-03-24T08:40:36.360424Z","iopub.status.idle":"2025-03-24T08:40:36.370159Z","shell.execute_reply.started":"2025-03-24T08:40:36.360405Z","shell.execute_reply":"2025-03-24T08:40:36.369294Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\ninputs = torch.Tensor([\n    [1,2,1],\n    [2,1,2],\n    [2,1,2]\n \n])\n\nx_2 = inputs[1]\nd_in = inputs.shape[1]\nd_out = 6","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T08:40:36.372455Z","iopub.execute_input":"2025-03-24T08:40:36.372738Z","iopub.status.idle":"2025-03-24T08:40:36.382289Z","shell.execute_reply.started":"2025-03-24T08:40:36.372717Z","shell.execute_reply":"2025-03-24T08:40:36.381686Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inputs.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T08:40:36.383979Z","iopub.execute_input":"2025-03-24T08:40:36.384279Z","iopub.status.idle":"2025-03-24T08:40:36.397199Z","shell.execute_reply.started":"2025-03-24T08:40:36.384256Z","shell.execute_reply":"2025-03-24T08:40:36.396418Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.manual_seed(123)\n\nWq = torch.nn.Parameter(torch.rand(d_in, d_out))\nWk = torch.nn.Parameter(torch.rand(d_in, d_out))\nWv = torch.nn.Parameter(torch.rand(d_in, d_out))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T08:40:36.397982Z","iopub.execute_input":"2025-03-24T08:40:36.398244Z","iopub.status.idle":"2025-03-24T08:40:36.410926Z","shell.execute_reply.started":"2025-03-24T08:40:36.398203Z","shell.execute_reply":"2025-03-24T08:40:36.410113Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"keys = inputs @ Wk\nquery = inputs @ Wq\n\nkeys, query","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T08:40:36.411837Z","iopub.execute_input":"2025-03-24T08:40:36.412119Z","iopub.status.idle":"2025-03-24T08:40:36.424632Z","shell.execute_reply.started":"2025-03-24T08:40:36.412092Z","shell.execute_reply":"2025-03-24T08:40:36.423721Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"d_k = keys.shape[1]\nattention_scores = query @ keys.T\nattention_scores_norm = torch.softmax(attention_scores / d_k**0.5, dim=-1)\nattention_scores_norm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T08:40:36.425345Z","iopub.execute_input":"2025-03-24T08:40:36.425543Z","iopub.status.idle":"2025-03-24T08:40:36.439392Z","shell.execute_reply.started":"2025-03-24T08:40:36.425520Z","shell.execute_reply":"2025-03-24T08:40:36.438439Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"query","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T08:40:36.440130Z","iopub.execute_input":"2025-03-24T08:40:36.440366Z","iopub.status.idle":"2025-03-24T08:40:36.449592Z","shell.execute_reply.started":"2025-03-24T08:40:36.440348Z","shell.execute_reply":"2025-03-24T08:40:36.448604Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nclass SelfAttention(nn.Module):\n    def __init__(self, d_in, d_out, d_k):\n        super().__init__()\n        # self.Wq = torch.nn.Parameter(torch.rand(d_in, d_out))\n        # self.Wk = torch.nn.Paramter(torch.rand(d_in, d_out))\n        # self.Wv = torch.nn.Paramter(torch.rand(d_in, d_out))\n\n        self.Wq = torch.nn.Linear(torch.rand(d_in, d_out, bias = False))\n        self.Wk = torch.nn.Linear(torch.rand(d_in, d_out, bias = False))\n        self.Wv = torch.nn.Linear(torch.rand(d_in, d_out, bias = False))\n\n    def forward(self, x):\n        key = inputs @ Wk\n        query = inputs @ Wq\n        value = inputs @ Wv\n\n        attention_scores = query @ key.T\n        attention_scores_norm = torch.softmax(attention_scores / d_k**0.5, dim=-1)\n\n        context_vector = attention_scores_norm @ value\n\n        return context_vector\n\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T08:40:36.450508Z","iopub.execute_input":"2025-03-24T08:40:36.450787Z","iopub.status.idle":"2025-03-24T08:40:36.462154Z","shell.execute_reply.started":"2025-03-24T08:40:36.450758Z","shell.execute_reply":"2025-03-24T08:40:36.461323Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nclass CasualSelfAttention(nn.Module):\n    def __init__(self, d_in, d_out, d_k, context_length):\n        super().__init__()\n\n\n        self.Wq = torch.nn.Linear(torch.rand(d_in, d_out, bias = False))\n        self.Wk = torch.nn.Linear(torch.rand(d_in, d_out, bias = False))\n        self.Wv = torch.nn.Linear(torch.rand(d_in, d_out, bias = False))\n\n    def forward(self, x):\n        \n        key = inputs @ Wk\n        query = inputs @ Wq\n        value = inputs @ Wv\n\n        attention_scores = query @ key.T\n\n        mask = torch.tril(torch.ones(context_length, context_length))\n        mask_attention = attention_scores.masked_fill(mask.bool(), -torch.inf)\n        attention_scores_norm = torch.softmax(mask_attention / d_k**0.5, dim=-1)\n\n        context_vector = attention_scores_norm @ value\n        return context_vector\n\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T08:40:36.462874Z","iopub.execute_input":"2025-03-24T08:40:36.463096Z","iopub.status.idle":"2025-03-24T08:40:36.478578Z","shell.execute_reply.started":"2025-03-24T08:40:36.463078Z","shell.execute_reply":"2025-03-24T08:40:36.477778Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass CasualAttention(nn.Module):\n    def __init__(self, d_in, d_out, dropout, context_length):\n        super().__init__()\n\n\n        self.Wq = torch.nn.Linear(d_in, d_out, bias = False)\n        self.Wk = torch.nn.Linear(d_in, d_out, bias = False)\n        self.Wv = torch.nn.Linear(d_in, d_out, bias = False)\n        self.dropout = torch.nn.Dropout(dropout)\n        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n        \n    def forward(self, x):\n        B, num_tokens, d_int = x.shape\n        key = self.Wk(x)\n        query = self.Wq(x)\n        value = self.Wv(x)\n\n        attention_scores = query @ key.transpose(1, 2)\n\n        # mask = torch.tril(torch.ones(context_length, context_length))\n        mask_attention = attention_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n        attention_scores_norm = torch.softmax(mask_attention / (d_out)**0.5, dim=-1)\n\n        attention_scores_dropout = self.dropout(attention_scores_norm)\n\n        context_vector = attention_scores_dropout @ value\n        return context_vector\n\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T14:33:13.185777Z","iopub.status.idle":"2025-03-25T14:33:13.186139Z","shell.execute_reply":"2025-03-25T14:33:13.185980Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import torch\ntorch.manual_seed(123)\ninputs = torch.Tensor([\n    [1,2,1],\n    [2,1,2],\n    [2,1,2],\n    [1,2,1],\n    [2,1,2],\n    [2,1,2]\n \n \n])\nbatch = torch.stack((inputs, inputs), dim=0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T08:40:36.494324Z","iopub.execute_input":"2025-03-24T08:40:36.494545Z","iopub.status.idle":"2025-03-24T08:40:36.509669Z","shell.execute_reply.started":"2025-03-24T08:40:36.494510Z","shell.execute_reply":"2025-03-24T08:40:36.508897Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# context_length = batch.shape[1]\n# dropout = 0.3\n# d_in = inputs.shape[1]\n# d_out = 6\n# d_k = d_in\n# ca = CasualAttention(d_in, d_out, dropout, context_length)\n# ca(batch)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T08:40:36.510471Z","iopub.execute_input":"2025-03-24T08:40:36.510776Z","iopub.status.idle":"2025-03-24T08:40:36.522503Z","shell.execute_reply.started":"2025-03-24T08:40:36.510754Z","shell.execute_reply":"2025-03-24T08:40:36.521692Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, d_in, d_out, dropout, context_length, num_heads):\n        super().__init__()\n        self.heads = nn.ModuleList([CasualAttention(d_in, d_out, dropout, context_length) for _ in range(num_heads)])\n\n    def forward(self, x):\n        return torch.concat([head(x) for head in self.heads])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T08:40:36.523363Z","iopub.execute_input":"2025-03-24T08:40:36.523574Z","iopub.status.idle":"2025-03-24T08:40:36.534432Z","shell.execute_reply.started":"2025-03-24T08:40:36.523548Z","shell.execute_reply":"2025-03-24T08:40:36.533818Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **GPT2 Execution**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\ntorch.manual_seed(123)\ninputs = torch.Tensor([\n    [1,2,1],\n    [2,1,2],\n    [2,1,2],\n    [1,2,1],\n    [2,1,2],\n    [2,1,2]\n])\nbatch = torch.stack((inputs, inputs), dim=0)\n# batch = torch.Tensor(inputs.reshape(-1))\ncontext_length = batch.shape[1]\ndropout = 0.3\nd_in = inputs.shape[1]\nd_out = 6\nd_k = d_in\nnum_heads = 4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T08:04:19.222851Z","iopub.execute_input":"2025-03-30T08:04:19.223270Z","iopub.status.idle":"2025-03-30T08:04:20.891367Z","shell.execute_reply.started":"2025-03-30T08:04:19.223231Z","shell.execute_reply":"2025-03-30T08:04:20.890520Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# batch.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T08:04:22.965661Z","iopub.execute_input":"2025-03-30T08:04:22.966070Z","iopub.status.idle":"2025-03-30T08:04:22.969445Z","shell.execute_reply.started":"2025-03-30T08:04:22.966042Z","shell.execute_reply":"2025-03-30T08:04:22.968595Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# torch.manual_seed(123)\n# mha = MultiHeadAttention(d_in, d_out, dropout, context_length, num_heads)\n# mha_out = mha(batch[:1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T08:04:23.157763Z","iopub.execute_input":"2025-03-30T08:04:23.158035Z","iopub.status.idle":"2025-03-30T08:04:23.161221Z","shell.execute_reply.started":"2025-03-30T08:04:23.158015Z","shell.execute_reply":"2025-03-30T08:04:23.160319Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# mha_out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T08:04:05.389174Z","iopub.status.idle":"2025-03-30T08:04:05.389551Z","shell.execute_reply":"2025-03-30T08:04:05.389376Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MultiHeadCasualAttention(nn.Module):\n    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n        super().__init__()\n        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n\n        self.d_out = d_out\n        self.num_heads = num_heads\n        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n\n    def forward(self, x):\n        b, num_tokens, d_in = x.shape\n\n        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n        queries = self.W_query(x)\n        values = self.W_value(x)\n\n        # We implicitly split the matrix by adding a `num_heads` dimension\n        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n\n        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n        keys = keys.transpose(1, 2)\n        queries = queries.transpose(1, 2)\n        values = values.transpose(1, 2)\n\n        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n\n        # Original mask truncated to the number of tokens and converted to boolean\n        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n\n        # Use the mask to fill attention scores\n        attn_scores.masked_fill_(mask_bool, -torch.inf)\n\n        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n\n        # Shape: (b, num_tokens, num_heads, head_dim)\n        context_vec = (attn_weights @ values).transpose(1, 2)\n\n        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n        context_vec = self.out_proj(context_vec)  # optional projection\n\n        return context_vec\n\n\n# mha_ch03 = MultiHeadCasualAttention(\n#     d_in=3,\n#     d_out=24,\n#     context_length=12,\n#     dropout=0.0,\n#     num_heads=12,\n#     qkv_bias=False\n# ).to(\"cpu\")\n\n# out = mha_ch03(batch[:1])\n# print(out.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T08:04:25.622728Z","iopub.execute_input":"2025-03-30T08:04:25.623043Z","iopub.status.idle":"2025-03-30T08:04:25.630891Z","shell.execute_reply.started":"2025-03-30T08:04:25.623017Z","shell.execute_reply":"2025-03-30T08:04:25.630077Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"GPT_CONFIG_124M = {\n    \"vocab_size\": 50257,    # Vocabulary size\n    \"context_length\": 512, # Context length\n    \"emb_dim\": 768,         # Embedding dimension\n    \"n_heads\": 12,          # Number of attention heads\n    \"n_layers\": 12,         # Number of layers\n    \"drop_rate\": 0.1,       # Dropout rate\n    \"qkv_bias\": False       # Query-Key-Value bias\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T08:04:28.530523Z","iopub.execute_input":"2025-03-30T08:04:28.530858Z","iopub.status.idle":"2025-03-30T08:04:28.535129Z","shell.execute_reply.started":"2025-03-30T08:04:28.530830Z","shell.execute_reply":"2025-03-30T08:04:28.534148Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class GELU(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return 0.5 * x * (1 + torch.tanh(\n            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n            (x + 0.044715 * torch.pow(x, 3))\n        ))\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n            GELU(),\n            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.attention = MultiHeadCasualAttention(\n            d_in= cfg[\"emb_dim\"],\n            d_out=cfg[\"emb_dim\"],\n            context_length=cfg[\"context_length\"],\n            dropout=cfg[\"drop_rate\"],\n            num_heads=cfg[\"n_heads\"],\n            qkv_bias=cfg[\"qkv_bias\"]\n        )\n\n        self.ff = FeedForward(cfg)\n        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n\n    def forward(self, x):\n        # This block does nothing and just returns its input.\n        shortcut = x\n        x = self.norm1(x)\n        x = self.attention(x)\n        x = self.drop_shortcut(x)\n\n        x = x + shortcut\n\n        shortcut = x\n\n        x = self.norm2(x)\n        x = self.ff(x)\n        x = self.drop_shortcut(x)\n        x = x + shortcut\n        return x\n\n\nclass LayerNorm(nn.Module):\n    def __init__(self, emb_dim, eps=1e-5):\n        super().__init__()\n        self.eps = eps\n        self.scale = nn.Parameter(torch.ones(emb_dim))\n        self.shift = nn.Parameter(torch.zeros(emb_dim))\n        \n\n    def forward(self, x):\n        # This layer does nothing and just returns its input.\n\n        mean = x.mean(dim = -1, keepdim = True)\n        var = x.var(dim=-1,keepdim = True, unbiased = False)\n        norm_x = (x-mean)/torch.sqrt(var+self.eps)\n        return self.scale * norm_x + self.shift","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T08:04:30.966743Z","iopub.execute_input":"2025-03-30T08:04:30.967018Z","iopub.status.idle":"2025-03-30T08:04:30.976648Z","shell.execute_reply.started":"2025-03-30T08:04:30.966999Z","shell.execute_reply":"2025-03-30T08:04:30.975505Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"torch.manual_seed(123)\n\nx = torch.rand(2, 4, 768)  # Shape: [batch_size, num_tokens, emb_dim]\nblock = TransformerBlock(GPT_CONFIG_124M)\noutput = block(x)\n\nprint(\"Input shape:\", x.shape)\nprint(\"Output shape:\", output.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T08:04:34.454653Z","iopub.execute_input":"2025-03-30T08:04:34.454938Z","iopub.status.idle":"2025-03-30T08:04:34.679968Z","shell.execute_reply.started":"2025-03-30T08:04:34.454919Z","shell.execute_reply":"2025-03-30T08:04:34.679099Z"}},"outputs":[{"name":"stdout","text":"Input shape: torch.Size([2, 4, 768])\nOutput shape: torch.Size([2, 4, 768])\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"class GPTModel(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n\n        self.trf_blocks = nn.Sequential(\n            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n        self.out_head = nn.Linear(\n            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n        )\n\n    def forward(self, in_idx):\n        \n        batch_size, seq_len = in_idx.shape\n        tok_embeds = self.tok_emb(in_idx)\n        pos_embeds = self.pos_emb(torch.arange(seq_len, device = in_idx.device))\n        x = tok_embeds + pos_embeds\n        x = self.drop_emb(x)\n        x = self.trf_blocks(x)\n        x = self.final_norm(x)\n\n        logits = self.out_head(x)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T08:04:36.526022Z","iopub.execute_input":"2025-03-30T08:04:36.526310Z","iopub.status.idle":"2025-03-30T08:04:36.532147Z","shell.execute_reply.started":"2025-03-30T08:04:36.526279Z","shell.execute_reply":"2025-03-30T08:04:36.531256Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import tiktoken\n\ntokenizer = tiktoken.get_encoding(\"gpt2\")\n\nbatch = []\n\ntxt1 = \"Every effort moves you\"\ntxt2 = \"Every day holds a\"\n\nbatch.append(torch.tensor(tokenizer.encode(txt1)))\nbatch.append(torch.tensor(tokenizer.encode(txt2)))\nbatch = torch.stack(batch, dim=0)\nprint(batch)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T07:06:55.681445Z","iopub.execute_input":"2025-03-25T07:06:55.681726Z","iopub.status.idle":"2025-03-25T07:06:57.316385Z","shell.execute_reply.started":"2025-03-25T07:06:55.681703Z","shell.execute_reply":"2025-03-25T07:06:57.315658Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# torch.manual_seed(123)\n# model = GPTModel(GPT_CONFIG_124M)\n\n# out = model(batch)\n# print(\"Input batch:\\n\", batch)\n# print(\"\\nOutput shape:\", out.shape)\n# print(out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T08:44:22.209701Z","iopub.execute_input":"2025-03-24T08:44:22.209983Z","iopub.status.idle":"2025-03-24T08:44:22.213299Z","shell.execute_reply.started":"2025-03-24T08:44:22.209961Z","shell.execute_reply":"2025-03-24T08:44:22.212454Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# total_params = sum(p.numel() for p in model.parameters())\n# print(f\"Total number of parameters: {total_params:,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T08:44:22.430926Z","iopub.execute_input":"2025-03-24T08:44:22.431232Z","iopub.status.idle":"2025-03-24T08:44:22.434342Z","shell.execute_reply.started":"2025-03-24T08:44:22.431206Z","shell.execute_reply":"2025-03-24T08:44:22.433500Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tiktoken\n\ntokenizer = tiktoken.get_encoding(\"gpt2\")\n\nbatch = []\n\ntxt1 = \"Every effort moves you\"\ntxt2 = \"Every day holds a\"\n\nbatch.append(torch.tensor(tokenizer.encode(txt1)))\nbatch.append(torch.tensor(tokenizer.encode(txt2)))\nbatch = torch.stack(batch, dim=0)\nprint(batch)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T07:07:00.572799Z","iopub.execute_input":"2025-03-25T07:07:00.573132Z","iopub.status.idle":"2025-03-25T07:07:00.579115Z","shell.execute_reply.started":"2025-03-25T07:07:00.573103Z","shell.execute_reply":"2025-03-25T07:07:00.578222Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.manual_seed(123)\nmodel = GPTModel(GPT_CONFIG_124M)\n\nlogits = model(batch)\nprint(\"Output shape:\", logits.shape)\nprint(logits)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T16:40:07.215178Z","iopub.status.idle":"2025-03-29T16:40:07.215553Z","shell.execute_reply":"2025-03-29T16:40:07.215398Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"logits.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T07:07:02.243866Z","iopub.execute_input":"2025-03-25T07:07:02.244138Z","iopub.status.idle":"2025-03-25T07:07:02.249688Z","shell.execute_reply.started":"2025-03-25T07:07:02.244116Z","shell.execute_reply":"2025-03-25T07:07:02.248708Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_text_simple(model, idx, max_new_tokens, context_size):\n    # idx is (batch, n_tokens) array of indices in the current context\n    for _ in range(max_new_tokens):\n        \n        # Crop current context if it exceeds the supported context size\n        # E.g., if LLM supports only 5 tokens, and the context size is 10\n        # then only the last 5 tokens are used as context\n        idx_cond = idx[:, -context_size:]\n        \n        # Get the predictions\n        with torch.no_grad():\n            logits = model(idx_cond)\n        \n        # Focus only on the last time step\n        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n        logits = logits[:, -1, :]  \n\n        # Apply softmax to get probabilities\n        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n\n        # Get the idx of the vocab entry with the highest probability value\n        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n\n        # Append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n\n    return idx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T14:34:16.384543Z","iopub.execute_input":"2025-03-25T14:34:16.384862Z","iopub.status.idle":"2025-03-25T14:34:16.389824Z","shell.execute_reply.started":"2025-03-25T14:34:16.384835Z","shell.execute_reply":"2025-03-25T14:34:16.389021Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"start_context = \"Hello, I am\"\n\nencoded = tokenizer.encode(start_context)\nprint(\"encoded:\", encoded)\n\nencoded_tensor = torch.tensor(encoded).unsqueeze(0)\nprint(\"encoded_tensor.shape:\", encoded_tensor.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T07:07:03.652021Z","iopub.execute_input":"2025-03-25T07:07:03.652230Z","iopub.status.idle":"2025-03-25T07:07:03.657794Z","shell.execute_reply.started":"2025-03-25T07:07:03.652213Z","shell.execute_reply":"2025-03-25T07:07:03.657033Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model.eval() # disable dropout\n\n# out = generate_text_simple(\n#     model=model,\n#     idx=encoded_tensor, \n#     max_new_tokens=16, \n#     context_size=GPT_CONFIG_124M[\"context_length\"]\n# )\n\n# print(\"Output:\", out)\n# decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n# print(decoded_text)\n# print(\"Output length:\", len(out[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T07:07:03.880073Z","iopub.execute_input":"2025-03-25T07:07:03.880265Z","iopub.status.idle":"2025-03-25T07:07:03.883221Z","shell.execute_reply.started":"2025-03-25T07:07:03.880248Z","shell.execute_reply":"2025-03-25T07:07:03.882607Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\n\n\nclass GPTDatasetV1(Dataset):\n    def __init__(self, txt, tokenizer, max_length, stride):\n        self.input_ids = []\n        self.target_ids = []\n\n        # Tokenize the entire text\n        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n\n        # Use a sliding window to chunk the book into overlapping sequences of max_length\n        for i in range(0, len(token_ids) - max_length, stride):\n            input_chunk = token_ids[i:i + max_length]\n            target_chunk = token_ids[i + 1: i + max_length + 1]\n            self.input_ids.append(torch.tensor(input_chunk))\n            self.target_ids.append(torch.tensor(target_chunk))\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return self.input_ids[idx], self.target_ids[idx]\n\n\ndef create_dataloader_v1(txt, batch_size=4, max_length=256,\n                         stride=128, shuffle=True, drop_last=True, num_workers=0):\n    # Initialize the tokenizer\n    tokenizer = tiktoken.get_encoding(\"gpt2\")\n\n    # Create dataset\n    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n\n    # Create dataloader\n    dataloader = DataLoader(\n        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n\n    return dataloader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T16:40:07.216164Z","iopub.status.idle":"2025-03-29T16:40:07.216540Z","shell.execute_reply":"2025-03-29T16:40:07.216385Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n# If the `previous_chapters.py` file is not available locally,\n# you can import it from the `llms-from-scratch` PyPI package.\n# For details, see: https://github.com/rasbt/LLMs-from-scratch/tree/main/pkg\n# E.g.,\n# from llms_from_scratch.ch04 import GPTModel\n\nGPT_CONFIG_124M = {\n    \"vocab_size\": 50257,   # Vocabulary size\n    \"context_length\": 512, # Shortened context length (orig: 1024)\n    \"emb_dim\": 768,        # Embedding dimension\n    \"n_heads\": 12,         # Number of attention heads\n    \"n_layers\": 12,        # Number of layers\n    \"drop_rate\": 0.1,      # Dropout rate\n    \"qkv_bias\": False      # Query-key-value bias\n}\n\ntorch.manual_seed(123)\nmodel = GPTModel(GPT_CONFIG_124M)\nmodel.eval();  # Disable dropout during inference","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T16:40:07.217209Z","iopub.status.idle":"2025-03-29T16:40:07.217587Z","shell.execute_reply":"2025-03-29T16:40:07.217430Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tiktoken\n\n# Alternatively:\n# from llms_from_scratch.ch04 import generate_text_simple\n\ndef text_to_token_ids(text, tokenizer):\n    # encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n    return encoded_tensor\n\ndef token_ids_to_text(token_ids, tokenizer):\n    flat = token_ids.squeeze(0) # remove batch dimension\n    return tokenizer.decode(flat.tolist())\n\nstart_context = \"Every effort moves you\"\ntokenizer = tiktoken.get_encoding(\"gpt2\")\n\ntoken_ids = generate_text_simple(\n    model=model,\n    idx=text_to_token_ids(start_context, tokenizer),\n    max_new_tokens=10,\n    context_size=GPT_CONFIG_124M[\"context_length\"]\n)\n\nprint(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T14:34:10.004824Z","iopub.execute_input":"2025-03-25T14:34:10.005148Z","iopub.status.idle":"2025-03-25T14:34:11.034414Z","shell.execute_reply.started":"2025-03-25T14:34:10.005119Z","shell.execute_reply":"2025-03-25T14:34:11.033318Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n                       [40,    1107, 588]])   #  \"I really like\"]\n\ntargets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n                        [1107,  588, 11311]]) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T08:44:47.961417Z","iopub.execute_input":"2025-03-24T08:44:47.961740Z","iopub.status.idle":"2025-03-24T08:44:47.965906Z","shell.execute_reply.started":"2025-03-24T08:44:47.961714Z","shell.execute_reply":"2025-03-24T08:44:47.964923Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# with torch.no_grad():\n#     logits = model(inputs)\n\n# probas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\n# print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T08:40:40.636081Z","iopub.execute_input":"2025-03-24T08:40:40.636368Z","iopub.status.idle":"2025-03-24T08:40:40.648326Z","shell.execute_reply.started":"2025-03-24T08:40:40.636345Z","shell.execute_reply":"2025-03-24T08:40:40.647418Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n# print(\"Token IDs:\\n\", token_ids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T08:40:40.649163Z","iopub.execute_input":"2025-03-24T08:40:40.649398Z","iopub.status.idle":"2025-03-24T08:40:40.662780Z","shell.execute_reply.started":"2025-03-24T08:40:40.649377Z","shell.execute_reply":"2025-03-24T08:40:40.661983Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n# print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T08:40:40.663425Z","iopub.execute_input":"2025-03-24T08:40:40.663816Z","iopub.status.idle":"2025-03-24T08:40:40.676101Z","shell.execute_reply.started":"2025-03-24T08:40:40.663786Z","shell.execute_reply":"2025-03-24T08:40:40.675265Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# import urllib.request\n\n# file_path = \"the-verdict.txt\"\n# url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n\n# if not os.path.exists(file_path):\n#     with urllib.request.urlopen(url) as response:\n#         text_data = response.read().decode('utf-8')\n#     with open(file_path, \"w\", encoding=\"utf-8\") as file:\n#         file.write(text_data)\n# else:\n#     with open(file_path, \"r\", encoding=\"utf-8\") as file:\n#         text_data = file.read()\n\n# # flytech/python-codes-25k","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T08:40:40.676937Z","iopub.execute_input":"2025-03-24T08:40:40.677233Z","iopub.status.idle":"2025-03-24T08:40:40.688301Z","shell.execute_reply.started":"2025-03-24T08:40:40.677201Z","shell.execute_reply":"2025-03-24T08:40:40.687400Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\n\ntrain_dataset = load_dataset('flytech/python-codes-25k', split='train')\n# train_dataset = load_dataset('flytech/python-codes-25k', split='test')\n\n# One can map the dataset in any way, for the sake of example:\ndataset = train_dataset.map(lambda example: {'text': example['instruction'] + ' ' + example['input'] + ' ' + example['output']})['text']\n# Remember that you don't need to map if the dataset has a \"text\" field already:)\n\ncombined_dataset = \"<|endoftext|>\".join(dataset[:])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T14:34:21.336199Z","iopub.execute_input":"2025-03-25T14:34:21.336553Z","iopub.status.idle":"2025-03-25T14:34:30.036147Z","shell.execute_reply.started":"2025-03-25T14:34:21.336527Z","shell.execute_reply":"2025-03-25T14:34:30.035034Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(dataset[:1][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T08:45:01.998455Z","iopub.execute_input":"2025-03-24T08:45:01.998787Z","iopub.status.idle":"2025-03-24T08:45:02.003023Z","shell.execute_reply.started":"2025-03-24T08:45:01.998754Z","shell.execute_reply":"2025-03-24T08:45:02.002316Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(dataset)\nencoded = tokenizer.encode(combined_dataset, allowed_special={'<|endoftext|>'})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T07:09:43.032478Z","iopub.execute_input":"2025-03-25T07:09:43.032772Z","iopub.status.idle":"2025-03-25T07:09:43.096736Z","shell.execute_reply.started":"2025-03-25T07:09:43.032751Z","shell.execute_reply":"2025-03-25T07:09:43.096141Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"total_characters = len(combined_dataset)\ntotal_tokens = len(tokenizer.encode(combined_dataset, allowed_special={'<|endoftext|>'}))\n\nprint(\"Characters:\", total_characters)\nprint(\"Tokens:\", total_tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T07:09:43.777444Z","iopub.execute_input":"2025-03-25T07:09:43.777735Z","iopub.status.idle":"2025-03-25T07:09:43.844785Z","shell.execute_reply.started":"2025-03-25T07:09:43.777713Z","shell.execute_reply":"2025-03-25T07:09:43.844176Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"text_data = combined_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T07:09:32.289834Z","iopub.execute_input":"2025-03-25T07:09:32.290200Z","iopub.status.idle":"2025-03-25T07:09:32.293863Z","shell.execute_reply.started":"2025-03-25T07:09:32.290172Z","shell.execute_reply":"2025-03-25T07:09:32.293009Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\n\ntrain_dataset = load_dataset('flytech/python-codes-25k', split='train')\ndataset = train_dataset.map(lambda example: {'text': example['instruction'] + ' ' + example['input'] + ' ' + example['output']})['text']\n\ncombined_dataset = \"<|endoftext|>\".join(dataset[:])\n\ntrain_ratio = 0.90\nsplit_idx = int(train_ratio * len(text_data))\ntrain_data = text_data[:split_idx]\nval_data = text_data[split_idx:]\n\n\ntorch.manual_seed(123)\n\ntrain_loader = create_dataloader_v1(\n    train_data,\n    batch_size=16,\n    max_length=GPT_CONFIG_124M[\"context_length\"],\n    stride=GPT_CONFIG_124M[\"context_length\"],\n    drop_last=True,\n    shuffle=True,\n    num_workers=0\n)\n\nval_loader = create_dataloader_v1(\n    val_data,\n    batch_size=16,\n    max_length=GPT_CONFIG_124M[\"context_length\"],\n    stride=GPT_CONFIG_124M[\"context_length\"],\n    drop_last=False,\n    shuffle=False,\n    num_workers=0\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T07:09:32.455673Z","iopub.execute_input":"2025-03-25T07:09:32.455883Z","iopub.status.idle":"2025-03-25T07:09:32.571599Z","shell.execute_reply.started":"2025-03-25T07:09:32.455865Z","shell.execute_reply":"2025-03-25T07:09:32.570947Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"split_idx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T08:45:08.083472Z","iopub.execute_input":"2025-03-24T08:45:08.083782Z","iopub.status.idle":"2025-03-24T08:45:08.088432Z","shell.execute_reply.started":"2025-03-24T08:45:08.083755Z","shell.execute_reply":"2025-03-24T08:45:08.087746Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sanity check\n\nif total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n    print(\"Not enough tokens for the training loader. \"\n          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n          \"increase the `training_ratio`\")\n\nif total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n    print(\"Not enough tokens for the validation loader. \"\n          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n          \"decrease the `training_ratio`\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T07:09:50.784286Z","iopub.execute_input":"2025-03-25T07:09:50.784578Z","iopub.status.idle":"2025-03-25T07:09:50.788738Z","shell.execute_reply.started":"2025-03-25T07:09:50.784555Z","shell.execute_reply":"2025-03-25T07:09:50.787970Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T08:40:41.807169Z","iopub.execute_input":"2025-03-24T08:40:41.807438Z","iopub.status.idle":"2025-03-24T08:40:41.821469Z","shell.execute_reply.started":"2025-03-24T08:40:41.807418Z","shell.execute_reply":"2025-03-24T08:40:41.820533Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Train loader:\")\n\ntrain_ds = 0\ntest_ds = 0\nfor x, y in train_loader:\n    # print(x.shape, y.shape)\n    train_ds += 1\n\nprint(\"\\nValidation loader:\")\nfor x, y in val_loader:\n    # print(x.shape, y.shape)\n    test_ds += 1\n\nprint(train_ds, test_ds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T07:09:53.217714Z","iopub.execute_input":"2025-03-25T07:09:53.218042Z","iopub.status.idle":"2025-03-25T07:09:53.227697Z","shell.execute_reply.started":"2025-03-25T07:09:53.218009Z","shell.execute_reply":"2025-03-25T07:09:53.227003Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"14*32 + 2*32","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T08:40:41.840053Z","iopub.execute_input":"2025-03-24T08:40:41.840324Z","iopub.status.idle":"2025-03-24T08:40:41.845288Z","shell.execute_reply.started":"2025-03-24T08:40:41.840297Z","shell.execute_reply":"2025-03-24T08:40:41.844504Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calc_loss_batch(input_batch, target_batch, model, device):\n    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n    logits = model(input_batch)\n    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n    return loss\n\n\ndef calc_loss_loader(data_loader, model, device, num_batches=None):\n    total_loss = 0.\n    if len(data_loader) == 0:\n        return float(\"nan\")\n    elif num_batches is None:\n        num_batches = len(data_loader)\n    else:\n        # Reduce the number of batches to match the total number of batches in the data loader\n        # if num_batches exceeds the number of batches in the data loader\n        num_batches = min(num_batches, len(data_loader))\n    for i, (input_batch, target_batch) in enumerate(data_loader):\n        if i < num_batches:\n            loss = calc_loss_batch(input_batch, target_batch, model, device)\n            total_loss += loss.item()\n        else:\n            break\n    return total_loss / num_batches","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T16:40:13.896357Z","iopub.execute_input":"2025-03-29T16:40:13.896649Z","iopub.status.idle":"2025-03-29T16:40:13.902443Z","shell.execute_reply.started":"2025-03-29T16:40:13.896628Z","shell.execute_reply":"2025-03-29T16:40:13.901614Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T14:34:48.488659Z","iopub.execute_input":"2025-03-25T14:34:48.488953Z","iopub.status.idle":"2025-03-25T14:34:48.494504Z","shell.execute_reply.started":"2025-03-25T14:34:48.488931Z","shell.execute_reply":"2025-03-25T14:34:48.493582Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n                       eval_freq, eval_iter, start_context, tokenizer):\n    # Initialize lists to track losses and tokens seen\n    train_losses, val_losses, track_tokens_seen = [], [], []\n    tokens_seen, global_step = 0, -1\n\n    # Main training loop\n    for epoch in range(num_epochs):\n        model.train()  # Set model to training mode\n        \n        for input_batch, target_batch in train_loader:\n            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n            loss = calc_loss_batch(input_batch, target_batch, model, device)\n            loss.backward() # Calculate loss gradients\n            optimizer.step() # Update model weights using loss gradients\n            tokens_seen += input_batch.numel()\n            global_step += 1\n\n            # Optional evaluation step\n            if global_step % eval_freq == 0:\n                train_loss, val_loss = evaluate_model(\n                    model, train_loader, val_loader, device, eval_iter)\n                train_losses.append(train_loss)\n                val_losses.append(val_loss)\n                track_tokens_seen.append(tokens_seen)\n                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n\n        # Print a sample text after each epoch\n        generate_and_print_sample(\n            model, tokenizer, device, start_context\n        )\n\n    return train_losses, val_losses, track_tokens_seen\n\n\ndef evaluate_model(model, train_loader, val_loader, device, eval_iter):\n    model.eval()\n    with torch.no_grad():\n        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n    model.train()\n    return train_loss, val_loss\n\n\n\n\ndef generate_and_print_sample(model, tokenizer, device, start_context):\n    model.eval()\n    context_size = model.pos_emb.weight.shape[0]\n    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n    with torch.no_grad():\n        token_ids = generate_text_simple(\n            model=model, idx=encoded,\n            max_new_tokens=50, context_size=context_size\n        )\n    decoded_text = token_ids_to_text(token_ids, tokenizer)\n    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n    model.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T16:40:16.641484Z","iopub.execute_input":"2025-03-29T16:40:16.641889Z","iopub.status.idle":"2025-03-29T16:40:16.654480Z","shell.execute_reply.started":"2025-03-29T16:40:16.641849Z","shell.execute_reply":"2025-03-29T16:40:16.653350Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T14:34:56.967929Z","iopub.execute_input":"2025-03-25T14:34:56.968223Z","iopub.status.idle":"2025-03-25T14:34:56.972348Z","shell.execute_reply.started":"2025-03-25T14:34:56.968200Z","shell.execute_reply":"2025-03-25T14:34:56.971333Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# torch.manual_seed(123)\n# model = GPTModel(GPT_CONFIG_124M)\n# model.to(device)\n# optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n\n# num_epochs = 10\n# train_losses, val_losses, tokens_seen = train_model_simple(\n#     model, train_loader, val_loader, optimizer, device,\n#     num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n#     start_context=\"Give me a python program to add two numbers\", tokenizer=tokenizer\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T14:35:00.464301Z","iopub.execute_input":"2025-03-25T14:35:00.464647Z","iopub.status.idle":"2025-03-25T14:35:00.468231Z","shell.execute_reply.started":"2025-03-25T14:35:00.464619Z","shell.execute_reply":"2025-03-25T14:35:00.467573Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"combined_dataset[:100]","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-24T08:41:00.123Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\n\n\ndef plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n    fig, ax1 = plt.subplots(figsize=(5, 3))\n\n    # Plot training and validation loss against epochs\n    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n    ax1.set_xlabel(\"Epochs\")\n    ax1.set_ylabel(\"Loss\")\n    ax1.legend(loc=\"upper right\")\n    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n\n    # Create a second x-axis for tokens seen\n    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n    ax2.set_xlabel(\"Tokens seen\")\n\n    fig.tight_layout()  # Adjust layout to make room\n    plt.savefig(\"loss-plot.pdf\")\n    plt.show()\n\nepochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\nplot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T08:58:10.959854Z","iopub.execute_input":"2025-03-24T08:58:10.960167Z","iopub.status.idle":"2025-03-24T08:58:12.321112Z","shell.execute_reply.started":"2025-03-24T08:58:10.960130Z","shell.execute_reply":"2025-03-24T08:58:12.320231Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.to(\"cpu\")\nmodel.eval()\n\ntokenizer = tiktoken.get_encoding(\"gpt2\")\n\ntoken_ids = generate_text_simple(\n    model=model,\n    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n    max_new_tokens=25,\n    context_size=GPT_CONFIG_124M[\"context_length\"]\n)\n\nprint(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-24T08:41:00.123Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    # For-loop is the same as before: Get logits, and only focus on last time step\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -context_size:]\n        with torch.no_grad():\n            logits = model(idx_cond)\n        logits = logits[:, -1, :]\n\n        # New: Filter logits with top_k sampling\n        if top_k is not None:\n            # Keep only top_k values\n            top_logits, _ = torch.topk(logits, top_k)\n            min_val = top_logits[:, -1]\n            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n\n        # New: Apply temperature scaling\n        if temperature > 0.0:\n            logits = logits / temperature\n\n            # Apply softmax to get probabilities\n            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n\n            # Sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n\n        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n        else:\n            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n\n        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n            break\n\n        # Same as before: append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n\n    return idx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T16:40:53.110545Z","iopub.execute_input":"2025-03-29T16:40:53.110854Z","iopub.status.idle":"2025-03-29T16:40:53.116947Z","shell.execute_reply.started":"2025-03-29T16:40:53.110827Z","shell.execute_reply":"2025-03-29T16:40:53.116018Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"torch.manual_seed(123)\n\ntoken_ids = generate(\n    model=model,\n    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n    max_new_tokens=50,\n    context_size=GPT_CONFIG_124M[\"context_length\"],\n    top_k=25,\n    temperature=1.4\n)\n\nprint(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T16:40:53.313659Z","iopub.execute_input":"2025-03-29T16:40:53.313968Z","iopub.status.idle":"2025-03-29T16:40:53.325986Z","shell.execute_reply.started":"2025-03-29T16:40:53.313940Z","shell.execute_reply":"2025-03-29T16:40:53.324976Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-ebfafa97f799>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m token_ids = generate(\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_to_token_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Every effort moves you\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"],"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error"}],"execution_count":7},{"cell_type":"code","source":"# %%writefile gpt2_multigpu.py\nimport torch\nimport torch.nn as nn\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nimport torch.distributed as dist\nimport torch.optim as optim\nimport tiktoken\nfrom torch.utils.data import DataLoader, Dataset\nimport os\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport torch.nn.functional as F\n\nGPT_CONFIG_124M = {\n    \"vocab_size\": 50257,    # Vocabulary size\n    \"context_length\": 512, # Context length\n    \"emb_dim\": 768,         # Embedding dimension\n    \"n_heads\": 12,          # Number of attention heads\n    \"n_layers\": 12,         # Number of layers\n    \"drop_rate\": 0.1,       # Dropout rate\n    \"qkv_bias\": False       # Query-Key-Value bias\n}\n\nclass GELU(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return 0.5 * x * (1 + torch.tanh(\n            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n            (x + 0.044715 * torch.pow(x, 3))\n        ))\n\nclass FeedForward(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n            GELU(),\n            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.attention = MultiHeadCasualAttention(\n            d_in= cfg[\"emb_dim\"],\n            d_out=cfg[\"emb_dim\"],\n            context_length=cfg[\"context_length\"],\n            dropout=cfg[\"drop_rate\"],\n            num_heads=cfg[\"n_heads\"],\n            qkv_bias=cfg[\"qkv_bias\"]\n        )\n\n        self.ff = FeedForward(cfg)\n        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n\n    def forward(self, x):\n        # This block does nothing and just returns its input.\n        shortcut = x\n        x = self.norm1(x)\n        x = self.attention(x)\n        x = self.drop_shortcut(x)\n\n        x = x + shortcut\n\n        shortcut = x\n\n        x = self.norm2(x)\n        x = self.ff(x)\n        x = self.drop_shortcut(x)\n        x = x + shortcut\n        return x\n\nclass LayerNorm(nn.Module):\n    def __init__(self, emb_dim, eps=1e-5):\n        super().__init__()\n        self.eps = eps\n        self.scale = nn.Parameter(torch.ones(emb_dim))\n        self.shift = nn.Parameter(torch.zeros(emb_dim))\n    \n    def forward(self, x):\n        # This layer does nothing and just returns its input.\n\n        mean = x.mean(dim = -1, keepdim = True)\n        var = x.var(dim=-1,keepdim = True, unbiased = False)\n        norm_x = (x-mean)/torch.sqrt(var+self.eps)\n        return self.scale * norm_x + self.shift\n\nclass MultiHeadCasualAttention(nn.Module):\n    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n        super().__init__()\n        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n\n        self.d_out = d_out\n        self.num_heads = num_heads\n        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n\n        self.rope = RoPE(self.head_dim, context_length)\n\n    \n    def forward(self, x):\n        b, num_tokens, d_in = x.shape\n\n        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n        queries = self.W_query(x)\n        values = self.W_value(x)\n\n        # We implicitly split the matrix by adding a `num_heads` dimension\n        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n\n        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n        keys = keys.transpose(1, 2)\n        queries = queries.transpose(1, 2)\n        values = values.transpose(1, 2)\n\n        queries, keys = self.rope(queries, keys)\n\n        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n        \n        # Use scaled_dot_product_attention which can leverage Flash Attention\n        attn_output = F.scaled_dot_product_attention(\n            queries, keys, values,\n            attn_mask=mask_bool,\n            dropout_p=self.dropout.p if self.training else 0.0, # Apply dropout if in training mode\n            is_causal=False # Mask is provided, so is_causal is False\n        )\n\n        # Shape: (b, num_heads, num_tokens, head_dim) -> (b, num_tokens, num_heads, head_dim)\n        context_vec = attn_output.transpose(1, 2).contiguous().view(b, num_tokens, self.d_out)\n\n        context_vec = self.out_proj(context_vec)  # optional projection\n\n        return context_vec\n\n        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n        # attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n\n        # # Original mask truncated to the number of tokens and converted to boolean\n        # mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n\n        # # Use the mask to fill attention scores\n        # attn_scores.masked_fill_(mask_bool, -torch.inf)\n\n        # attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n        # attn_weights = self.dropout(attn_weights)\n\n        # # Shape: (b, num_tokens, num_heads, head_dim)\n        # context_vec = (attn_weights @ values).transpose(1, 2)\n\n        # # Combine heads, where self.d_out = self.num_heads * self.head_dim\n        # context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n        # context_vec = self.out_proj(context_vec)  # optional projection\n\n        # return context_vec\n\nimport torch\nimport torch.nn as nn\nimport math\n\ndef rotate_half(x):\n    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n    x1 = x[..., :x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2:]\n    return torch.cat((-x2, x1), dim=-1)\n\ndef apply_rotary_pos_emb(q, k, cos, sin, offset: int = 0):\n    # The first two dimensions of q and k should be (batch, num_heads).\n    # The last dimension is the embedding dimension, which we'll rotate.\n    q_rotated = (q * cos[:, offset:offset + q.shape[2], :]) + (rotate_half(q) * sin[:, offset:offset + q.shape[2], :])\n    k_rotated = (k * cos[:, offset:offset + k.shape[2], :]) + (rotate_half(k) * sin[:, offset:offset + k.shape[2], :])\n    return q_rotated, k_rotated\n\nclass RoPE(nn.Module):\n    def __init__(self, dim, context_length, base=10000):\n        super().__init__()\n        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n        t = torch.arange(context_length, device=inv_freq.device)\n        freqs = torch.outer(t, inv_freq)\n        emb = torch.cat((freqs, freqs), dim=-1)\n        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :])\n        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :])\n\n    def forward(self, q, k):\n        # q and k have shapes (batch, num_heads, seq_len, head_dim)\n        seq_len = q.shape[2]\n        cos = self.cos_cached[:, :, :seq_len, :]\n        sin = self.sin_cached[:, :, :seq_len, :]\n        q_rotated, k_rotated = apply_rotary_pos_emb(q, k, cos, sin)\n        return q_rotated, k_rotated\n\n\n# Assuming you have your GPT-2 model defined already\nclass GPTModel(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n\n        self.trf_blocks = nn.Sequential(\n            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n        self.out_head = nn.Linear(\n            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n        )\n\n    def forward(self, in_idx):\n        # print(in_idx.shape)\n        batch_size, seq_len = in_idx.shape\n        tok_embeds = self.tok_emb(in_idx)\n        pos_embeds = self.pos_emb(torch.arange(seq_len, device = in_idx.device))\n        x = tok_embeds + pos_embeds\n        x = self.drop_emb(x)\n        x = self.trf_blocks(x)\n        x = self.final_norm(x)\n\n        logits = self.out_head(x)\n        return logits\n\n# Custom Dataset (assuming you have this)\nclass CustomDataset(Dataset):\n    def __init__(self, txt, tokenizer, max_length, stride):\n        self.input_ids = []\n        self.target_ids = []\n\n        # Tokenize the entire text\n        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n\n        # Use a sliding window to chunk the book into overlapping sequences of max_length\n        for i in range(0, len(token_ids) - max_length, stride):\n            input_chunk = token_ids[i:i + max_length]\n            target_chunk = token_ids[i + 1: i + max_length + 1]\n            self.input_ids.append(torch.tensor(input_chunk))\n            self.target_ids.append(torch.tensor(target_chunk))\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return self.input_ids[idx], self.target_ids[idx]\n    \ndef create_dataloader_v1(txt, batch_size=4, max_length=256,\n                         stride=128, shuffle=True, drop_last=True, num_workers=0):\n    # Initialize the tokenizer\n    tokenizer = tiktoken.get_encoding(\"gpt2\")\n\n    # Create dataset\n    dataset = CustomDataset(txt, tokenizer, max_length, stride)\n\n    # Create dataloader\n    # dataloader = DataLoader(\n    #     dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n\n    return dataset\n\n\ndef load_train_val_dataset():\n    from datasets import load_dataset\n\n    train_dataset = load_dataset('flytech/python-codes-25k', split='train')\n    # train_dataset = load_dataset('flytech/python-codes-25k', split='test')\n\n    # One can map the dataset in any way, for the sake of example:\n    dataset = train_dataset.map(lambda example: {'text': example['instruction'] + ' ' + example['input'] + ' ' + example['output']})['text']\n    # Remember that you don't need to map if the dataset has a \"text\" field already:)\n    train_ratio = 0.90\n    # text_data = \"<|endoftext|>\".join(dataset[:50])\n    dataset = dataset[:]\n    split_idx = int(train_ratio * len(dataset))\n\n    train_data = dataset[:split_idx]\n    val_data = dataset[split_idx:]\n    train_data = \"<|endoftext|>\".join(train_data[:])\n    val_data = \"<|endoftext|>\".join(val_data[:])\n    # print(\"split_idx: \", split_idx)\n\n    torch.manual_seed(123)\n\n    train_loader = create_dataloader_v1(\n        train_data,\n        batch_size=8,\n        max_length=GPT_CONFIG_124M[\"context_length\"],\n        stride=GPT_CONFIG_124M[\"context_length\"],\n        drop_last=True,\n        shuffle=True,\n        num_workers=0\n    )\n\n    val_loader = create_dataloader_v1(\n        val_data,\n        batch_size=8,\n        max_length=GPT_CONFIG_124M[\"context_length\"],\n        stride=GPT_CONFIG_124M[\"context_length\"],\n        drop_last=False,\n        shuffle=False,\n        num_workers=0\n    )\n    # print(\"length of dataset\", len(train_data), len(val_data))\n    # print(\"length of loader\", len(train_loader), len(val_loader))\n    return train_loader, val_loader\n\ndef text_to_token_ids(text, tokenizer):\n    # encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n    return encoded_tensor\n\ndef token_ids_to_text(token_ids, tokenizer):\n    flat = token_ids.squeeze(0) # remove batch dimension\n    return tokenizer.decode(flat.tolist())\n\ndef calc_loss_batch(input_batch, target_batch, model, device):\n    try:\n        input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n        # with torch.no_grad():\n        logits = model(input_batch)\n        loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n        del input_batch, target_batch, logits\n        torch.cuda.empty_cache()\n        return loss\n    except Exception as exp:\n        print(exp)\n        return torch.tensor(0)\n\ndef calc_loss_loader(data_loader, model, device, num_batches=None):\n    total_loss = 0.\n    if len(data_loader) == 0:\n        return float(\"nan\")\n    elif num_batches is None:\n        num_batches = len(data_loader)\n    else:\n        # Reduce the number of batches to match the total number of batches in the data loader\n        # if num_batches exceeds the number of batches in the data loader\n        num_batches = min(num_batches, len(data_loader))\n    for i, (input_batch, target_batch) in enumerate(data_loader):\n        if i < num_batches:\n            loss = calc_loss_batch(input_batch, target_batch, model, device)\n            total_loss += loss.item()\n        else:\n            break\n    return total_loss / num_batches\n\n\ndef setup_ddp(rank, world_size):\n    \"\"\"Initialize the distributed environment\"\"\"\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n\ndef cleanup():\n    \"\"\"Clean up the distributed environment\"\"\"\n    dist.destroy_process_group()\n\ndef plot_losses(train_losses, val_losses, epochs):\n    plt.figure(figsize=(10, 6))\n    plt.plot(train_losses, label='Train Loss (per batch)')\n    plt.plot([i * (len(train_losses) // epochs) for i in range(epochs + 1)], [val_losses[0]] + val_losses, label='Validation Loss (per epoch)', marker='o')\n    plt.xlabel('Batch / Epoch')\n    plt.ylabel('Loss')\n    plt.title('Train and Validation Loss')\n    plt.legend()\n    plt.grid(True)\n    plt.draw()\n    plt.pause(0.1)\n\ndef train(batch_size=8, epochs=15):\n    from tqdm.notebook import tqdm\n    \"\"\"Training function with DDP\"\"\"\n    # Setup DDP\n    # setup_ddp(rank, world_size)\n    \n    # Set device\n    # device = torch.device(f'cuda:{rank}')\n    # torch.cuda.set_device(device)\n    device = torch.device(\"cuda\")\n    \n    # Create model and move to device\n    model = GPTModel(GPT_CONFIG_124M).to(device)\n    \n    # Wrap model with DDP\n    # model = DDP(model, device_ids=[rank])\n    \n    # Create optimizer\n    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n    \n    # Prepare data\n    train_data, val_data = load_train_val_dataset()\n    # dataset = CustomDataset(train_data)\n    # Use DistributedSampler for DDP\n    # train_sampler = torch.utils.data.distributed.DistributedSampler(\n    #     train_data,\n    #     num_replicas=world_size,\n    #     rank=rank\n    # )\n    train_dataloader = DataLoader(\n        train_data,\n        batch_size=batch_size,\n        # sampler=train_sampler,\n        num_workers=4\n    )\n\n    # val_sampler = torch.utils.data.distributed.DistributedSampler(\n    #     val_data,\n    #     num_replicas=world_size,\n    #     rank=rank,\n    #     shuffle=False\n    # )\n    val_loader = DataLoader(\n        val_data,\n        batch_size=batch_size,\n        num_workers=4,\n        drop_last=False\n    )\n    print(\"train_data: \",  len(train_dataloader), \" val_data: \", len(val_loader))\n    # val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True, drop_last=False, num_workers=4)\n    \n    # if rank == 0:\n    #     plt.ion()\n        \n    # Training loop\n    model.train()\n    training_loss = []\n    validation_loss = []\n    # with tqdm(total=epochs * len(train_dataloader)) as pbar:\n    for epoch in tqdm(range(epochs), desc='Epochs'):\n        # train_sampler.set_epoch(epoch)  # Ensure shuffling works properly across epochs\n        \n        total_loss = 0\n        batch_idx = 0\n        for input_batch, target_batch in tqdm(train_dataloader, desc='Batch', leave=False):\n            optimizer.zero_grad()\n            batch_idx += 1\n\n            loss = calc_loss_batch(input_batch, target_batch, model, device)  # Assuming your model returns a loss\n            \n            # Backward pass\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n            # pbar.update(1)\n            \n            if batch_idx % 100 == 0:\n                print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item():.4f}')\n        \n        # if rank == 0:\n        avg_loss = total_loss / len(train_dataloader)\n        # print(f'Epoch {epoch} completed. Average Train Loss: {avg_loss:.4f}')\n        training_loss.append(avg_loss)\n\n        total_val_loss = 0\n        val_idx = 0\n        with torch.no_grad():\n            for input_batch, target_batch in (val_loader):\n                model.eval()\n                val_loss = calc_loss_batch(input_batch, target_batch, model, device)\n                val_idx += 1\n                total_val_loss += (val_loss.item())\n\n        if val_idx:\n            val_loss = total_val_loss/(val_idx)\n            validation_loss.append(val_loss)\n            print(f'Epoch {epoch} completed. Average Train Loss: {avg_loss:.4f}. Validation Loss: {val_loss:.4f}')\n        print(\"---\"*20)\n\n    \n    # Save model (only from rank 0)\n    # if rank == 0:\n    # torch.save(model.module.state_dict(), 'gpt2_ddp_model_3.pth')\n        torch.save(model.to(torch.device(\"cpu\")).state_dict(), f'gpt2_ddp_model_4_{epoch}.pth')\n        model.to(torch.device(\"cuda\"))\n    torch.save({\"train_loss\": training_loss, \"validation_loss\": validation_loss}, f\"loss_gpt2_ddp_model_4.pth\")\n        # plot_losses(training_loss, validation_loss, epoch + 1)\n        # plt.ioff() # Turn off interactive mode\n        # plt.show()\n    \n        # Cleanup\n        # cleanup()\n# \ndef main():\n    # Model configuration\n    GPT_CONFIG_124M = {\n                        \"vocab_size\": 50257,   # Vocabulary size\n                        \"context_length\": 512, # Shortened context length (orig: 1024)\n                        \"emb_dim\": 768,        # Embedding dimension\n                        \"n_heads\": 12,         # Number of attention heads\n                        \"n_layers\": 12,        # Number of layers\n                        \"drop_rate\": 0.1,      # Dropout rate\n                        \"qkv_bias\": False      # Query-key-value bias\n                    }\n\n    # world_size = torch.cuda.device_count()\n    \n    # # Launch DDP training\n    # torch.multiprocessing.spawn(\n    #     train,\n    #     args=(world_size,),\n    #     nprocs=world_size,\n    #     join=True\n    # )\n\n# if __name__ == \"__main__\":\n    # Set multiprocessing start methodpbar.update(1)\n    # torch.multiprocessing.set_start_method('spawn')\n    # main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T18:02:22.290669Z","iopub.execute_input":"2025-03-31T18:02:22.291061Z","iopub.status.idle":"2025-03-31T18:02:22.329930Z","shell.execute_reply.started":"2025-03-31T18:02:22.291038Z","shell.execute_reply":"2025-03-31T18:02:22.329006Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"train(batch_size=10, epochs=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T18:02:28.143280Z","iopub.execute_input":"2025-03-31T18:02:28.143606Z","iopub.status.idle":"2025-03-31T18:14:15.456098Z","shell.execute_reply.started":"2025-03-31T18:02:28.143580Z","shell.execute_reply":"2025-03-31T18:14:15.454646Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/3.29k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"799b8a79f2ae4b7784242c38e5a53a3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"python-codes-25k.json:   0%|          | 0.00/26.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50b4ecbe7a4243819631d89e84a07c2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"python-codes-25k.jsonl:   0%|          | 0.00/25.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"020170a7c2a34de4a596aa7f7c4759b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/49626 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e620481431134cd89084ef51180a94c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/49626 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca72fded607e47879343350e7f06f731"}},"metadata":{}},{"name":"stdout","text":"train_data:  1338  val_data:  176\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epochs:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8d0093ef7a34361a9a2513e336b010d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batch:   0%|          | 0/1338 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0ef3a82d0c24ec7a2f7e5c8961f87ea"}},"metadata":{}},{"name":"stdout","text":"Epoch: 0, Batch: 100, Loss: 3.1769\nEpoch: 0, Batch: 200, Loss: 0.7039\nEpoch: 0, Batch: 300, Loss: 0.3968\nEpoch: 0, Batch: 400, Loss: 0.2091\nEpoch: 0, Batch: 500, Loss: 0.2319\nEpoch: 0, Batch: 600, Loss: 0.5465\nEpoch: 0, Batch: 700, Loss: 0.1304\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-e20e25d5f4c2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-2-72b6a529b7f7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(batch_size, epochs)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":3},{"cell_type":"code","source":"GPT_CONFIG_124M = {\n    \"vocab_size\": 50257,    # Vocabulary size\n    \"context_length\": 512, # Context length\n    \"emb_dim\": 768,         # Embedding dimension\n    \"n_heads\": 12,          # Number of attention heads\n    \"n_layers\": 12,         # Number of layers\n    \"drop_rate\": 0.1,       # Dropout rate\n    \"qkv_bias\": False       # Query-Key-Value bias\n}\n\nsnapshot_model = GPTModel(GPT_CONFIG_124M)\n\n# /kaggle/input/py-gpt2/pytorch/default/1/gpt2_ddp_model_1.pth\n\nsnapshot_model.load_state_dict(torch.load(\"/kaggle/working/gpt2_ddp_model_3_14.pth\", weights_only=True))\n# snapshot_model.eval()\ndevice = torch.device(\"cpu\")\nsnapshot_model.to(device)\ntorch.save(snapshot_model.state_dict(), f'gpt2_ddp_model_3_14_cpu.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T14:14:00.155201Z","iopub.execute_input":"2025-03-31T14:14:00.155525Z","iopub.status.idle":"2025-03-31T14:14:03.015279Z","shell.execute_reply.started":"2025-03-31T14:14:00.155490Z","shell.execute_reply":"2025-03-31T14:14:03.014517Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    # idx.to(device)\n    # For-loop is the same as before: Get logits, and only focus on last time step\n    for _ in range(max_new_tokens):\n        \n        idx_cond = idx[:, -context_size:]\n        with torch.no_grad():\n            logits = model(idx_cond)\n        logits = logits[:, -1, :]\n\n        # New: Filter logits with top_k sampling\n        if top_k is not None:\n            # Keep only top_k values\n            top_logits, _ = torch.topk(logits, top_k)\n            min_val = top_logits[:, -1]\n            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n\n        # New: Apply temperature scaling\n        if temperature > 0.0:\n            logits = logits / temperature\n\n            # Apply softmax to get probabilities\n            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n\n            # Sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n\n        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n        else:\n            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n\n        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n            break\n\n        # Same as before: append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n\n    return idx\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T14:21:34.990149Z","iopub.execute_input":"2025-03-31T14:21:34.990456Z","iopub.status.idle":"2025-03-31T14:21:34.997360Z","shell.execute_reply.started":"2025-03-31T14:21:34.990434Z","shell.execute_reply":"2025-03-31T14:21:34.996572Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"torch.manual_seed(123)\ntokenizer = tiktoken.get_encoding(\"gpt2\")\nimport time\nstart = time.time()\n\ntoken_ids = generate(\n    model=snapshot_model.to(torch.device(\"cpu\")),\n    idx=text_to_token_ids(\"Python program to calculate the average\", tokenizer),\n    max_new_tokens=250,\n    context_size=GPT_CONFIG_124M[\"context_length\"],\n    top_k=5,\n    temperature=0\n)\nend = time.time()\nprint(\"Time taken: \", end-start)\nprint(\"Tokens/s: \", 250/(end-start))\nprint(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T14:21:37.379559Z","iopub.execute_input":"2025-03-31T14:21:37.379874Z","iopub.status.idle":"2025-03-31T14:22:41.125475Z","shell.execute_reply.started":"2025-03-31T14:21:37.379852Z","shell.execute_reply":"2025-03-31T14:22:41.124650Z"}},"outputs":[{"name":"stdout","text":"Time taken:  63.73817420005798\nTokens/s:  3.922296224164083\nOutput text:\n Python program to calculate the average value of three numbers?  ```python\n# Function to calculate the average of three numbers\n\ndef sum_of_num(num_list):\n  \n  \n   \n # Calculate the average_average\n   total = num_sum(1)\n \n   \n   a = num_sum(num_list_of_list)\n      \n   \n  for num in num_list:\n        print(num_list)\n```<|endoftext|>Develop a machine learning model in Python to predict the values of a type given input data Input: Input: [type: \"price' and the number of people\n {%s: 1},\n {2: '%s', 3: 'price'},\n {'name: '%' : 'John', 4: 'Developer'},\n: {'number: 4: 'Joe', ' lambda 1: 'price'},\n]  ```python\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Function to classify\ndef\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n%%script bash\npython3 gpt2_multigpu.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T08:17:49.653959Z","iopub.execute_input":"2025-03-30T08:17:49.654331Z","iopub.status.idle":"2025-03-30T08:18:07.625222Z","shell.execute_reply.started":"2025-03-30T08:17:49.654303Z","shell.execute_reply":"2025-03-30T08:18:07.624345Z"},"_kg_hide-output":true},"outputs":[{"name":"stdout","text":"length of dataset 30180 3447\nlength of loader 19 2\nval_data:  1\n","output_type":"stream"},{"name":"stderr","text":"Using the latest cached version of the dataset since flytech/python-codes-25k couldn't be found on the Hugging Face Hub\nFound the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/flytech___python-codes-25k/default/0.0.0/0ed98ff2a76c5d133d8c157b814189a5a17ebd20 (last modified on Sun Mar 30 08:15:53 2025).\nTraceback (most recent call last):\n  File \"/kaggle/working/gpt2_multigpu.py\", line 456, in <module>\n    main()\n  File \"/kaggle/working/gpt2_multigpu.py\", line 451, in main\n    train(batch_size=8, epochs=20)\n  File \"/kaggle/working/gpt2_multigpu.py\", line 391, in train\n    loss.backward()\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 581, in backward\n    torch.autograd.backward(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n    _engine_run_backward(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)","\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2471\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2474\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<decorator-gen-103>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'python3 gpt2_multigpu.py\\n'' returned non-zero exit status 1."],"ename":"CalledProcessError","evalue":"Command 'b'python3 gpt2_multigpu.py\\n'' returned non-zero exit status 1.","output_type":"error"}],"execution_count":14},{"cell_type":"code","source":"from tqdm.contrib import itertools\nimport time\nfor i1, i2 in itertools.product(range(5), range(300)):\n    # do something, e.g. sleep\n    time.sleep(0.01)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T10:04:10.001913Z","iopub.execute_input":"2025-03-30T10:04:10.002348Z","execution_failed":"2025-03-30T10:04:22.964Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"941a9b46ff0e4fd3bc779496632b2e4e"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"import tiktoken\nGPT_CONFIG_124M = {\n    \"vocab_size\": 50257,    # Vocabulary size\n    \"context_length\": 512, # Context length\n    \"emb_dim\": 768,         # Embedding dimension\n    \"n_heads\": 12,          # Number of attention heads\n    \"n_layers\": 12,         # Number of layers\n    \"drop_rate\": 0.1,       # Dropout rate\n    \"qkv_bias\": False       # Query-Key-Value bias\n}\n\n# Alternatively:\n# from llms_from_scratch.ch04 import generate_text_simple\n\ndef text_to_token_ids(text, tokenizer):\n    # encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n    return encoded_tensor\n\ndef token_ids_to_text(token_ids, tokenizer):\n    flat = token_ids.squeeze(0) # remove batch dimension\n    return tokenizer.decode(flat.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T10:25:27.307183Z","iopub.execute_input":"2025-03-27T10:25:27.307489Z","iopub.status.idle":"2025-03-27T10:25:27.312734Z","shell.execute_reply.started":"2025-03-27T10:25:27.307461Z","shell.execute_reply":"2025-03-27T10:25:27.311768Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"device = torch.device(\"cpu\")\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T10:25:30.097378Z","iopub.execute_input":"2025-03-27T10:25:30.097792Z","iopub.status.idle":"2025-03-27T10:25:30.102960Z","shell.execute_reply.started":"2025-03-27T10:25:30.097760Z","shell.execute_reply":"2025-03-27T10:25:30.102160Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"device(type='cpu')"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n\n    # For-loop is the same as before: Get logits, and only focus on last time step\n    for _ in range(max_new_tokens):\n        device = torch.device(\"cpu\")\n        idx.to(device)\n        idx_cond = idx[:, -context_size:]\n        with torch.no_grad():\n            logits = model(idx_cond)\n        logits = logits[:, -1, :]\n\n        # New: Filter logits with top_k sampling\n        if top_k is not None:\n            # Keep only top_k values\n            top_logits, _ = torch.topk(logits, top_k)\n            min_val = top_logits[:, -1]\n            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n\n        # New: Apply temperature scaling\n        if temperature > 0.0:\n            logits = logits / temperature\n\n            # Apply softmax to get probabilities\n            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n\n            # Sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n\n        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n        else:\n            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n\n        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n            break\n\n        # Same as before: append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n\n    return idx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T10:25:31.557495Z","iopub.execute_input":"2025-03-27T10:25:31.557819Z","iopub.status.idle":"2025-03-27T10:25:31.564210Z","shell.execute_reply.started":"2025-03-27T10:25:31.557790Z","shell.execute_reply":"2025-03-27T10:25:31.563274Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"snapshot_model = GPTModel(GPT_CONFIG_124M)\n\nsnapshot_model.load_state_dict(torch.load(\"/kaggle/working/gpt2_ddp_model_2_cpu.pth\", weights_only=True))\n# snapshot_model.eval()\ndevice = torch.device(\"cpu\")\nsnapshot_model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T10:25:36.513353Z","iopub.execute_input":"2025-03-27T10:25:36.513658Z","iopub.status.idle":"2025-03-27T10:25:38.297461Z","shell.execute_reply.started":"2025-03-27T10:25:36.513633Z","shell.execute_reply":"2025-03-27T10:25:38.296752Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"GPTModel(\n  (tok_emb): Embedding(50257, 768)\n  (pos_emb): Embedding(512, 768)\n  (drop_emb): Dropout(p=0.1, inplace=False)\n  (trf_blocks): Sequential(\n    (0): TransformerBlock(\n      (attention): MultiHeadCasualAttention(\n        (W_query): Linear(in_features=768, out_features=768, bias=False)\n        (W_key): Linear(in_features=768, out_features=768, bias=False)\n        (W_value): Linear(in_features=768, out_features=768, bias=False)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.1, inplace=False)\n    )\n    (1): TransformerBlock(\n      (attention): MultiHeadCasualAttention(\n        (W_query): Linear(in_features=768, out_features=768, bias=False)\n        (W_key): Linear(in_features=768, out_features=768, bias=False)\n        (W_value): Linear(in_features=768, out_features=768, bias=False)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.1, inplace=False)\n    )\n    (2): TransformerBlock(\n      (attention): MultiHeadCasualAttention(\n        (W_query): Linear(in_features=768, out_features=768, bias=False)\n        (W_key): Linear(in_features=768, out_features=768, bias=False)\n        (W_value): Linear(in_features=768, out_features=768, bias=False)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.1, inplace=False)\n    )\n    (3): TransformerBlock(\n      (attention): MultiHeadCasualAttention(\n        (W_query): Linear(in_features=768, out_features=768, bias=False)\n        (W_key): Linear(in_features=768, out_features=768, bias=False)\n        (W_value): Linear(in_features=768, out_features=768, bias=False)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.1, inplace=False)\n    )\n    (4): TransformerBlock(\n      (attention): MultiHeadCasualAttention(\n        (W_query): Linear(in_features=768, out_features=768, bias=False)\n        (W_key): Linear(in_features=768, out_features=768, bias=False)\n        (W_value): Linear(in_features=768, out_features=768, bias=False)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.1, inplace=False)\n    )\n    (5): TransformerBlock(\n      (attention): MultiHeadCasualAttention(\n        (W_query): Linear(in_features=768, out_features=768, bias=False)\n        (W_key): Linear(in_features=768, out_features=768, bias=False)\n        (W_value): Linear(in_features=768, out_features=768, bias=False)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.1, inplace=False)\n    )\n    (6): TransformerBlock(\n      (attention): MultiHeadCasualAttention(\n        (W_query): Linear(in_features=768, out_features=768, bias=False)\n        (W_key): Linear(in_features=768, out_features=768, bias=False)\n        (W_value): Linear(in_features=768, out_features=768, bias=False)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.1, inplace=False)\n    )\n    (7): TransformerBlock(\n      (attention): MultiHeadCasualAttention(\n        (W_query): Linear(in_features=768, out_features=768, bias=False)\n        (W_key): Linear(in_features=768, out_features=768, bias=False)\n        (W_value): Linear(in_features=768, out_features=768, bias=False)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.1, inplace=False)\n    )\n    (8): TransformerBlock(\n      (attention): MultiHeadCasualAttention(\n        (W_query): Linear(in_features=768, out_features=768, bias=False)\n        (W_key): Linear(in_features=768, out_features=768, bias=False)\n        (W_value): Linear(in_features=768, out_features=768, bias=False)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.1, inplace=False)\n    )\n    (9): TransformerBlock(\n      (attention): MultiHeadCasualAttention(\n        (W_query): Linear(in_features=768, out_features=768, bias=False)\n        (W_key): Linear(in_features=768, out_features=768, bias=False)\n        (W_value): Linear(in_features=768, out_features=768, bias=False)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.1, inplace=False)\n    )\n    (10): TransformerBlock(\n      (attention): MultiHeadCasualAttention(\n        (W_query): Linear(in_features=768, out_features=768, bias=False)\n        (W_key): Linear(in_features=768, out_features=768, bias=False)\n        (W_value): Linear(in_features=768, out_features=768, bias=False)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.1, inplace=False)\n    )\n    (11): TransformerBlock(\n      (attention): MultiHeadCasualAttention(\n        (W_query): Linear(in_features=768, out_features=768, bias=False)\n        (W_key): Linear(in_features=768, out_features=768, bias=False)\n        (W_value): Linear(in_features=768, out_features=768, bias=False)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.1, inplace=False)\n    )\n  )\n  (final_norm): LayerNorm()\n  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n)"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"# torch.save(snapshot_model.state_dict(), 'gpt2_ddp_model_2_cpu.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T10:25:45.411812Z","iopub.execute_input":"2025-03-27T10:25:45.412162Z","iopub.status.idle":"2025-03-27T10:25:45.415563Z","shell.execute_reply.started":"2025-03-27T10:25:45.412135Z","shell.execute_reply":"2025-03-27T10:25:45.414499Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# device = next(snapshot_model.parameters()).device\n# print(f\"Model is on: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T10:25:48.242425Z","iopub.execute_input":"2025-03-27T10:25:48.242769Z","iopub.status.idle":"2025-03-27T10:25:48.246087Z","shell.execute_reply.started":"2025-03-27T10:25:48.242743Z","shell.execute_reply":"2025-03-27T10:25:48.245183Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"tokenizer = tiktoken.get_encoding(\"gpt2\")\n\ntoken_ids = generate(\n    model=snapshot_model,\n    idx=text_to_token_ids(\"logic to return if a number is prime or not: \", tokenizer),\n    max_new_tokens=256,\n    context_size=GPT_CONFIG_124M[\"context_length\"]\n)\n\nprint(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T10:25:51.137626Z","iopub.execute_input":"2025-03-27T10:25:51.138027Z","iopub.status.idle":"2025-03-27T10:27:00.753007Z","shell.execute_reply.started":"2025-03-27T10:25:51.137991Z","shell.execute_reply":"2025-03-27T10:27:00.752152Z"}},"outputs":[{"name":"stdout","text":"Output text:\n logic to return if a number is prime or not:  ```python\ndef is_prime(n):\n if n == 1:\n return False\n for i in range(2, int(n**0.5) + 1):\n if n % i == 0:\n return False\n return True\n\nnum = int(input(\"Enter a number: \"))\nprint(\"The number is\", is_prime(num))\n```<|endoftext|>Implement a basic chatbot in Python using the NLTK library for a library  ```python\nimport json\nimport nltk\nfrom nltk.tokenize import word_tokenize\n\n# load the user response\ndef get_response(user_input):\n    response = get_response(user_input=True)\n    if response.status_code == 200:\n        return 'Error: Failed message'\n    else:\n        return 'Not a response message'\n\n# respond to get user input\ndef respond(user_input):\n      if user_input in responses:\n        return 'I'm sorry, I don't understand.'\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"#","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T13:41:41.693701Z","iopub.execute_input":"2025-03-25T13:41:41.694036Z","iopub.status.idle":"2025-03-25T13:41:41.697881Z","shell.execute_reply.started":"2025-03-25T13:41:41.694006Z","shell.execute_reply":"2025-03-25T13:41:41.696771Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_model_whole(model, train_loader, val_loader, device, eval_iter):\n    \n    model.eval()\n    with torch.no_grad():\n        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n            \n    model.train()\n    return train_loss, val_loss\n\ndef calc_loss_batch(input_batch, target_batch, model, device):\n    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n    logits = model(input_batch)\n    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n    return loss\n\n\ndef calc_loss_loader(data_loader, model, device, num_batches=None):\n    total_loss = 0.\n\n    if len(data_loader) == 0:\n        return float(\"nan\")\n    elif num_batches is None:\n        num_batches = len(data_loader)\n    else:\n        # Reduce the number of batches to match the total number of batches in the data loader\n        # if num_batches exceeds the number of batches in the data loader\n        num_batches = min(num_batches, len(data_loader))\n\n    print(num_batches)\n    for i, (input_batch, target_batch) in enumerate(data_loader):\n        if i < num_batches:\n            loss = calc_loss_batch(input_batch.reshape(1, 512), target_batch.reshape(1, 512), model, device)\n            total_loss += loss.item()\n        else:\n            break\n    return total_loss / num_batches","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T14:32:26.144712Z","iopub.execute_input":"2025-03-25T14:32:26.145094Z","iopub.status.idle":"2025-03-25T14:32:26.152217Z","shell.execute_reply.started":"2025-03-25T14:32:26.145061Z","shell.execute_reply":"2025-03-25T14:32:26.151308Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, txt, tokenizer, max_length, stride):\n        self.input_ids = []\n        self.target_ids = []\n\n        # Tokenize the entire text\n        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n\n        # Use a sliding window to chunk the book into overlapping sequences of max_length\n        for i in range(0, len(token_ids) - max_length, stride):\n            input_chunk = token_ids[i:i + max_length]\n            target_chunk = token_ids[i + 1: i + max_length + 1]\n            self.input_ids.append(torch.tensor(input_chunk))\n            self.target_ids.append(torch.tensor(target_chunk))\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return self.input_ids[idx], self.target_ids[idx]\n    \ndef create_dataloader_v1(txt, batch_size=4, max_length=256,\n                         stride=128, shuffle=True, drop_last=True, num_workers=0):\n    # Initialize the tokenizer\n    tokenizer = tiktoken.get_encoding(\"gpt2\")\n\n    # Create dataset\n    dataset = CustomDataset(txt, tokenizer, max_length, stride)\n\n    # Create dataloader\n    dataloader = DataLoader(\n        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T14:14:27.149420Z","iopub.execute_input":"2025-03-25T14:14:27.149734Z","iopub.status.idle":"2025-03-25T14:14:27.156823Z","shell.execute_reply.started":"2025-03-25T14:14:27.149710Z","shell.execute_reply":"2025-03-25T14:14:27.155976Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T13:53:01.918928Z","iopub.execute_input":"2025-03-25T13:53:01.919293Z","iopub.status.idle":"2025-03-25T13:53:01.923362Z","shell.execute_reply.started":"2025-03-25T13:53:01.919264Z","shell.execute_reply":"2025-03-25T13:53:01.922302Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\n\ntrain_dataset = load_dataset('flytech/python-codes-25k', split='train')\ndataset = train_dataset.map(lambda example: {'text': example['instruction'] + ' ' + example['input'] + ' ' + example['output']})['text']\n\ncombined_dataset = \"<|endoftext|>\".join(dataset[:])\n\ntrain_ratio = 0.90\nsplit_idx = int(train_ratio * len(combined_dataset))\ntrain_data = combined_dataset[:split_idx]\nval_data = combined_dataset[split_idx:]\n\n\ntorch.manual_seed(123)\n\ntrain_loader = create_dataloader_v1(\n    train_data,\n    batch_size=16,\n    max_length=GPT_CONFIG_124M[\"context_length\"],\n    stride=GPT_CONFIG_124M[\"context_length\"],\n    drop_last=True,\n    shuffle=True,\n    num_workers=0\n)\n\nval_loader = create_dataloader_v1(\n    val_data,\n    batch_size=16,\n    max_length=GPT_CONFIG_124M[\"context_length\"],\n    stride=GPT_CONFIG_124M[\"context_length\"],\n    drop_last=False,\n    shuffle=False,\n    num_workers=0\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T14:16:54.629475Z","iopub.execute_input":"2025-03-25T14:16:54.629820Z","iopub.status.idle":"2025-03-25T14:17:02.173153Z","shell.execute_reply.started":"2025-03-25T14:16:54.629792Z","shell.execute_reply":"2025-03-25T14:17:02.172207Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(train_loader), len(val_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T14:15:31.529785Z","iopub.execute_input":"2025-03-25T14:15:31.530025Z","iopub.status.idle":"2025-03-25T14:15:31.535503Z","shell.execute_reply.started":"2025-03-25T14:15:31.530004Z","shell.execute_reply":"2025-03-25T14:15:31.534740Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T14:17:15.815650Z","iopub.execute_input":"2025-03-25T14:17:15.815980Z","iopub.status.idle":"2025-03-25T14:17:15.821655Z","shell.execute_reply.started":"2025-03-25T14:17:15.815955Z","shell.execute_reply":"2025-03-25T14:17:15.820644Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\ntrain_losses, val_losses = evaluate_model(\n                    snapshot_model.to(device), \n                    val_loader, \n                    val_loader, \n                    device, \n                    eval_iter=None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T14:17:44.710284Z","iopub.execute_input":"2025-03-25T14:17:44.710600Z","iopub.status.idle":"2025-03-25T14:20:26.335890Z","shell.execute_reply.started":"2025-03-25T14:17:44.710577Z","shell.execute_reply":"2025-03-25T14:20:26.335004Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_losses","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T14:20:39.839332Z","iopub.execute_input":"2025-03-25T14:20:39.839663Z","iopub.status.idle":"2025-03-25T14:20:39.844987Z","shell.execute_reply.started":"2025-03-25T14:20:39.839639Z","shell.execute_reply":"2025-03-25T14:20:39.844047Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\n\nnum_epochs = 10\ndef plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n    fig, ax1 = plt.subplots(figsize=(5, 3))\n\n    # Plot training and validation loss against epochs\n    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n    ax1.set_xlabel(\"Epochs\")\n    ax1.set_ylabel(\"Loss\")\n    ax1.legend(loc=\"upper right\")\n    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n\n    # Create a second x-axis for tokens seen\n    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n    ax2.set_xlabel(\"Tokens seen\")\n\n    fig.tight_layout()  # Adjust layout to make room\n    plt.savefig(\"loss-plot.pdf\")\n    plt.show()\n\nepochs_tensor = torch.linspace(0, num_epochs, len(train_loss))\nplot_losses(epochs_tensor, tokens_seen, train_loss, val_loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T14:03:20.875761Z","iopub.execute_input":"2025-03-25T14:03:20.876144Z","iopub.status.idle":"2025-03-25T14:03:20.892332Z","shell.execute_reply.started":"2025-03-25T14:03:20.876081Z","shell.execute_reply":"2025-03-25T14:03:20.890986Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T14:03:28.946198Z","iopub.execute_input":"2025-03-25T14:03:28.946529Z","iopub.status.idle":"2025-03-25T14:03:28.951919Z","shell.execute_reply.started":"2025-03-25T14:03:28.946505Z","shell.execute_reply":"2025-03-25T14:03:28.950948Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom torch.cuda.amp import autocast\n\ndef evaluate(model, val_loader, device, context_size, loss_fn=F.cross_entropy):\n    model.eval()  # Set model to evaluation mode\n    total_loss = 0.0\n    total_tokens = 0\n    \n    with torch.no_grad():  # Disable gradient computation\n        for batch_idx, (inputs, targets) in enumerate(val_loader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            \n            # Ensure inputs respect context size\n            if inputs.size(1) > context_size:\n                inputs = inputs[:, -context_size:]\n                targets = targets[:, -context_size:]\n            \n            # Forward pass with mixed precision (optional)\n            with autocast():\n                logits = model(inputs)  # Shape: [batch_size, seq_len, vocab_size]\n                loss = loss_fn(logits.view(-1, logits.size(-1)), targets.view(-1))\n            \n            total_loss += loss.item() * targets.numel()  # Weighted by number of tokens\n            total_tokens += targets.numel()\n    \n    # Compute average loss and perplexity\n    avg_loss = total_loss / total_tokens\n    perplexity = torch.exp(torch.tensor(avg_loss)).item()\n    \n    model.train()  # Switch back to training mode\n    return avg_loss, perplexity\n\n# Example usage\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = snapshot_model.to(device)\n\n# Assuming you have a validation DataLoader (val_loader)\n# val_loader could contain tokenized sequences (inputs, targets)\navg_loss, perplexity = evaluate(\n    model=model,\n    val_loader=val_loader,\n    device=device,\n    context_size=512  # Adjust to your model's context size\n)\nprint(f\"Validation Loss: {avg_loss:.4f}, Perplexity: {perplexity:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Generate","metadata":{}},{"cell_type":"code","source":"import tiktoken\ndef text_to_token_ids(text, tokenizer):\n    # encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n    return encoded_tensor\n\ndef token_ids_to_text(token_ids, tokenizer):\n    flat = token_ids.squeeze(0) # remove batch dimension\n    return tokenizer.decode(flat.tolist())\n\ntokenizer = tiktoken.get_encoding(\"gpt2\")\n\ndef generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    # For-loop is the same as before: Get logits, and only focus on last time step\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -context_size:]\n        with torch.no_grad():\n            logits = model(idx_cond)\n        logits = logits[:, -1, :]\n\n        # New: Filter logits with top_k sampling\n        if top_k is not None:\n            # Keep only top_k values\n            top_logits, _ = torch.topk(logits, top_k)\n            min_val = top_logits[:, -1]\n            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n\n        # New: Apply temperature scaling\n        if temperature > 0.0:\n            logits = logits / temperature\n\n            # Apply softmax to get probabilities\n            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n\n            # Sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n\n        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n        else:\n            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n\n        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n            break\n\n        # Same as before: append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n\n    return idx\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T16:51:55.722612Z","iopub.execute_input":"2025-03-29T16:51:55.722940Z","iopub.status.idle":"2025-03-29T16:51:56.867360Z","shell.execute_reply.started":"2025-03-29T16:51:55.722919Z","shell.execute_reply":"2025-03-29T16:51:56.866441Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import torch\n# If the `previous_chapters.py` file is not available locally,\n# you can import it from the `llms-from-scratch` PyPI package.\n# For details, see: https://github.com/rasbt/LLMs-from-scratch/tree/main/pkg\n# E.g.,\n# from llms_from_scratch.ch04 import GPTModel\n\nGPT_CONFIG_124M = {\n    \"vocab_size\": 50257,   # Vocabulary size\n    \"context_length\": 512, # Shortened context length (orig: 1024)\n    \"emb_dim\": 768,        # Embedding dimension\n    \"n_heads\": 12,         # Number of attention heads\n    \"n_layers\": 12,        # Number of layers\n    \"drop_rate\": 0.1,      # Dropout rate\n    \"qkv_bias\": False      # Query-key-value bias\n}\n\ntorch.manual_seed(123)\n# model = GPTModel(GPT_CONFIG_124M)\n# model.eval();  # Disable dropout during inference","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nclass GELU(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return 0.5 * x * (1 + torch.tanh(\n            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n            (x + 0.044715 * torch.pow(x, 3))\n        ))\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n            GELU(),\n            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.attention = MultiHeadCasualAttention(\n            d_in= cfg[\"emb_dim\"],\n            d_out=cfg[\"emb_dim\"],\n            context_length=cfg[\"context_length\"],\n            dropout=cfg[\"drop_rate\"],\n            num_heads=cfg[\"n_heads\"],\n            qkv_bias=cfg[\"qkv_bias\"]\n        )\n\n        self.ff = FeedForward(cfg)\n        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n\n    def forward(self, x):\n        # This block does nothing and just returns its input.\n        shortcut = x\n        x = self.norm1(x)\n        x = self.attention(x)\n        x = self.drop_shortcut(x)\n\n        x = x + shortcut\n\n        shortcut = x\n\n        x = self.norm2(x)\n        x = self.ff(x)\n        x = self.drop_shortcut(x)\n        x = x + shortcut\n        return x\n\n\nclass LayerNorm(nn.Module):\n    def __init__(self, emb_dim, eps=1e-5):\n        super().__init__()\n        self.eps = eps\n        self.scale = nn.Parameter(torch.ones(emb_dim))\n        self.shift = nn.Parameter(torch.zeros(emb_dim))\n        \n\n    def forward(self, x):\n        # This layer does nothing and just returns its input.\n\n        mean = x.mean(dim = -1, keepdim = True)\n        var = x.var(dim=-1,keepdim = True, unbiased = False)\n        norm_x = (x-mean)/torch.sqrt(var+self.eps)\n        return self.scale * norm_x + self.shift\n\nclass MultiHeadCasualAttention(nn.Module):\n    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n        super().__init__()\n        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n\n        self.d_out = d_out\n        self.num_heads = num_heads\n        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n\n    def forward(self, x):\n        b, num_tokens, d_in = x.shape\n\n        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n        queries = self.W_query(x)\n        values = self.W_value(x)\n\n        # We implicitly split the matrix by adding a `num_heads` dimension\n        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n\n        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n        keys = keys.transpose(1, 2)\n        queries = queries.transpose(1, 2)\n        values = values.transpose(1, 2)\n\n        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n\n        # Original mask truncated to the number of tokens and converted to boolean\n        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n\n        # Use the mask to fill attention scores\n        attn_scores.masked_fill_(mask_bool, -torch.inf)\n\n        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n\n        # Shape: (b, num_tokens, num_heads, head_dim)\n        context_vec = (attn_weights @ values).transpose(1, 2)\n\n        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n        context_vec = self.out_proj(context_vec)  # optional projection\n\n        return context_vec\n\n\n\nclass GPTModel(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n\n        self.trf_blocks = nn.Sequential(\n            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n        self.out_head = nn.Linear(\n            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n        )\n\n    def forward(self, in_idx):\n        \n        batch_size, seq_len = in_idx.shape\n        tok_embeds = self.tok_emb(in_idx)\n        pos_embeds = self.pos_emb(torch.arange(seq_len, device = in_idx.device))\n        x = tok_embeds + pos_embeds\n        x = self.drop_emb(x)\n        x = self.trf_blocks(x)\n        x = self.final_norm(x)\n\n        logits = self.out_head(x)\n        return logits\n\nsnapshot_model = GPTModel(GPT_CONFIG_124M)\nsnapshot_model.load_state_dict(torch.load(\"/kaggle/input/gpt2-cpu/pytorch/default/1/gpt2_ddp_model_2_cpu.pth\", weights_only=True))\n# snapshot_model.eval()\ndevice = torch.device(\"cpu\")\nsnapshot_model.to(device)\nprint()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T16:53:45.173163Z","iopub.execute_input":"2025-03-29T16:53:45.173510Z","iopub.status.idle":"2025-03-29T16:53:50.855775Z","shell.execute_reply.started":"2025-03-29T16:53:45.173488Z","shell.execute_reply":"2025-03-29T16:53:50.854912Z"}},"outputs":[{"name":"stdout","text":"\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# snapshot_model = GPTModel(GPT_CONFIG_124M)\n\n# snapshot_model.load_state_dict(torch.load(\"/kaggle/working/gpt2_ddp_model_2_cpu.pth\", weights_only=True))\n# # snapshot_model.eval()\n# device = torch.device(\"cpu\")\n# snapshot_model.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# token_ids = generate(\n#     model=snapshot_model,\n#     idx=text_to_token_ids(\"logic to return if a number is prime or not: \", tokenizer),\n#     max_new_tokens=256,\n#     context_size=GPT_CONFIG_124M[\"context_length\"]\n# )\n\n# print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\nstart = time.time()\n\ntoken_ids = generate(\n    model=snapshot_model,\n    idx=text_to_token_ids(\"logic to return if a number is prime or not: \", tokenizer),\n    max_new_tokens=250,\n    context_size=GPT_CONFIG_124M[\"context_length\"],\n    top_k=5,\n    temperature=0\n)\nend = time.time()\nprint(\"Time taken: \", end-start)\nprint(\"Tokens/s: \", 250/(end-start))\nprint(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T16:55:39.484373Z","iopub.execute_input":"2025-03-29T16:55:39.484734Z","iopub.status.idle":"2025-03-29T16:56:47.319445Z","shell.execute_reply.started":"2025-03-29T16:55:39.484707Z","shell.execute_reply":"2025-03-29T16:56:47.318451Z"}},"outputs":[{"name":"stdout","text":"Time taken:  67.82775211334229\nTokens/s:  3.685806947903009\nOutput text:\n logic to return if a number is prime or not:  ```python\ndef is_prime(num):\n    if num <= 1:\n        return False\n    for i in range(2, int(num**0.5)+1):\n        if num % i == 0:\n            return False\n    return True\n```<|endoftext|>Create a Python script to generate a histogram given a set of data [2, 5, 8, 10, 3, 9]  ```python\nimport matplotlib.pyplot as plt\n\ndef histogram(data):\n    plt.hist(data)\n    plt.xlabel('Index')\n    plt.ylabel('Frequency')\n    plt.title('Frequency')\n    plt.show()\n\ndata = [2, 5, 8, 10, 3, 9]\nplt.plot(data)\n```<|endoftext|>Create a basic web-based web server in Python that can serve static pages from a given URL  ```python\n\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}