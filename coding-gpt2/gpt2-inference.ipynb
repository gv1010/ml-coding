{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T13:58:30.953572Z",
     "iopub.status.busy": "2025-03-29T13:58:30.953259Z",
     "iopub.status.idle": "2025-03-29T13:58:34.338479Z",
     "shell.execute_reply": "2025-03-29T13:58:34.337624Z",
     "shell.execute_reply.started": "2025-03-29T13:58:30.953544Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T13:58:34.339888Z",
     "iopub.status.busy": "2025-03-29T13:58:34.339455Z",
     "iopub.status.idle": "2025-03-29T13:58:34.348533Z",
     "shell.execute_reply": "2025-03-29T13:58:34.347641Z",
     "shell.execute_reply.started": "2025-03-29T13:58:34.339856Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T13:58:34.350473Z",
     "iopub.status.busy": "2025-03-29T13:58:34.350237Z",
     "iopub.status.idle": "2025-03-29T13:58:34.372445Z",
     "shell.execute_reply": "2025-03-29T13:58:34.371637Z",
     "shell.execute_reply.started": "2025-03-29T13:58:34.350454Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadCasualAttention(\n",
    "            d_in= cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This block does nothing and just returns its input.\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attention(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "        return x\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # This layer does nothing and just returns its input.\n",
    "\n",
    "        mean = x.mean(dim = -1, keepdim = True)\n",
    "        var = x.var(dim=-1,keepdim = True, unbiased = False)\n",
    "        norm_x = (x-mean)/torch.sqrt(var+self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "class MultiHeadCasualAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # optional projection\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        \n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device = in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T13:58:34.374085Z",
     "iopub.status.busy": "2025-03-29T13:58:34.373853Z",
     "iopub.status.idle": "2025-03-29T13:58:34.384830Z",
     "shell.execute_reply": "2025-03-29T13:58:34.384077Z",
     "shell.execute_reply.started": "2025-03-29T13:58:34.374052Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 512, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T13:58:34.385933Z",
     "iopub.status.busy": "2025-03-29T13:58:34.385671Z",
     "iopub.status.idle": "2025-03-29T13:58:42.219400Z",
     "shell.execute_reply": "2025-03-29T13:58:42.218587Z",
     "shell.execute_reply.started": "2025-03-29T13:58:34.385906Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gv/envs/ml/lib/python3.11/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(512, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attention): MultiHeadCasualAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attention): MultiHeadCasualAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attention): MultiHeadCasualAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attention): MultiHeadCasualAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attention): MultiHeadCasualAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attention): MultiHeadCasualAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attention): MultiHeadCasualAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attention): MultiHeadCasualAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (attention): MultiHeadCasualAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (attention): MultiHeadCasualAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (attention): MultiHeadCasualAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (attention): MultiHeadCasualAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snapshot_model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "# /kaggle/input/py-gpt2/pytorch/default/1/gpt2_ddp_model_1.pth\n",
    "\n",
    "snapshot_model.load_state_dict(torch.load(\"/home/gv/Downloads/apps/gpt2_ddp_model_2_cpu.pth\", weights_only=True))\n",
    "# snapshot_model.eval()\n",
    "device = torch.device(\"cpu\")\n",
    "snapshot_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T13:58:42.220780Z",
     "iopub.status.busy": "2025-03-29T13:58:42.220408Z",
     "iopub.status.idle": "2025-03-29T13:58:42.258242Z",
     "shell.execute_reply": "2025-03-29T13:58:42.257589Z",
     "shell.execute_reply.started": "2025-03-29T13:58:42.220736Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Custom Dataset (assuming you have this)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tiktoken\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "    \n",
    "def create_dataloader_v1(txt, batch_size=8, max_length=256,\n",
    "                         stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = CustomDataset(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def load_train_val_dataset():\n",
    "    from datasets import load_dataset\n",
    "\n",
    "    train_dataset = load_dataset('flytech/python-codes-25k', split='train')\n",
    "    # train_dataset = load_dataset('flytech/python-codes-25k', split='test')\n",
    "\n",
    "    # One can map the dataset in any way, for the sake of example:\n",
    "    dataset = train_dataset.map(lambda example: {'text': example['instruction'] + ' ' + example['input'] + ' ' + example['output']})['text']\n",
    "    # Remember that you don't need to map if the dataset has a \"text\" field already:)\n",
    "    train_ratio = 0.90\n",
    "    # text_data = \"<|endoftext|>\".join(dataset[:50])\n",
    "    dataset = dataset[:]\n",
    "    split_idx = int(train_ratio * len(dataset))\n",
    "\n",
    "    train_data = dataset[:split_idx]\n",
    "    val_data = dataset[split_idx:]\n",
    "    train_data = \"<|endoftext|>\".join(train_data[:])\n",
    "    val_data = \"<|endoftext|>\".join(val_data[:])\n",
    "    # print(\"split_idx: \", split_idx)\n",
    "\n",
    "    torch.manual_seed(123)\n",
    "\n",
    "    train_loader = create_dataloader_v1(\n",
    "        train_data,\n",
    "        batch_size=8,\n",
    "        max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "        stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "        drop_last=True,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    val_loader = create_dataloader_v1(\n",
    "        val_data,\n",
    "        batch_size=8,\n",
    "        max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "        stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "        drop_last=False,\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    print(\"length of dataset\", len(train_data), len(val_data))\n",
    "    # print(\"length of loader\", len(train_loader), len(val_loader))\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T13:58:42.259323Z",
     "iopub.status.busy": "2025-03-29T13:58:42.259038Z",
     "iopub.status.idle": "2025-03-29T13:58:42.265001Z",
     "shell.execute_reply": "2025-03-29T13:58:42.264226Z",
     "shell.execute_reply.started": "2025-03-29T13:58:42.259291Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    try:\n",
    "        input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_batch)\n",
    "            loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "        del input_batch, target_batch, logits\n",
    "        torch.cuda.empty_cache()\n",
    "        return loss\n",
    "    except Exception as exp:\n",
    "        print(exp)\n",
    "        return 0\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T13:58:42.267236Z",
     "iopub.status.busy": "2025-03-29T13:58:42.267050Z",
     "iopub.status.idle": "2025-03-29T13:58:56.138104Z",
     "shell.execute_reply": "2025-03-29T13:58:56.137237Z",
     "shell.execute_reply.started": "2025-03-29T13:58:42.267219Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9cf3cc3e258476d9ae290c105eed104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/3.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35946fd0023542b5a40140e6b3d39b24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/26.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79fa7308fe16467a9048883ef5032074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/25.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa259c791f184ee6a3f3765d6d01d9fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/49626 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b0fc3f93b014e91a73f29e850e91bce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/49626 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset 19989246 3200684\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = load_train_val_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T17:14:00.134801Z",
     "iopub.status.busy": "2025-03-27T17:14:00.134464Z",
     "iopub.status.idle": "2025-03-27T17:14:00.140832Z",
     "shell.execute_reply": "2025-03-27T17:14:00.139663Z",
     "shell.execute_reply.started": "2025-03-27T17:14:00.134771Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T13:58:56.139884Z",
     "iopub.status.busy": "2025-03-29T13:58:56.139597Z",
     "iopub.status.idle": "2025-03-29T13:58:56.143381Z",
     "shell.execute_reply": "2025-03-29T13:58:56.142445Z",
     "shell.execute_reply.started": "2025-03-29T13:58:56.139860Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 220 * 512 * 2 * 8 * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T17:19:02.362238Z",
     "iopub.status.busy": "2025-03-27T17:19:02.361903Z",
     "iopub.status.idle": "2025-03-27T17:33:04.793896Z",
     "shell.execute_reply": "2025-03-27T17:33:04.793001Z",
     "shell.execute_reply.started": "2025-03-27T17:19:02.362213Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# total_train_loss = 0\n",
    "# train_idx = 0\n",
    "# snapshot_model.eval()\n",
    "\n",
    "# for input_batch, target_batch in tqdm(train_loader, total = len(train_loader)):  \n",
    "#     train_loss = calc_loss_batch(input_batch, target_batch, snapshot_model, device=torch.device(\"cuda\"))\n",
    "#     train_idx += 1\n",
    "#     total_train_loss += (train_loss.item())\n",
    "#     del input_batch, target_batch\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# # if val_idx:\n",
    "# # val_loss = total_val_loss/(val_idx)\n",
    "# # validation_loss.append(val_loss)\n",
    "# # print(f'Validation Loss: {val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T13:58:56.144582Z",
     "iopub.status.busy": "2025-03-29T13:58:56.144279Z",
     "iopub.status.idle": "2025-03-29T13:58:56.365181Z",
     "shell.execute_reply": "2025-03-29T13:58:56.364414Z",
     "shell.execute_reply.started": "2025-03-29T13:58:56.144558Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def validate_model(model, val_loader ):\n",
    "\n",
    "    total_val_loss = 0\n",
    "    val_idx = 0\n",
    "    val_loss_list = []\n",
    "    model.eval()\n",
    "    \n",
    "    for input_batch, target_batch in tqdm(val_loader, total = len(val_loader)):\n",
    "        val_loss = calc_loss_batch(input_batch, target_batch, model, device=torch.device(\"cpu\"))\n",
    "        val_idx += 1\n",
    "        total_val_loss += (val_loss)\n",
    "        val_loss_list.append(val_loss)\n",
    "        # break\n",
    "\n",
    "    print(\"Val loss: \", total_val_loss/val_idx)\n",
    "    return val_loss_list, total_val_loss/val_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T17:40:31.731011Z",
     "iopub.status.busy": "2025-03-27T17:40:31.730687Z",
     "iopub.status.idle": "2025-03-27T17:40:31.734570Z",
     "shell.execute_reply": "2025-03-27T17:40:31.733750Z",
     "shell.execute_reply.started": "2025-03-27T17:40:31.730972Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# next(iter(train_loader))[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T13:58:56.366253Z",
     "iopub.status.busy": "2025-03-29T13:58:56.365965Z",
     "iopub.status.idle": "2025-03-29T13:58:56.375994Z",
     "shell.execute_reply": "2025-03-29T13:58:56.375162Z",
     "shell.execute_reply.started": "2025-03-29T13:58:56.366230Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_model_size(model):\n",
    "    \"\"\"\n",
    "    Calculates the total size of a PyTorch model in bytes.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The PyTorch model.\n",
    "\n",
    "    Returns:\n",
    "        int: The total size of the model in bytes.\n",
    "    \"\"\"\n",
    "    total_params = 0\n",
    "    for param in model.parameters():\n",
    "        total_params += param.numel() * param.element_size()\n",
    "    return format_size(total_params)\n",
    "\n",
    "def format_size(size_bytes):\n",
    "    \"\"\"\n",
    "    Formats a size in bytes into a human-readable format (KB, MB, GB).\n",
    "\n",
    "    Args:\n",
    "        size_bytes (int): The size in bytes.\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted size.\n",
    "    \"\"\"\n",
    "    if size_bytes < 1024:\n",
    "        return f\"{size_bytes} bytes\"\n",
    "    elif size_bytes < (1024 * 1024):\n",
    "        return f\"{size_bytes / 1024:.2f} KB\"\n",
    "    elif size_bytes < (1024 * 1024 * 1024):\n",
    "        return f\"{size_bytes / (1024 * 1024):.2f} MB\"\n",
    "    else:\n",
    "        return f\"{size_bytes / (1024 * 1024 * 1024):.2f} GB\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantized Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T13:58:56.379590Z",
     "iopub.status.busy": "2025-03-29T13:58:56.379306Z",
     "iopub.status.idle": "2025-03-29T13:58:56.390556Z",
     "shell.execute_reply": "2025-03-29T13:58:56.389731Z",
     "shell.execute_reply.started": "2025-03-29T13:58:56.379561Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'620.33 MB'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_model_size(snapshot_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T13:58:59.785758Z",
     "iopub.status.busy": "2025-03-29T13:58:59.785434Z",
     "iopub.status.idle": "2025-03-29T13:59:01.296706Z",
     "shell.execute_reply": "2025-03-29T13:59:01.296028Z",
     "shell.execute_reply.started": "2025-03-29T13:58:59.785731Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.quantization import quantize_dynamic\n",
    "model_quantized = quantize_dynamic(\n",
    "    model=snapshot_model, qconfig_spec={nn.Linear}, dtype=torch.qint8, inplace=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T13:59:03.876498Z",
     "iopub.status.busy": "2025-03-29T13:59:03.876217Z",
     "iopub.status.idle": "2025-03-29T13:59:03.882166Z",
     "shell.execute_reply": "2025-03-29T13:59:03.881280Z",
     "shell.execute_reply.started": "2025-03-29T13:59:03.876476Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'148.88 MB'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_model_size(model_quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T14:12:29.421067Z",
     "iopub.status.busy": "2025-03-29T14:12:29.420747Z",
     "iopub.status.idle": "2025-03-29T14:12:30.581480Z",
     "shell.execute_reply": "2025-03-29T14:12:30.580502Z",
     "shell.execute_reply.started": "2025-03-29T14:12:29.421043Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(model_quantized.to(torch.device(\"cpu\")).state_dict(), \"GPT2_quantize_1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T14:09:28.470192Z",
     "iopub.status.busy": "2025-03-29T14:09:28.469892Z",
     "iopub.status.idle": "2025-03-29T14:09:28.483803Z",
     "shell.execute_reply": "2025-03-29T14:09:28.482927Z",
     "shell.execute_reply.started": "2025-03-29T14:09:28.470169Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model_quantized.to(device)\n",
    "snapshot_model.to(device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### validation of GPT2 Dynamic Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T14:09:31.622899Z",
     "iopub.status.busy": "2025-03-29T14:09:31.622555Z",
     "iopub.status.idle": "2025-03-29T14:11:05.102223Z",
     "shell.execute_reply": "2025-03-29T14:11:05.101323Z",
     "shell.execute_reply.started": "2025-03-29T14:09:31.622870Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 220/220 [47:43<00:00, 13.01s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  tensor(1.3223)\n",
      "Base Model Val Loss:  tensor(1.3223)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_, snapshot_loss = validate_model(snapshot_model, val_loader )\n",
    "print(\"Base Model Val Loss: \", snapshot_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T14:11:41.434431Z",
     "iopub.status.busy": "2025-03-29T14:11:41.434159Z",
     "iopub.status.idle": "2025-03-29T14:11:41.692166Z",
     "shell.execute_reply": "2025-03-29T14:11:41.691431Z",
     "shell.execute_reply.started": "2025-03-29T14:11:41.434409Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 220/220 [38:54<00:00, 10.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  tensor(1.3350)\n",
      "Quantized Model Val Loss:  tensor(1.3350)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_, quant_model_loss = validate_model(model_quantized, val_loader )\n",
    "print(\"Quantized Model Val Loss: \", quant_model_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T13:55:51.372818Z",
     "iopub.status.busy": "2025-03-29T13:55:51.372319Z",
     "iopub.status.idle": "2025-03-29T13:55:51.377296Z",
     "shell.execute_reply": "2025-03-29T13:55:51.375816Z",
     "shell.execute_reply.started": "2025-03-29T13:55:51.372781Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# snapshot_model.qconfig = torch.ao.quantization.default_qconfig\n",
    "# net_quantized = torch.ao.quantization.prepare(snapshot_model) # Insert observers\n",
    "# net_quantized = torch.ao.quantization.convert(net_quantized)\n",
    "\n",
    "\n",
    "# model_size_bytes = calculate_model_size(net_quantized)\n",
    "# model_size_formatted = format_size(model_size_bytes)\n",
    "\n",
    "# print(f\"Total size of the model: {model_size_formatted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T17:55:10.793850Z",
     "iopub.status.busy": "2025-03-27T17:55:10.793542Z",
     "iopub.status.idle": "2025-03-27T17:56:49.440392Z",
     "shell.execute_reply": "2025-03-27T17:56:49.439400Z",
     "shell.execute_reply.started": "2025-03-27T17:55:10.793827Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Define the quantization configuration\n",
    "# model_old = snapshot_model\n",
    "# snapshot_model.to(torch.device(\"cpu\"))\n",
    "# quantization_config = torch.ao.quantization.get_default_qconfig('fbgemm')\n",
    "# snapshot_model.config.torch_dtype = torch.float32 # Ensure original dtype is float32\n",
    "\n",
    "# # Prepare the model for static quantization\n",
    "# prepared_model = torch.ao.quantization.prepare(snapshot_model, quantization_config)\n",
    "\n",
    "# # quantized_model = torch.quantization.quantize_dynamic(\n",
    "# #     snapshot_model.to(torch.device(\"cpu\")), {torch.nn.Linear}, dtype=torch.qint8\n",
    "# # )\n",
    "\n",
    "# quantized_model = torch.ao.quantization.convert(prepared_model, inplace=False)\n",
    "\n",
    "# torch.save(quantized_model.state_dict(), '/kaggle/working/quantized_8b_gpt2_model.pth')\n",
    "\n",
    "# val_loss_list, val_loss = validate_model(quantized_model.to(torch.device(\"cuda\")), val_loader)\n",
    "# print(f\"Quantized Average Loss: {val_loss}\")\n",
    "# # print(f\"Quantized Perplexity: {quantized_perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T17:58:10.321247Z",
     "iopub.status.busy": "2025-03-27T17:58:10.320909Z",
     "iopub.status.idle": "2025-03-27T17:58:10.326517Z",
     "shell.execute_reply": "2025-03-27T17:58:10.325509Z",
     "shell.execute_reply.started": "2025-03-27T17:58:10.321221Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# quantization_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T17:57:56.152934Z",
     "iopub.status.busy": "2025-03-27T17:57:56.152548Z",
     "iopub.status.idle": "2025-03-27T17:57:56.185436Z",
     "shell.execute_reply": "2025-03-27T17:57:56.184498Z",
     "shell.execute_reply.started": "2025-03-27T17:57:56.152907Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# for name, param in quantized_model.named_parameters():\n",
    "#     print(f\"Parameter name: {name}, Data type: {param.data.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    # encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:  84.72966575622559\n",
      "Tokens/s:  2.950560441478326\n",
      "Output text:\n",
      " Python program to calculate the average age of a group of people people = [\n",
      " {'name': 'John', 'age': 30},\n",
      " {'name': 'Jane', 'age': 28},\n",
      " {'name': 'Jane', 'age': 28}\n",
      "]  ```python\n",
      "def calculate_average_age(grades):\n",
      " total = 0\n",
      " for record in people:\n",
      " total += record['age']\n",
      " return total / len(people)\n",
      "\n",
      "people = [{'name': 'John', 'age': 30},\n",
      " {'name': 'Jane', 'age': 25},\n",
      " {'name': 'James', 'age': 28}\n",
      "]\n",
      "\n",
      "total = calculate_average_age(people)\n",
      "print(total) # Output: 60\n",
      "```<|endoftext|>Create a Python program to identify if the given string contains any of the characters such that the string contains at least two numbers: 6, 8  ```python\n",
      "def is_two_numbers(string):\n",
      "    \"\"\"\n",
      "    This function checks if two strings are anagrams or not\n",
      "    \n",
      "    Parameters:\n",
      "    string (str): the string\n",
      "   \n",
      " \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "token_ids = generate(\n",
    "    model=snapshot_model,\n",
    "    idx=text_to_token_ids(\"Python program to calculate the average\", tokenizer),\n",
    "    max_new_tokens=250,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=5,\n",
    "    temperature=0\n",
    ")\n",
    "end = time.time()\n",
    "print(\"Time taken: \", end-start)\n",
    "print(\"Tokens/s: \", 250/(end-start))\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:  62.362414598464966\n",
      "Tokens/s:  4.0088248925203365\n",
      "Output text:\n",
      " Python program to calculate the average age of a person based on their weight and weight in the school  ```python\n",
      "def calculate_average_age(weight):\n",
      "    total_age = 0\n",
      "    for person in weight:\n",
      "        total_age += person['age']\n",
      "    return total_age\n",
      "```<|endoftext|>Create a basic web scraper in Python to extract information from an e-commerce website. The scraper should be able to extract product names and prices from an e-commerce website  ```python\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "# Get the html content\n",
      "url = 'https://example.com/product'\n",
      "\n",
      "# Make the response\n",
      "response = requests.get(url)\n",
      "\n",
      "# Parse the response\n",
      "soup = BeautifulSoup(response.text, 'html.parser')\n",
      "\n",
      "# Get the list of product names and prices\n",
      "products = soup.find_all('div', {'class': 'product-product-list-price'})\n",
      "\n",
      "# Loop over the products and print the result\n",
      "for product in products:\n",
      " print(product.text)\n",
      "```<|endoftext|>Given a list\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model_quantized,\n",
    "    idx=text_to_token_ids(\"Python program to calculate the average\", tokenizer),\n",
    "    max_new_tokens=250,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=5,\n",
    "    temperature=0\n",
    ")\n",
    "end = time.time()\n",
    "print(\"Time taken: \", end-start)\n",
    "print(\"Tokens/s: \", 250/(end-start))\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 278802,
     "modelInstanceId": 257504,
     "sourceId": 301510,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 280904,
     "modelInstanceId": 259730,
     "sourceId": 304328,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
