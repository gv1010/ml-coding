{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":301510,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":257504,"modelId":278802}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-27T17:13:33.893395Z","iopub.execute_input":"2025-03-27T17:13:33.893603Z","iopub.status.idle":"2025-03-27T17:13:33.897401Z","shell.execute_reply.started":"2025-03-27T17:13:33.893583Z","shell.execute_reply":"2025-03-27T17:13:33.896447Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T17:13:33.898268Z","iopub.execute_input":"2025-03-27T17:13:33.898451Z","iopub.status.idle":"2025-03-27T17:13:38.033376Z","shell.execute_reply.started":"2025-03-27T17:13:33.898434Z","shell.execute_reply":"2025-03-27T17:13:38.032420Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    # For-loop is the same as before: Get logits, and only focus on last time step\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -context_size:]\n        with torch.no_grad():\n            logits = model(idx_cond)\n        logits = logits[:, -1, :]\n\n        # New: Filter logits with top_k sampling\n        if top_k is not None:\n            # Keep only top_k values\n            top_logits, _ = torch.topk(logits, top_k)\n            min_val = top_logits[:, -1]\n            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n\n        # New: Apply temperature scaling\n        if temperature > 0.0:\n            logits = logits / temperature\n\n            # Apply softmax to get probabilities\n            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n\n            # Sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n\n        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n        else:\n            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n\n        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n            break\n\n        # Same as before: append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n\n    return idx\n\ndef calc_loss_batch(input_batch, target_batch, model, device):\n    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n    logits = model(input_batch)\n    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n    return loss\n\n\ndef calc_loss_loader(data_loader, model, device, num_batches=None):\n    total_loss = 0.\n    if len(data_loader) == 0:\n        return float(\"nan\")\n    elif num_batches is None:\n        num_batches = len(data_loader)\n    else:\n        # Reduce the number of batches to match the total number of batches in the data loader\n        # if num_batches exceeds the number of batches in the data loader\n        num_batches = min(num_batches, len(data_loader))\n    for i, (input_batch, target_batch) in enumerate(data_loader):\n        if i < num_batches:\n            loss = calc_loss_batch(input_batch, target_batch, model, device)\n            total_loss += loss.item()\n        else:\n            break\n    return total_loss / num_batches","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T17:13:38.034323Z","iopub.execute_input":"2025-03-27T17:13:38.034779Z","iopub.status.idle":"2025-03-27T17:13:38.043480Z","shell.execute_reply.started":"2025-03-27T17:13:38.034737Z","shell.execute_reply":"2025-03-27T17:13:38.042467Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class GELU(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return 0.5 * x * (1 + torch.tanh(\n            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n            (x + 0.044715 * torch.pow(x, 3))\n        ))\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n            GELU(),\n            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.attention = MultiHeadCasualAttention(\n            d_in= cfg[\"emb_dim\"],\n            d_out=cfg[\"emb_dim\"],\n            context_length=cfg[\"context_length\"],\n            dropout=cfg[\"drop_rate\"],\n            num_heads=cfg[\"n_heads\"],\n            qkv_bias=cfg[\"qkv_bias\"]\n        )\n\n        self.ff = FeedForward(cfg)\n        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n\n    def forward(self, x):\n        # This block does nothing and just returns its input.\n        shortcut = x\n        x = self.norm1(x)\n        x = self.attention(x)\n        x = self.drop_shortcut(x)\n\n        x = x + shortcut\n\n        shortcut = x\n\n        x = self.norm2(x)\n        x = self.ff(x)\n        x = self.drop_shortcut(x)\n        x = x + shortcut\n        return x\n\n\nclass LayerNorm(nn.Module):\n    def __init__(self, emb_dim, eps=1e-5):\n        super().__init__()\n        self.eps = eps\n        self.scale = nn.Parameter(torch.ones(emb_dim))\n        self.shift = nn.Parameter(torch.zeros(emb_dim))\n        \n\n    def forward(self, x):\n        # This layer does nothing and just returns its input.\n\n        mean = x.mean(dim = -1, keepdim = True)\n        var = x.var(dim=-1,keepdim = True, unbiased = False)\n        norm_x = (x-mean)/torch.sqrt(var+self.eps)\n        return self.scale * norm_x + self.shift\n\nclass MultiHeadCasualAttention(nn.Module):\n    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n        super().__init__()\n        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n\n        self.d_out = d_out\n        self.num_heads = num_heads\n        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n\n    def forward(self, x):\n        b, num_tokens, d_in = x.shape\n\n        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n        queries = self.W_query(x)\n        values = self.W_value(x)\n\n        # We implicitly split the matrix by adding a `num_heads` dimension\n        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n\n        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n        keys = keys.transpose(1, 2)\n        queries = queries.transpose(1, 2)\n        values = values.transpose(1, 2)\n\n        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n\n        # Original mask truncated to the number of tokens and converted to boolean\n        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n\n        # Use the mask to fill attention scores\n        attn_scores.masked_fill_(mask_bool, -torch.inf)\n\n        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n\n        # Shape: (b, num_tokens, num_heads, head_dim)\n        context_vec = (attn_weights @ values).transpose(1, 2)\n\n        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n        context_vec = self.out_proj(context_vec)  # optional projection\n\n        return context_vec\n\n\n\nclass GPTModel(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n\n        self.trf_blocks = nn.Sequential(\n            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n        self.out_head = nn.Linear(\n            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n        )\n\n    def forward(self, in_idx):\n        \n        batch_size, seq_len = in_idx.shape\n        tok_embeds = self.tok_emb(in_idx)\n        pos_embeds = self.pos_emb(torch.arange(seq_len, device = in_idx.device))\n        x = tok_embeds + pos_embeds\n        x = self.drop_emb(x)\n        x = self.trf_blocks(x)\n        x = self.final_norm(x)\n\n        logits = self.out_head(x)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T17:13:38.046176Z","iopub.execute_input":"2025-03-27T17:13:38.046452Z","iopub.status.idle":"2025-03-27T17:13:38.071499Z","shell.execute_reply.started":"2025-03-27T17:13:38.046418Z","shell.execute_reply":"2025-03-27T17:13:38.070646Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Load Model","metadata":{}},{"cell_type":"code","source":"GPT_CONFIG_124M = {\n    \"vocab_size\": 50257,    # Vocabulary size\n    \"context_length\": 512, # Context length\n    \"emb_dim\": 768,         # Embedding dimension\n    \"n_heads\": 12,          # Number of attention heads\n    \"n_layers\": 12,         # Number of layers\n    \"drop_rate\": 0.1,       # Dropout rate\n    \"qkv_bias\": False       # Query-Key-Value bias\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T17:13:38.072730Z","iopub.execute_input":"2025-03-27T17:13:38.073030Z","iopub.status.idle":"2025-03-27T17:13:38.090411Z","shell.execute_reply.started":"2025-03-27T17:13:38.072999Z","shell.execute_reply":"2025-03-27T17:13:38.089305Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"snapshot_model = GPTModel(GPT_CONFIG_124M)\n\n# /kaggle/input/py-gpt2/pytorch/default/1/gpt2_ddp_model_1.pth\n\nsnapshot_model.load_state_dict(torch.load(\"/kaggle/input/gpt2-cpu/pytorch/default/1/gpt2_ddp_model_2_cpu.pth\", weights_only=True))\n# snapshot_model.eval()\ndevice = torch.device(\"cuda\")\nsnapshot_model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T17:13:38.091366Z","iopub.execute_input":"2025-03-27T17:13:38.091725Z","iopub.status.idle":"2025-03-27T17:13:44.505547Z","shell.execute_reply.started":"2025-03-27T17:13:38.091694Z","shell.execute_reply":"2025-03-27T17:13:44.504561Z"},"scrolled":true},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"GPTModel(\n  (tok_emb): Embedding(50257, 768)\n  (pos_emb): Embedding(512, 768)\n  (drop_emb): Dropout(p=0.1, inplace=False)\n  (trf_blocks): Sequential(\n    (0): TransformerBlock(\n      (attention): MultiHeadCasualAttention(\n        (W_query): Linear(in_features=768, out_features=768, bias=False)\n        (W_key): Linear(in_features=768, out_features=768, bias=False)\n        (W_value): Linear(in_features=768, out_features=768, bias=False)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.1, inplace=False)\n    )\n    (1): TransformerBlock(\n      (attention): MultiHeadCasualAttention(\n        (W_query): Linear(in_features=768, out_features=768, bias=False)\n        (W_key): Linear(in_features=768, out_features=768, bias=False)\n        (W_value): Linear(in_features=768, out_features=768, bias=False)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.1, inplace=False)\n    )\n    (2): TransformerBlock(\n      (attention): MultiHeadCasualAttention(\n        (W_query): Linear(in_features=768, out_features=768, bias=False)\n        (W_key): Linear(in_features=768, out_features=768, bias=False)\n        (W_value): Linear(in_features=768, out_features=768, bias=False)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.1, inplace=False)\n    )\n    (3): TransformerBlock(\n      (attention): MultiHeadCasualAttention(\n        (W_query): Linear(in_features=768, out_features=768, bias=False)\n        (W_key): Linear(in_features=768, out_features=768, bias=False)\n        (W_value): Linear(in_features=768, out_features=768, bias=False)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.1, inplace=False)\n    )\n    (4): TransformerBlock(\n      (attention): MultiHeadCasualAttention(\n        (W_query): Linear(in_features=768, out_features=768, bias=False)\n        (W_key): Linear(in_features=768, out_features=768, bias=False)\n        (W_value): Linear(in_features=768, out_features=768, bias=False)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.1, inplace=False)\n    )\n    (5): TransformerBlock(\n      (attention): MultiHeadCasualAttention(\n        (W_query): Linear(in_features=768, out_features=768, bias=False)\n        (W_key): Linear(in_features=768, out_features=768, bias=False)\n        (W_value): Linear(in_features=768, out_features=768, bias=False)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.1, inplace=False)\n    )\n    (6): TransformerBlock(\n      (attention): MultiHeadCasualAttention(\n        (W_query): Linear(in_features=768, out_features=768, bias=False)\n        (W_key): Linear(in_features=768, out_features=768, bias=False)\n        (W_value): Linear(in_features=768, out_features=768, bias=False)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.1, inplace=False)\n    )\n    (7): TransformerBlock(\n      (attention): MultiHeadCasualAttention(\n        (W_query): Linear(in_features=768, out_features=768, bias=False)\n        (W_key): Linear(in_features=768, out_features=768, bias=False)\n        (W_value): Linear(in_features=768, out_features=768, bias=False)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.1, inplace=False)\n    )\n    (8): TransformerBlock(\n      (attention): MultiHeadCasualAttention(\n        (W_query): Linear(in_features=768, out_features=768, bias=False)\n        (W_key): Linear(in_features=768, out_features=768, bias=False)\n        (W_value): Linear(in_features=768, out_features=768, bias=False)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.1, inplace=False)\n    )\n    (9): TransformerBlock(\n      (attention): MultiHeadCasualAttention(\n        (W_query): Linear(in_features=768, out_features=768, bias=False)\n        (W_key): Linear(in_features=768, out_features=768, bias=False)\n        (W_value): Linear(in_features=768, out_features=768, bias=False)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.1, inplace=False)\n    )\n    (10): TransformerBlock(\n      (attention): MultiHeadCasualAttention(\n        (W_query): Linear(in_features=768, out_features=768, bias=False)\n        (W_key): Linear(in_features=768, out_features=768, bias=False)\n        (W_value): Linear(in_features=768, out_features=768, bias=False)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.1, inplace=False)\n    )\n    (11): TransformerBlock(\n      (attention): MultiHeadCasualAttention(\n        (W_query): Linear(in_features=768, out_features=768, bias=False)\n        (W_key): Linear(in_features=768, out_features=768, bias=False)\n        (W_value): Linear(in_features=768, out_features=768, bias=False)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.1, inplace=False)\n    )\n  )\n  (final_norm): LayerNorm()\n  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n)"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"# Validation","metadata":{}},{"cell_type":"code","source":"# Custom Dataset (assuming you have this)\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport tiktoken\nfrom torch.utils.data import DataLoader, Dataset\nimport os\nclass CustomDataset(Dataset):\n    def __init__(self, txt, tokenizer, max_length, stride):\n        self.input_ids = []\n        self.target_ids = []\n\n        # Tokenize the entire text\n        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n\n        # Use a sliding window to chunk the book into overlapping sequences of max_length\n        for i in range(0, len(token_ids) - max_length, stride):\n            input_chunk = token_ids[i:i + max_length]\n            target_chunk = token_ids[i + 1: i + max_length + 1]\n            self.input_ids.append(torch.tensor(input_chunk))\n            self.target_ids.append(torch.tensor(target_chunk))\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return self.input_ids[idx], self.target_ids[idx]\n    \ndef create_dataloader_v1(txt, batch_size=8, max_length=256,\n                         stride=128, shuffle=True, drop_last=True, num_workers=0):\n    # Initialize the tokenizer\n    tokenizer = tiktoken.get_encoding(\"gpt2\")\n\n    # Create dataset\n    dataset = CustomDataset(txt, tokenizer, max_length, stride)\n\n    # Create dataloader\n    dataloader = DataLoader(\n        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n\n    return dataloader\n\n\ndef load_train_val_dataset():\n    from datasets import load_dataset\n\n    train_dataset = load_dataset('flytech/python-codes-25k', split='train')\n    # train_dataset = load_dataset('flytech/python-codes-25k', split='test')\n\n    # One can map the dataset in any way, for the sake of example:\n    dataset = train_dataset.map(lambda example: {'text': example['instruction'] + ' ' + example['input'] + ' ' + example['output']})['text']\n    # Remember that you don't need to map if the dataset has a \"text\" field already:)\n    train_ratio = 0.90\n    # text_data = \"<|endoftext|>\".join(dataset[:50])\n    dataset = dataset[:]\n    split_idx = int(train_ratio * len(dataset))\n\n    train_data = dataset[:split_idx]\n    val_data = dataset[split_idx:]\n    train_data = \"<|endoftext|>\".join(train_data[:])\n    val_data = \"<|endoftext|>\".join(val_data[:])\n    # print(\"split_idx: \", split_idx)\n\n    torch.manual_seed(123)\n\n    train_loader = create_dataloader_v1(\n        train_data,\n        batch_size=8,\n        max_length=GPT_CONFIG_124M[\"context_length\"],\n        stride=GPT_CONFIG_124M[\"context_length\"],\n        drop_last=True,\n        shuffle=True,\n        num_workers=0\n    )\n\n    val_loader = create_dataloader_v1(\n        val_data,\n        batch_size=8,\n        max_length=GPT_CONFIG_124M[\"context_length\"],\n        stride=GPT_CONFIG_124M[\"context_length\"],\n        drop_last=False,\n        shuffle=False,\n        num_workers=0\n    )\n    print(\"length of dataset\", len(train_data), len(val_data))\n    # print(\"length of loader\", len(train_loader), len(val_loader))\n    return train_loader, val_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T17:18:38.923045Z","iopub.execute_input":"2025-03-27T17:18:38.923352Z","iopub.status.idle":"2025-03-27T17:18:38.934051Z","shell.execute_reply.started":"2025-03-27T17:18:38.923329Z","shell.execute_reply":"2025-03-27T17:18:38.933044Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"\ndef token_ids_to_text(token_ids, tokenizer):\n    flat = token_ids.squeeze(0) # remove batch dimension\n    return tokenizer.decode(flat.tolist())\n\ndef calc_loss_batch(input_batch, target_batch, model, device):\n    try:\n        input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n        with torch.no_grad():\n            logits = model(input_batch)\n            loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n        del input_batch, target_batch, logits\n        torch.cuda.empty_cache()\n        return loss\n    except Exception as exp:\n        print(exp)\n        return 0\n\ndef calc_loss_loader(data_loader, model, device, num_batches=None):\n    total_loss = 0.\n    if len(data_loader) == 0:\n        return float(\"nan\")\n    elif num_batches is None:\n        num_batches = len(data_loader)\n    else:\n        # Reduce the number of batches to match the total number of batches in the data loader\n        # if num_batches exceeds the number of batches in the data loader\n        num_batches = min(num_batches, len(data_loader))\n    for i, (input_batch, target_batch) in enumerate(data_loader):\n        if i < num_batches:\n            loss = calc_loss_batch(input_batch, target_batch, model, device)\n            total_loss += loss\n        else:\n            break\n    return total_loss / num_batches","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T17:18:42.589693Z","iopub.execute_input":"2025-03-27T17:18:42.589993Z","iopub.status.idle":"2025-03-27T17:18:42.596884Z","shell.execute_reply.started":"2025-03-27T17:18:42.589970Z","shell.execute_reply":"2025-03-27T17:18:42.595695Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"train_loader, val_loader = load_train_val_dataset()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T17:18:45.260790Z","iopub.execute_input":"2025-03-27T17:18:45.261127Z","iopub.status.idle":"2025-03-27T17:18:52.814884Z","shell.execute_reply.started":"2025-03-27T17:18:45.261099Z","shell.execute_reply":"2025-03-27T17:18:52.813728Z"}},"outputs":[{"name":"stdout","text":"length of dataset 19989246 3200684\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"len(val_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T17:14:00.134464Z","iopub.execute_input":"2025-03-27T17:14:00.134801Z","iopub.status.idle":"2025-03-27T17:14:00.140832Z","shell.execute_reply.started":"2025-03-27T17:14:00.134771Z","shell.execute_reply":"2025-03-27T17:14:00.139663Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"220"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"220 * 512 * 2 * 8 * 4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T17:14:01.885533Z","iopub.execute_input":"2025-03-27T17:14:01.885858Z","iopub.status.idle":"2025-03-27T17:14:01.891332Z","shell.execute_reply.started":"2025-03-27T17:14:01.885830Z","shell.execute_reply":"2025-03-27T17:14:01.890391Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"7208960"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"from tqdm import tqdm\ntotal_train_loss = 0\ntrain_idx = 0\nsnapshot_model.eval()\n\nfor input_batch, target_batch in tqdm(train_loader, total = len(train_loader)):  \n    train_loss = calc_loss_batch(input_batch, target_batch, snapshot_model, device=torch.device(\"cuda\"))\n    train_idx += 1\n    total_train_loss += (train_loss.item())\n    del input_batch, target_batch\n    torch.cuda.empty_cache()\n\n\n# if val_idx:\n# val_loss = total_val_loss/(val_idx)\n# validation_loss.append(val_loss)\n# print(f'Validation Loss: {val_loss:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T17:19:02.361903Z","iopub.execute_input":"2025-03-27T17:19:02.362238Z","iopub.status.idle":"2025-03-27T17:33:04.793896Z","shell.execute_reply.started":"2025-03-27T17:19:02.362213Z","shell.execute_reply":"2025-03-27T17:33:04.793001Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 1672/1672 [12:24<00:00,  2.25it/s]\n100%|██████████| 220/220 [01:37<00:00,  2.25it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss:  0.19891726750987426\nVal loss:  tensor(1.3223, device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"from tqdm import tqdm\n\ndef validate_model(model, val_loader ):\n\n    total_val_loss = 0\n    val_idx = 0\n    val_loss_list = []\n    model.eval()\n    \n    for input_batch, target_batch in tqdm(val_loader, total = len(val_loader)):\n        val_loss = calc_loss_batch(input_batch, target_batch, model, device=torch.device(\"cuda\"))\n        val_idx += 1\n        total_val_loss += (val_loss)\n        val_loss_list.append(val_loss)\n\n    print(\"Val loss: \", total_val_loss/val_idx)\n    return val_loss_list, total_val_loss/val_idx\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T17:42:04.814529Z","iopub.execute_input":"2025-03-27T17:42:04.814848Z","iopub.status.idle":"2025-03-27T17:42:04.820166Z","shell.execute_reply.started":"2025-03-27T17:42:04.814823Z","shell.execute_reply":"2025-03-27T17:42:04.819208Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# next(iter(train_loader))[0].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T17:40:31.730687Z","iopub.execute_input":"2025-03-27T17:40:31.731011Z","iopub.status.idle":"2025-03-27T17:40:31.734570Z","shell.execute_reply.started":"2025-03-27T17:40:31.730972Z","shell.execute_reply":"2025-03-27T17:40:31.733750Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"# Model Size","metadata":{}},{"cell_type":"code","source":"\ndef calculate_model_size(model):\n    \"\"\"\n    Calculates the total size of a PyTorch model in bytes.\n\n    Args:\n        model (torch.nn.Module): The PyTorch model.\n\n    Returns:\n        int: The total size of the model in bytes.\n    \"\"\"\n    total_params = 0\n    for param in model.parameters():\n        total_params += param.numel() * param.element_size()\n    return total_params\n\ndef format_size(size_bytes):\n    \"\"\"\n    Formats a size in bytes into a human-readable format (KB, MB, GB).\n\n    Args:\n        size_bytes (int): The size in bytes.\n\n    Returns:\n        str: The formatted size.\n    \"\"\"\n    if size_bytes < 1024:\n        return f\"{size_bytes} bytes\"\n    elif size_bytes < (1024 * 1024):\n        return f\"{size_bytes / 1024:.2f} KB\"\n    elif size_bytes < (1024 * 1024 * 1024):\n        return f\"{size_bytes / (1024 * 1024):.2f} MB\"\n    else:\n        return f\"{size_bytes / (1024 * 1024 * 1024):.2f} GB\"\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T18:04:57.473132Z","iopub.execute_input":"2025-03-27T18:04:57.473455Z","iopub.status.idle":"2025-03-27T18:04:57.478503Z","shell.execute_reply.started":"2025-03-27T18:04:57.473430Z","shell.execute_reply":"2025-03-27T18:04:57.477634Z"}},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":"# Quantized Model","metadata":{}},{"cell_type":"code","source":"snapshot_model.qconfig = torch.ao.quantization.default_qconfig\nnet_quantized = torch.ao.quantization.prepare(snapshot_model) # Insert observers\nnet_quantized = torch.ao.quantization.convert(net_quantized)\n\n\nmodel_size_bytes = calculate_model_size(net_quantized)\nmodel_size_formatted = format_size(model_size_bytes)\n\nprint(f\"Total size of the model: {model_size_formatted}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T18:05:08.144505Z","iopub.execute_input":"2025-03-27T18:05:08.144812Z","iopub.status.idle":"2025-03-27T18:05:08.740006Z","shell.execute_reply.started":"2025-03-27T18:05:08.144790Z","shell.execute_reply":"2025-03-27T18:05:08.738916Z"}},"outputs":[{"name":"stdout","text":"Total size of the model: 620.33 MB\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"# Define the quantization configuration\nmodel_old = snapshot_model\nsnapshot_model.to(torch.device(\"cpu\"))\nquantization_config = torch.ao.quantization.get_default_qconfig('fbgemm')\nsnapshot_model.config.torch_dtype = torch.float32 # Ensure original dtype is float32\n\n# Prepare the model for static quantization\nprepared_model = torch.ao.quantization.prepare(snapshot_model, quantization_config)\n\n# quantized_model = torch.quantization.quantize_dynamic(\n#     snapshot_model.to(torch.device(\"cpu\")), {torch.nn.Linear}, dtype=torch.qint8\n# )\n\nquantized_model = torch.ao.quantization.convert(prepared_model, inplace=False)\n\ntorch.save(quantized_model.state_dict(), '/kaggle/working/quantized_8b_gpt2_model.pth')\n\nval_loss_list, val_loss = validate_model(quantized_model.to(torch.device(\"cuda\")), val_loader)\nprint(f\"Quantized Average Loss: {val_loss}\")\n# print(f\"Quantized Perplexity: {quantized_perplexity}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T17:55:10.793542Z","iopub.execute_input":"2025-03-27T17:55:10.793850Z","iopub.status.idle":"2025-03-27T17:56:49.440392Z","shell.execute_reply.started":"2025-03-27T17:55:10.793827Z","shell.execute_reply":"2025-03-27T17:56:49.439400Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/quantize.py:392: UserWarning: None of the submodule got qconfig applied. Make sure you passed correct configuration through `qconfig_dict` or by assigning the `.qconfig` attribute directly on submodules\n  warnings.warn(\n100%|██████████| 220/220 [01:36<00:00,  2.27it/s]","output_type":"stream"},{"name":"stdout","text":"Val loss:  tensor(1.3223, device='cuda:0')\nQuantized Average Loss: 1.3222649097442627\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"quantization_config","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T17:58:10.320909Z","iopub.execute_input":"2025-03-27T17:58:10.321247Z","iopub.status.idle":"2025-03-27T17:58:10.326517Z","shell.execute_reply.started":"2025-03-27T17:58:10.321221Z","shell.execute_reply":"2025-03-27T17:58:10.325509Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"for name, param in quantized_model.named_parameters():\n    print(f\"Parameter name: {name}, Data type: {param.data.dtype}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T17:57:56.152548Z","iopub.execute_input":"2025-03-27T17:57:56.152934Z","iopub.status.idle":"2025-03-27T17:57:56.185436Z","shell.execute_reply.started":"2025-03-27T17:57:56.152907Z","shell.execute_reply":"2025-03-27T17:57:56.184498Z"}},"outputs":[{"name":"stdout","text":"Parameter name: tok_emb.weight, Data type: torch.float32\nParameter name: pos_emb.weight, Data type: torch.float32\nParameter name: trf_blocks.0.attention.W_query.weight, Data type: torch.float32\nParameter name: trf_blocks.0.attention.W_key.weight, Data type: torch.float32\nParameter name: trf_blocks.0.attention.W_value.weight, Data type: torch.float32\nParameter name: trf_blocks.0.attention.out_proj.weight, Data type: torch.float32\nParameter name: trf_blocks.0.attention.out_proj.bias, Data type: torch.float32\nParameter name: trf_blocks.0.ff.layers.0.weight, Data type: torch.float32\nParameter name: trf_blocks.0.ff.layers.0.bias, Data type: torch.float32\nParameter name: trf_blocks.0.ff.layers.2.weight, Data type: torch.float32\nParameter name: trf_blocks.0.ff.layers.2.bias, Data type: torch.float32\nParameter name: trf_blocks.0.norm1.scale, Data type: torch.float32\nParameter name: trf_blocks.0.norm1.shift, Data type: torch.float32\nParameter name: trf_blocks.0.norm2.scale, Data type: torch.float32\nParameter name: trf_blocks.0.norm2.shift, Data type: torch.float32\nParameter name: trf_blocks.1.attention.W_query.weight, Data type: torch.float32\nParameter name: trf_blocks.1.attention.W_key.weight, Data type: torch.float32\nParameter name: trf_blocks.1.attention.W_value.weight, Data type: torch.float32\nParameter name: trf_blocks.1.attention.out_proj.weight, Data type: torch.float32\nParameter name: trf_blocks.1.attention.out_proj.bias, Data type: torch.float32\nParameter name: trf_blocks.1.ff.layers.0.weight, Data type: torch.float32\nParameter name: trf_blocks.1.ff.layers.0.bias, Data type: torch.float32\nParameter name: trf_blocks.1.ff.layers.2.weight, Data type: torch.float32\nParameter name: trf_blocks.1.ff.layers.2.bias, Data type: torch.float32\nParameter name: trf_blocks.1.norm1.scale, Data type: torch.float32\nParameter name: trf_blocks.1.norm1.shift, Data type: torch.float32\nParameter name: trf_blocks.1.norm2.scale, Data type: torch.float32\nParameter name: trf_blocks.1.norm2.shift, Data type: torch.float32\nParameter name: trf_blocks.2.attention.W_query.weight, Data type: torch.float32\nParameter name: trf_blocks.2.attention.W_key.weight, Data type: torch.float32\nParameter name: trf_blocks.2.attention.W_value.weight, Data type: torch.float32\nParameter name: trf_blocks.2.attention.out_proj.weight, Data type: torch.float32\nParameter name: trf_blocks.2.attention.out_proj.bias, Data type: torch.float32\nParameter name: trf_blocks.2.ff.layers.0.weight, Data type: torch.float32\nParameter name: trf_blocks.2.ff.layers.0.bias, Data type: torch.float32\nParameter name: trf_blocks.2.ff.layers.2.weight, Data type: torch.float32\nParameter name: trf_blocks.2.ff.layers.2.bias, Data type: torch.float32\nParameter name: trf_blocks.2.norm1.scale, Data type: torch.float32\nParameter name: trf_blocks.2.norm1.shift, Data type: torch.float32\nParameter name: trf_blocks.2.norm2.scale, Data type: torch.float32\nParameter name: trf_blocks.2.norm2.shift, Data type: torch.float32\nParameter name: trf_blocks.3.attention.W_query.weight, Data type: torch.float32\nParameter name: trf_blocks.3.attention.W_key.weight, Data type: torch.float32\nParameter name: trf_blocks.3.attention.W_value.weight, Data type: torch.float32\nParameter name: trf_blocks.3.attention.out_proj.weight, Data type: torch.float32\nParameter name: trf_blocks.3.attention.out_proj.bias, Data type: torch.float32\nParameter name: trf_blocks.3.ff.layers.0.weight, Data type: torch.float32\nParameter name: trf_blocks.3.ff.layers.0.bias, Data type: torch.float32\nParameter name: trf_blocks.3.ff.layers.2.weight, Data type: torch.float32\nParameter name: trf_blocks.3.ff.layers.2.bias, Data type: torch.float32\nParameter name: trf_blocks.3.norm1.scale, Data type: torch.float32\nParameter name: trf_blocks.3.norm1.shift, Data type: torch.float32\nParameter name: trf_blocks.3.norm2.scale, Data type: torch.float32\nParameter name: trf_blocks.3.norm2.shift, Data type: torch.float32\nParameter name: trf_blocks.4.attention.W_query.weight, Data type: torch.float32\nParameter name: trf_blocks.4.attention.W_key.weight, Data type: torch.float32\nParameter name: trf_blocks.4.attention.W_value.weight, Data type: torch.float32\nParameter name: trf_blocks.4.attention.out_proj.weight, Data type: torch.float32\nParameter name: trf_blocks.4.attention.out_proj.bias, Data type: torch.float32\nParameter name: trf_blocks.4.ff.layers.0.weight, Data type: torch.float32\nParameter name: trf_blocks.4.ff.layers.0.bias, Data type: torch.float32\nParameter name: trf_blocks.4.ff.layers.2.weight, Data type: torch.float32\nParameter name: trf_blocks.4.ff.layers.2.bias, Data type: torch.float32\nParameter name: trf_blocks.4.norm1.scale, Data type: torch.float32\nParameter name: trf_blocks.4.norm1.shift, Data type: torch.float32\nParameter name: trf_blocks.4.norm2.scale, Data type: torch.float32\nParameter name: trf_blocks.4.norm2.shift, Data type: torch.float32\nParameter name: trf_blocks.5.attention.W_query.weight, Data type: torch.float32\nParameter name: trf_blocks.5.attention.W_key.weight, Data type: torch.float32\nParameter name: trf_blocks.5.attention.W_value.weight, Data type: torch.float32\nParameter name: trf_blocks.5.attention.out_proj.weight, Data type: torch.float32\nParameter name: trf_blocks.5.attention.out_proj.bias, Data type: torch.float32\nParameter name: trf_blocks.5.ff.layers.0.weight, Data type: torch.float32\nParameter name: trf_blocks.5.ff.layers.0.bias, Data type: torch.float32\nParameter name: trf_blocks.5.ff.layers.2.weight, Data type: torch.float32\nParameter name: trf_blocks.5.ff.layers.2.bias, Data type: torch.float32\nParameter name: trf_blocks.5.norm1.scale, Data type: torch.float32\nParameter name: trf_blocks.5.norm1.shift, Data type: torch.float32\nParameter name: trf_blocks.5.norm2.scale, Data type: torch.float32\nParameter name: trf_blocks.5.norm2.shift, Data type: torch.float32\nParameter name: trf_blocks.6.attention.W_query.weight, Data type: torch.float32\nParameter name: trf_blocks.6.attention.W_key.weight, Data type: torch.float32\nParameter name: trf_blocks.6.attention.W_value.weight, Data type: torch.float32\nParameter name: trf_blocks.6.attention.out_proj.weight, Data type: torch.float32\nParameter name: trf_blocks.6.attention.out_proj.bias, Data type: torch.float32\nParameter name: trf_blocks.6.ff.layers.0.weight, Data type: torch.float32\nParameter name: trf_blocks.6.ff.layers.0.bias, Data type: torch.float32\nParameter name: trf_blocks.6.ff.layers.2.weight, Data type: torch.float32\nParameter name: trf_blocks.6.ff.layers.2.bias, Data type: torch.float32\nParameter name: trf_blocks.6.norm1.scale, Data type: torch.float32\nParameter name: trf_blocks.6.norm1.shift, Data type: torch.float32\nParameter name: trf_blocks.6.norm2.scale, Data type: torch.float32\nParameter name: trf_blocks.6.norm2.shift, Data type: torch.float32\nParameter name: trf_blocks.7.attention.W_query.weight, Data type: torch.float32\nParameter name: trf_blocks.7.attention.W_key.weight, Data type: torch.float32\nParameter name: trf_blocks.7.attention.W_value.weight, Data type: torch.float32\nParameter name: trf_blocks.7.attention.out_proj.weight, Data type: torch.float32\nParameter name: trf_blocks.7.attention.out_proj.bias, Data type: torch.float32\nParameter name: trf_blocks.7.ff.layers.0.weight, Data type: torch.float32\nParameter name: trf_blocks.7.ff.layers.0.bias, Data type: torch.float32\nParameter name: trf_blocks.7.ff.layers.2.weight, Data type: torch.float32\nParameter name: trf_blocks.7.ff.layers.2.bias, Data type: torch.float32\nParameter name: trf_blocks.7.norm1.scale, Data type: torch.float32\nParameter name: trf_blocks.7.norm1.shift, Data type: torch.float32\nParameter name: trf_blocks.7.norm2.scale, Data type: torch.float32\nParameter name: trf_blocks.7.norm2.shift, Data type: torch.float32\nParameter name: trf_blocks.8.attention.W_query.weight, Data type: torch.float32\nParameter name: trf_blocks.8.attention.W_key.weight, Data type: torch.float32\nParameter name: trf_blocks.8.attention.W_value.weight, Data type: torch.float32\nParameter name: trf_blocks.8.attention.out_proj.weight, Data type: torch.float32\nParameter name: trf_blocks.8.attention.out_proj.bias, Data type: torch.float32\nParameter name: trf_blocks.8.ff.layers.0.weight, Data type: torch.float32\nParameter name: trf_blocks.8.ff.layers.0.bias, Data type: torch.float32\nParameter name: trf_blocks.8.ff.layers.2.weight, Data type: torch.float32\nParameter name: trf_blocks.8.ff.layers.2.bias, Data type: torch.float32\nParameter name: trf_blocks.8.norm1.scale, Data type: torch.float32\nParameter name: trf_blocks.8.norm1.shift, Data type: torch.float32\nParameter name: trf_blocks.8.norm2.scale, Data type: torch.float32\nParameter name: trf_blocks.8.norm2.shift, Data type: torch.float32\nParameter name: trf_blocks.9.attention.W_query.weight, Data type: torch.float32\nParameter name: trf_blocks.9.attention.W_key.weight, Data type: torch.float32\nParameter name: trf_blocks.9.attention.W_value.weight, Data type: torch.float32\nParameter name: trf_blocks.9.attention.out_proj.weight, Data type: torch.float32\nParameter name: trf_blocks.9.attention.out_proj.bias, Data type: torch.float32\nParameter name: trf_blocks.9.ff.layers.0.weight, Data type: torch.float32\nParameter name: trf_blocks.9.ff.layers.0.bias, Data type: torch.float32\nParameter name: trf_blocks.9.ff.layers.2.weight, Data type: torch.float32\nParameter name: trf_blocks.9.ff.layers.2.bias, Data type: torch.float32\nParameter name: trf_blocks.9.norm1.scale, Data type: torch.float32\nParameter name: trf_blocks.9.norm1.shift, Data type: torch.float32\nParameter name: trf_blocks.9.norm2.scale, Data type: torch.float32\nParameter name: trf_blocks.9.norm2.shift, Data type: torch.float32\nParameter name: trf_blocks.10.attention.W_query.weight, Data type: torch.float32\nParameter name: trf_blocks.10.attention.W_key.weight, Data type: torch.float32\nParameter name: trf_blocks.10.attention.W_value.weight, Data type: torch.float32\nParameter name: trf_blocks.10.attention.out_proj.weight, Data type: torch.float32\nParameter name: trf_blocks.10.attention.out_proj.bias, Data type: torch.float32\nParameter name: trf_blocks.10.ff.layers.0.weight, Data type: torch.float32\nParameter name: trf_blocks.10.ff.layers.0.bias, Data type: torch.float32\nParameter name: trf_blocks.10.ff.layers.2.weight, Data type: torch.float32\nParameter name: trf_blocks.10.ff.layers.2.bias, Data type: torch.float32\nParameter name: trf_blocks.10.norm1.scale, Data type: torch.float32\nParameter name: trf_blocks.10.norm1.shift, Data type: torch.float32\nParameter name: trf_blocks.10.norm2.scale, Data type: torch.float32\nParameter name: trf_blocks.10.norm2.shift, Data type: torch.float32\nParameter name: trf_blocks.11.attention.W_query.weight, Data type: torch.float32\nParameter name: trf_blocks.11.attention.W_key.weight, Data type: torch.float32\nParameter name: trf_blocks.11.attention.W_value.weight, Data type: torch.float32\nParameter name: trf_blocks.11.attention.out_proj.weight, Data type: torch.float32\nParameter name: trf_blocks.11.attention.out_proj.bias, Data type: torch.float32\nParameter name: trf_blocks.11.ff.layers.0.weight, Data type: torch.float32\nParameter name: trf_blocks.11.ff.layers.0.bias, Data type: torch.float32\nParameter name: trf_blocks.11.ff.layers.2.weight, Data type: torch.float32\nParameter name: trf_blocks.11.ff.layers.2.bias, Data type: torch.float32\nParameter name: trf_blocks.11.norm1.scale, Data type: torch.float32\nParameter name: trf_blocks.11.norm1.shift, Data type: torch.float32\nParameter name: trf_blocks.11.norm2.scale, Data type: torch.float32\nParameter name: trf_blocks.11.norm2.shift, Data type: torch.float32\nParameter name: final_norm.scale, Data type: torch.float32\nParameter name: final_norm.shift, Data type: torch.float32\nParameter name: out_head.weight, Data type: torch.float32\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}