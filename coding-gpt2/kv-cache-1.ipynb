{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nimport torch.distributed as dist\nimport torch.optim as optim\nimport tiktoken\nfrom torch.utils.data import DataLoader, Dataset\nimport os\n# import matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport torch.nn.functional as F\nimport time\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T07:43:14.689642Z","iopub.execute_input":"2025-04-07T07:43:14.689960Z","iopub.status.idle":"2025-04-07T07:43:14.694940Z","shell.execute_reply.started":"2025-04-07T07:43:14.689937Z","shell.execute_reply":"2025-04-07T07:43:14.693969Z"}},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":"### Gelu Layer Norm","metadata":{}},{"cell_type":"code","source":"class GELU(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return 0.5 * x * (1 + torch.tanh(\n            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n            (x + 0.044715 * torch.pow(x, 3))\n        ))\n\nclass FeedForward(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n            GELU(),\n            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n    \nclass LayerNorm(nn.Module):\n    def __init__(self, emb_dim, eps=1e-5):\n        super().__init__()\n        self.eps = eps\n        self.scale = nn.Parameter(torch.ones(emb_dim))\n        self.shift = nn.Parameter(torch.zeros(emb_dim))\n    \n    def forward(self, x):\n        # This layer does nothing and just returns its input.\n\n        mean = x.mean(dim = -1, keepdim = True)\n        var = x.var(dim=-1,keepdim = True, unbiased = False)\n        norm_x = (x-mean)/torch.sqrt(var+self.eps)\n        return self.scale * norm_x + self.shift","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T07:43:15.020215Z","iopub.execute_input":"2025-04-07T07:43:15.020520Z","iopub.status.idle":"2025-04-07T07:43:15.028093Z","shell.execute_reply.started":"2025-04-07T07:43:15.020489Z","shell.execute_reply":"2025-04-07T07:43:15.027074Z"}},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"### RoPE","metadata":{}},{"cell_type":"code","source":"def rotate_half(x):\n    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n    x1 = x[..., :x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2:]\n    return torch.cat((-x2, x1), dim=-1)\n\ndef apply_rotary_pos_emb(q, k, cos, sin, offset: int = 0):\n    # The first two dimensions of q and k should be (batch, num_heads).\n    # The last dimension is the embedding dimension, which we'll rotate.\n    q_rotated = (q * cos[:, offset:offset + q.shape[2], :]) + (rotate_half(q) * sin[:, offset:offset + q.shape[2], :])\n    k_rotated = (k * cos[:, offset:offset + k.shape[2], :]) + (rotate_half(k) * sin[:, offset:offset + k.shape[2], :])\n    return q_rotated, k_rotated\n\nclass RoPE(nn.Module):\n    def __init__(self, dim, context_length, base=10000):\n        super().__init__()\n        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n        t = torch.arange(context_length, device=inv_freq.device)\n        freqs = torch.outer(t, inv_freq)\n        emb = torch.cat((freqs, freqs), dim=-1)\n        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :])\n        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :])\n\n    def forward(self, q, k):\n        # q and k have shapes (batch, num_heads, seq_len, head_dim)\n        seq_len = q.shape[2]\n        cos = self.cos_cached[:, :, :seq_len, :]\n        sin = self.sin_cached[:, :, :seq_len, :]\n        q_rotated, k_rotated = apply_rotary_pos_emb(q, k, cos, sin)\n        return q_rotated, k_rotated\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T07:43:17.418613Z","iopub.execute_input":"2025-04-07T07:43:17.418908Z","iopub.status.idle":"2025-04-07T07:43:17.427244Z","shell.execute_reply.started":"2025-04-07T07:43:17.418886Z","shell.execute_reply":"2025-04-07T07:43:17.426438Z"}},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":"### Multi Head Attention","metadata":{}},{"cell_type":"code","source":"\nclass MultiHeadCasualAttention(nn.Module):\n    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False, use_cache = False):\n        super().__init__()\n        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n\n        self.context_length = context_length\n        self.use_cache = use_cache\n        self.d_out = d_out\n        self.num_heads = num_heads\n        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n\n        self.cache_k = None\n        self.cache_v = None\n\n        # self.rope = RoPE(self.head_dim, context_length)\n\n    \n    def forward(self, x, prev_key_value=None):\n        b, num_tokens, d_in = x.shape\n\n        \n        if self.use_cache and prev_key_value:\n            \n            x_last = x[:, -1:, :]\n            # print(\"x_last: \", x_last.shape)\n            keys = self.W_key(x_last)\n            queries = self.W_query(x_last)\n            values = self.W_value(x_last)\n\n            keys = keys.view(b, -1, self.num_heads, self.head_dim)\n            values = values.view(b, -1, self.num_heads, self.head_dim)\n            queries = queries.view(b, -1, self.num_heads, self.head_dim)\n\n            if prev_key_value is not None:\n                self.cache_k, self.cache_v = prev_key_value\n            else:\n                self.cache_k = keys\n                self.cache_v = values\n            \n            self.cache_k = torch.cat((self.cache_k, keys[:, -1:, :, :]), dim = 1)\n            self.cache_v = torch.cat((self.cache_v, values[:, -1:, :, :]), dim = 1)\n            # print(\"concat cache_k: \", self.cache_k.shape)\n\n            if self.cache_k.shape[1] > self.context_length:\n                self.cache_k = self.cache_k[:, -self.context_length:, :, :]\n                self.cache_v = self.cache_v[:, -self.context_length:, :, :]\n            \n            keys = self.cache_k.transpose(1, 2)\n            queries = queries[:, -1:, :, :].transpose(1, 2)\n            values = self.cache_v.transpose(1, 2)\n\n            # queries, keys = self.rope(queries, keys)\n\n            # mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n            \n            # Use scaled_dot_product_attention which can leverage Flash Attention\n            attn_output = F.scaled_dot_product_attention(\n                queries, keys, values,\n                attn_mask=None,\n                dropout_p=self.dropout.p if self.training else 0.0, # Apply dropout if in training mode\n                is_causal=True # Mask is provided, so is_causal is False\n            )\n\n            # Shape: (b, num_heads, num_tokens, head_dim) -> (b, num_tokens, num_heads, head_dim)\n            context_vec = attn_output.transpose(1, 2).contiguous().view(b, -1, self.d_out)\n\n            context_vec = self.out_proj(context_vec)  # optional projection\n\n            # print(\"final cache_k:\", self.cache_k.shape)\n\n            return context_vec, (self.cache_k, self.cache_v)\n\n        else:\n            keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n            queries = self.W_query(x)\n            values = self.W_value(x)\n\n    \n            keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n            values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n            queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n\n            # print(\"keys: \",keys.shape)\n            if prev_key_value is None:\n                self.cache_k = keys\n                self.cache_v = values\n            \n            \n            if self.cache_k.shape[1] > self.context_length:\n                self.cache_k = self.cache_k[:, -self.context_length:, :, :]\n                self.cache_v = self.cache_v[:, -self.context_length:, :, :]\n\n            # print(\"cache_k: \",self.cache_k.shape)\n            \n            keys = self.cache_k.transpose(1, 2)\n            queries = queries.transpose(1, 2)\n            values = self.cache_v.transpose(1, 2)\n\n            # queries, keys = self.rope(queries, keys)\n\n            mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n            \n            # Use scaled_dot_product_attention which can leverage Flash Attention\n            attn_output = F.scaled_dot_product_attention(\n                queries, keys, values,\n                attn_mask= None, #mask_bool,\n                dropout_p=self.dropout.p if self.training else 0.0, \n                is_causal=True # Mask is provided, so is_causal is False\n            )\n\n            # Shape: (b, num_heads, num_tokens, head_dim) -> (b, num_tokens, num_heads, head_dim)\n            context_vec = attn_output.transpose(1, 2).contiguous().view(b, num_tokens, self.d_out)\n\n            context_vec = self.out_proj(context_vec)  # optional projection\n\n            return context_vec, (self.cache_k, self.cache_v) if self.use_cache else (None, None)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T07:47:56.689533Z","iopub.execute_input":"2025-04-07T07:47:56.689849Z","iopub.status.idle":"2025-04-07T07:47:56.703878Z","shell.execute_reply.started":"2025-04-07T07:47:56.689828Z","shell.execute_reply":"2025-04-07T07:47:56.702909Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# GPT_CONFIG_124M","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T07:43:32.334700Z","iopub.execute_input":"2025-04-07T07:43:32.335021Z","iopub.status.idle":"2025-04-07T07:43:32.338503Z","shell.execute_reply.started":"2025-04-07T07:43:32.335000Z","shell.execute_reply":"2025-04-07T07:43:32.337729Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# import torch.nn.functional as F\n\n\n# cfg = {\n#         'vocab_size': 50257,\n#         'context_length': 4,\n#         'emb_dim': 768,\n#         'n_heads': 12,\n#         'n_layers': 12,\n#         'drop_rate': 0.1,\n#         'qkv_bias': False,\n#         'use_cache': True\n#         }\n\n# attention = MultiHeadCasualAttention(\n#     d_in=cfg[\"emb_dim\"],\n#     d_out=cfg[\"emb_dim\"],\n#     context_length=cfg[\"context_length\"],\n#     dropout=cfg[\"drop_rate\"],\n#     num_heads=cfg[\"n_heads\"],\n#     qkv_bias=cfg[\"qkv_bias\"],\n#     use_cache=True\n# )\n\n\n# batch_size = 8\n# initial_seq_len = 6\n# embedding_dim = cfg[\"emb_dim\"]\n# initial_input = torch.randn(batch_size, initial_seq_len, embedding_dim)\n\n# output, (prev_k, prev_v) = attention(initial_input)\n# print(len(prev_k))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T07:43:33.801854Z","iopub.execute_input":"2025-04-07T07:43:33.802225Z","iopub.status.idle":"2025-04-07T07:43:33.806167Z","shell.execute_reply.started":"2025-04-07T07:43:33.802195Z","shell.execute_reply":"2025-04-07T07:43:33.805128Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"\n# # Simulate generating a few more tokens\n# num_generate_steps = 30\n# generated_sequence = [output]\n# for _ in range(num_generate_steps):\n#     next_input = torch.randn(batch_size, 1, embedding_dim) # In a real scenario, this would be the embedding of the predicted next token\n\n#     # 4. Perform forward pass with the previous past_key_value\n#     output, prev_key_value = attention(next_input, prev_key_value=(prev_k, prev_v))\n\n#     # print(\"Shape of output after next generation step:\", output.shape)\n#     # if prev_key_value is not None:\n#     #     print(\"Shape of cached keys:\", prev_key_value[0].shape)\n#     #     print(\"Shape of cached values:\", prev_key_value[1].shape)\n\n#     generated_sequence.append(output)\n\n# len(generated_sequence), (prev_key_value[0].shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T07:43:35.633636Z","iopub.execute_input":"2025-04-07T07:43:35.633955Z","iopub.status.idle":"2025-04-07T07:43:35.637725Z","shell.execute_reply.started":"2025-04-07T07:43:35.633932Z","shell.execute_reply":"2025-04-07T07:43:35.636801Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"# prev_key_value[0].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:32:33.997115Z","iopub.execute_input":"2025-04-05T19:32:33.997428Z","iopub.status.idle":"2025-04-05T19:32:34.001116Z","shell.execute_reply.started":"2025-04-05T19:32:33.997404Z","shell.execute_reply":"2025-04-05T19:32:34.000141Z"}},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":"### Transformers block","metadata":{}},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.attention = MultiHeadCasualAttention(\n            d_in= cfg[\"emb_dim\"],\n            d_out=cfg[\"emb_dim\"],\n            context_length=cfg[\"context_length\"],\n            dropout=cfg[\"drop_rate\"],\n            num_heads=cfg[\"n_heads\"],\n            qkv_bias=cfg[\"qkv_bias\"],\n            use_cache=cfg[\"use_cache\"]\n        )\n\n        self.ff = FeedForward(cfg)\n        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n\n    def forward(self, x, prev_key_value=None):\n        # This block does nothing and just returns its input.\n        shortcut = x\n        x = self.norm1(x)\n        x, updated_key_value = self.attention(x, prev_key_value)\n        x = self.drop_shortcut(x)\n\n        x = x + shortcut\n\n        shortcut = x\n\n        x = self.norm2(x)\n        x = self.ff(x)\n        x = self.drop_shortcut(x)\n        x = x + shortcut\n        return x, updated_key_value","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T07:48:01.662690Z","iopub.execute_input":"2025-04-07T07:48:01.663000Z","iopub.status.idle":"2025-04-07T07:48:01.669329Z","shell.execute_reply.started":"2025-04-07T07:48:01.662977Z","shell.execute_reply":"2025-04-07T07:48:01.668452Z"}},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":"### GPT","metadata":{}},{"cell_type":"code","source":"# Assuming you have your GPT-2 model defined already\nclass GPTModel(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n\n        self.trf_blocks = nn.ModuleList(\n            [TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n        self.out_head = nn.Linear(\n            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n        )\n\n    def forward(self, in_idx, prev_key_value = None):\n        # print(in_idx.shape)\n\n        device = next(self.parameters()).device\n        in_idx = in_idx.to(device)\n        \n        batch_size, seq_len = in_idx.shape\n        tok_embeds = self.tok_emb(in_idx)\n        pos_embeds = self.pos_emb(torch.arange(seq_len, device = in_idx.device))\n        x = tok_embeds + pos_embeds\n        x = self.drop_emb(x)\n\n        key_value_states = []\n        for i, trf_block in enumerate(self.trf_blocks):\n            if prev_key_value is None:\n                layer_prev_key_value = None\n            else:\n                layer_prev_key_value = prev_key_value[i]\n            x, present_key_value = trf_block(x, layer_prev_key_value)\n            key_value_states.append(present_key_value)\n\n        # x = self.trf_blocks(x)\n\n        x = self.final_norm(x)\n        logits = self.out_head(x)\n        return logits, key_value_states","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T07:48:03.389193Z","iopub.execute_input":"2025-04-07T07:48:03.389494Z","iopub.status.idle":"2025-04-07T07:48:03.396689Z","shell.execute_reply.started":"2025-04-07T07:48:03.389464Z","shell.execute_reply":"2025-04-07T07:48:03.395789Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    # idx.to(device)\n    # For-loop is the same as before: Get logits, and only focus on last time step\n    start = time.time()\n    for _ in range(max_new_tokens):\n        \n        idx_cond = idx[:, -context_size:]\n        idx_cond.to(device)\n        prev_key_value = None\n        with torch.no_grad():\n            \n            logits, prev_key_value = model(idx_cond, prev_key_value)\n        logits = logits[:, -1, :]\n\n        # New: Filter logits with top_k sampling\n        if top_k is not None:\n            # Keep only top_k values\n            top_logits, _ = torch.topk(logits, top_k)\n            min_val = top_logits[:, -1]\n            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n\n        # New: Apply temperature scaling\n        if temperature > 0.0:\n            logits = logits / temperature\n\n            # Apply softmax to get probabilities\n            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n\n            # Sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n\n        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n        else:\n            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n\n        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n            break\n\n        # Same as before: append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next.to(torch.device(\"cpu\"))), dim=1)  # (batch_size, num_tokens+1)\n    te = time.time() - start\n    tokens_sec = context_size/te\n    print(\"time in seconds: \",te )\n    print(\"tokens/s: \", tokens_sec)\n    return idx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T08:33:05.689187Z","iopub.execute_input":"2025-04-07T08:33:05.689477Z","iopub.status.idle":"2025-04-07T08:33:05.697198Z","shell.execute_reply.started":"2025-04-07T08:33:05.689457Z","shell.execute_reply":"2025-04-07T08:33:05.696349Z"}},"outputs":[],"execution_count":80},{"cell_type":"code","source":"import tiktoken\n\ndef text_to_token_ids(text, tokenizer):\n    # encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n    return encoded_tensor\n\ndef token_ids_to_text(token_ids, tokenizer):\n    flat = token_ids.squeeze(0) # remove batch dimension\n    return tokenizer.decode(flat.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T08:33:07.648623Z","iopub.execute_input":"2025-04-07T08:33:07.648910Z","iopub.status.idle":"2025-04-07T08:33:07.653696Z","shell.execute_reply.started":"2025-04-07T08:33:07.648889Z","shell.execute_reply":"2025-04-07T08:33:07.652754Z"}},"outputs":[],"execution_count":81},{"cell_type":"code","source":"GPT_CONFIG_124M = {\n                        \"vocab_size\": 50257,   # Vocabulary size\n                        \"context_length\": 512, # Shortened context length (orig: 1024)\n                        \"emb_dim\": 768,        # Embedding dimension\n                        \"n_heads\": 12,         # Number of attention heads\n                        \"n_layers\": 12,        # Number of layers\n                        \"drop_rate\": 0.1,      # Dropout rate\n                        \"qkv_bias\": False,      # Query-key-value bias\n                        \"use_cache\" : False\n                    }\n\ncfg = GPT_CONFIG_124M\n\nsnapshot_model = GPTModel(GPT_CONFIG_124M)\n\ndevice = torch.device(\"cuda\")\n\nsnapshot_model.load_state_dict(torch.load(\"/kaggle/input/gpt2-cpu/pytorch/default/1/gpt2_ddp_model_2_cpu.pth\", weights_only=True))\nsnapshot_model.to(device)\n# print()\n\nstart_context = '''Edit the script to print out the number of times each letter appears in the given sentence We are learning Python Let's turn up the heat! It's getting hot in here! \"\"\"Edit the script to print out the number of times each letter appears in the given sentence.\"\"\"\n\nsentence = 'We are learning Python'\n\n# Create an empty dictionary\nletter_freq = {}\n\n# Iterate through the sentence\nfor c in sentence:\n# If the character is a letter\nif c.isalpha():\n# Increment the frequency in the dictionary\nletter_freq[c] = letter_freq.setdefault(c, 0) + 1\n\n# Print out the frequency of each letter'''\ntokenizer = tiktoken.get_encoding(\"gpt2\")\n\n\nprint(\"use_cache=False: \")\ntoken_ids = generate(\n    model=snapshot_model,\n    idx=text_to_token_ids(start_context, tokenizer),\n    max_new_tokens=256,\n    context_size=GPT_CONFIG_124M[\"context_length\"]\n)\n\n\nprint(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T08:33:09.270492Z","iopub.execute_input":"2025-04-07T08:33:09.270776Z","iopub.status.idle":"2025-04-07T08:33:21.226446Z","shell.execute_reply.started":"2025-04-07T08:33:09.270754Z","shell.execute_reply":"2025-04-07T08:33:21.225652Z"}},"outputs":[{"name":"stdout","text":"use_cache=False: \ntime in seconds:  10.14923644065857\ntokens/s:  50.447144767353265\nOutput text:\n Edit the script to print out the number of times each letter appears in the given sentence We are learning Python Let's turn up the heat! It's getting hot in here! \"\"\"Edit the script to print out the number of times each letter appears in the given sentence.\"\"\"\n\nsentence = 'We are learning Python'\n\n# Create an empty dictionary\nletter_freq = {}\n\n# Iterate through the sentence\nfor c in sentence:\n# If the character is a letter\nif c.isalpha():\n# Increment the frequency in the dictionary\nletter_freq[c] = letter_freq.setdefault(c, 0) + 1\n\n# Print out the frequency of each letter\nfor letter, freq in letter_freq:\n    print(f'{letter} letter is {freq}')\n```<|endoftext|>Create a Python program to calculate the average price of an array of numbers [2.4, 5.6, 7.6]  ```python\ndef calculate_average(numbers):\n    sum_price = 0\n    for x in numbers:\n        sum_price += x\n    return sum_price / len(numbers)\n\nprint(calculate_average([2.4, 5.4, 7.6, 7.4]))\n```<|endoftext|>Create a Python program to calculate the total cost of a meal given the ingredients of a meal, including their prices, and percentage Price rate/100  ```python\ndef calculate_total_cost(meal, prices):\n    total_cost = 0\n    for cost in meals.cost:\n        total_cost += cost\n    return total_cost\n\ncost = calculate_total_cost(700, 600)\nprint('The total cost\n","output_type":"stream"}],"execution_count":82},{"cell_type":"code","source":"use_cache_false = [(context_length, token/second, time)]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# use_cache_false_list = []\n# for power in tqdm(range(1, 12)):\n#     GPT_CONFIG_124M = {\n#                             \"vocab_size\": 50257,   # Vocabulary size\n#                             \"context_length\": 512, # Shortened context length (orig: 1024)\n#                             \"emb_dim\": 768,        # Embedding dimension\n#                             \"n_heads\": 12,         # Number of attention heads\n#                             \"n_layers\": 12,        # Number of layers\n#                             \"drop_rate\": 0.1,      # Dropout rate\n#                             \"qkv_bias\": False,      # Query-key-value bias\n#                             \"use_cache\" : False\n#                         }\n    \n#     cfg = GPT_CONFIG_124M\n    \n#     snapshot_model = GPTModel(GPT_CONFIG_124M)\n    \n#     device = torch.device(\"cuda\")\n    \n#     snapshot_model.load_state_dict(torch.load(\"/kaggle/input/gpt2-cpu/pytorch/default/1/gpt2_ddp_model_2_cpu.pth\", weights_only=True))\n#     snapshot_model.to(device)\n#     # print()\n    \n#     start_context = \"Write Python code for addition of two numbers:\"\n#     tokenizer = tiktoken.get_encoding(\"gpt2\")\n    \n#     max_new_tokens= 2**power\n#     # print(\"use_cache=False: \")\n#     token_ids, time_elps, token_sec = generate(\n#         model=snapshot_model,\n#         idx=text_to_token_ids(start_context, tokenizer),\n#         max_new_tokens= max_new_tokens,\n#         context_size=GPT_CONFIG_124M[\"context_length\"]\n#     )\n    \n#     use_cache_false_list.append((max_new_tokens, token_sec, time_elps ,token_ids_to_text(token_ids, tokenizer)))\n\n\n    \n#     # print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T08:32:31.150269Z","iopub.status.idle":"2025-04-07T08:32:31.150581Z","shell.execute_reply":"2025-04-07T08:32:31.150462Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n# json.dump(use_cache_false_list, open(\"use_cache_false_list.json\", \"w\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T08:09:35.122660Z","iopub.execute_input":"2025-04-07T08:09:35.122908Z","iopub.status.idle":"2025-04-07T08:09:35.127287Z","shell.execute_reply.started":"2025-04-07T08:09:35.122888Z","shell.execute_reply":"2025-04-07T08:09:35.126453Z"}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"# use_cache_false_list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T08:19:13.031485Z","iopub.execute_input":"2025-04-07T08:19:13.031766Z","iopub.status.idle":"2025-04-07T08:19:13.034868Z","shell.execute_reply.started":"2025-04-07T08:19:13.031746Z","shell.execute_reply":"2025-04-07T08:19:13.034220Z"}},"outputs":[],"execution_count":70},{"cell_type":"code","source":"# use_cache_true_list = []\n# for power in tqdm(range(1, 12)):\n#     GPT_CONFIG_124M = {\n#                             \"vocab_size\": 50257,   # Vocabulary size\n#                             \"context_length\": 512, # Shortened context length (orig: 1024)\n#                             \"emb_dim\": 768,        # Embedding dimension\n#                             \"n_heads\": 12,         # Number of attention heads\n#                             \"n_layers\": 12,        # Number of layers\n#                             \"drop_rate\": 0.1,      # Dropout rate\n#                             \"qkv_bias\": False,      # Query-key-value bias\n#                             \"use_cache\" : True\n#                         }\n    \n#     cfg = GPT_CONFIG_124M\n    \n#     snapshot_model = GPTModel(GPT_CONFIG_124M)\n    \n#     device = torch.device(\"cuda\")\n    \n#     snapshot_model.load_state_dict(torch.load(\"/kaggle/input/gpt2-cpu/pytorch/default/1/gpt2_ddp_model_2_cpu.pth\", weights_only=True))\n#     snapshot_model.to(device)\n#     # print()\n    \n#     start_context = \"Write Python code for addition of two numbers:\"\n#     tokenizer = tiktoken.get_encoding(\"gpt2\")\n    \n#     max_new_tokens= 2**power\n#     # print(\"use_cache=False: \")\n#     token_ids, time_elps, token_sec = generate(\n#         model=snapshot_model,\n#         idx=text_to_token_ids(start_context, tokenizer),\n#         max_new_tokens= max_new_tokens,\n#         context_size=GPT_CONFIG_124M[\"context_length\"]\n#     )\n    \n#     use_cache_true_list.append((max_new_tokens, token_sec, time_elps ,token_ids_to_text(token_ids, tokenizer)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T08:32:32.026536Z","iopub.execute_input":"2025-04-07T08:32:32.026892Z","iopub.status.idle":"2025-04-07T08:32:32.030787Z","shell.execute_reply.started":"2025-04-07T08:32:32.026851Z","shell.execute_reply":"2025-04-07T08:32:32.029800Z"}},"outputs":[],"execution_count":78},{"cell_type":"code","source":"import json\n# json.dump(use_cache_true_list, open(\"use_cache_true_list.json\", \"w\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T08:09:35.128707Z","iopub.execute_input":"2025-04-07T08:09:35.129027Z","iopub.status.idle":"2025-04-07T08:09:35.147148Z","shell.execute_reply.started":"2025-04-07T08:09:35.128999Z","shell.execute_reply":"2025-04-07T08:09:35.146327Z"}},"outputs":[],"execution_count":68},{"cell_type":"code","source":"# use_cache_true_list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T08:32:36.549720Z","iopub.execute_input":"2025-04-07T08:32:36.550017Z","iopub.status.idle":"2025-04-07T08:32:36.553741Z","shell.execute_reply.started":"2025-04-07T08:32:36.549994Z","shell.execute_reply":"2025-04-07T08:32:36.552935Z"}},"outputs":[],"execution_count":79},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# snapshot_model = GPTModel(GPT_CONFIG_124M)\n\n# device = torch.device(\"cpu\")\n\n# snapshot_model.load_state_dict(torch.load(\"/kaggle/input/gpt2-cpu/pytorch/default/1/gpt2_ddp_model_2_cpu.pth\", weights_only=True))\n# snapshot_model.to(device)\n# print()\n\n# from torch.quantization import quantize_dynamic\n# model_quantized = quantize_dynamic(\n#     model=snapshot_model, qconfig_spec={nn.Linear}, dtype=torch.qint8, inplace=False\n# )\n\n# torch.save(model_quantized.to(torch.device(\"cpu\")).state_dict(), \"GPT2_quantize_1.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T07:44:07.474181Z","iopub.execute_input":"2025-04-07T07:44:07.474500Z","iopub.status.idle":"2025-04-07T07:44:07.478088Z","shell.execute_reply.started":"2025-04-07T07:44:07.474476Z","shell.execute_reply":"2025-04-07T07:44:07.477117Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"GPT_CONFIG_124M = {\n                        \"vocab_size\": 50257,   # Vocabulary size\n                        \"context_length\": 512, # Shortened context length (orig: 1024)\n                        \"emb_dim\": 768,        # Embedding dimension\n                        \"n_heads\": 12,         # Number of attention heads\n                        \"n_layers\": 12,        # Number of layers\n                        \"drop_rate\": 0.1,      # Dropout rate\n                        \"qkv_bias\": False,      # Query-key-value bias\n                        \"use_cache\" : True\n                    }\n\ncfg = GPT_CONFIG_124M\n\nsnapshot_model = GPTModel(GPT_CONFIG_124M)\n\ndevice = torch.device(\"cuda\")\n\nsnapshot_model.load_state_dict(torch.load(\"/kaggle/input/gpt2-cpu/pytorch/default/1/gpt2_ddp_model_2_cpu.pth\", weights_only=True))\nsnapshot_model.to(device)\n# print()\n\nstart_context = '''Edit the script to print out the number of times each letter appears in the given sentence We are learning Python Let's turn up the heat! It's getting hot in here! \"\"\"Edit the script to print out the number of times each letter appears in the given sentence.\"\"\"\n\nsentence = 'We are learning Python'\n\n# Create an empty dictionary\nletter_freq = {}\n\n# Iterate through the sentence\nfor c in sentence:\n# If the character is a letter\nif c.isalpha():\n# Increment the frequency in the dictionary\nletter_freq[c] = letter_freq.setdefault(c, 0) + 1\n\n# Print out the frequency of each letter'''\n\ntokenizer = tiktoken.get_encoding(\"gpt2\")\n\n\nprint(\"use_cache=True: \")\ntoken_ids = generate(\n    model=snapshot_model,\n    idx=text_to_token_ids(start_context, tokenizer),\n    max_new_tokens=256,\n    context_size=GPT_CONFIG_124M[\"context_length\"]\n)\n\n\nprint(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T08:34:46.864179Z","iopub.execute_input":"2025-04-07T08:34:46.864498Z","iopub.status.idle":"2025-04-07T08:34:58.066074Z","shell.execute_reply.started":"2025-04-07T08:34:46.864475Z","shell.execute_reply":"2025-04-07T08:34:58.065128Z"}},"outputs":[{"name":"stdout","text":"use_cache=True: \ntime in seconds:  9.362242221832275\ntokens/s:  54.687754051699486\nOutput text:\n Edit the script to print out the number of times each letter appears in the given sentence We are learning Python Let's turn up the heat! It's getting hot in here! \"\"\"Edit the script to print out the number of times each letter appears in the given sentence.\"\"\"\n\nsentence = 'We are learning Python'\n\n# Create an empty dictionary\nletter_freq = {}\n\n# Iterate through the sentence\nfor c in sentence:\n# If the character is a letter\nif c.isalpha():\n# Increment the frequency in the dictionary\nletter_freq[c] = letter_freq.setdefault(c, 0) + 1\n\n# Print out the frequency of each letter in the sentence\nprint(f\"Letter counts: {letter_freq:.getcnt_frequency()}\")\n```<|endoftext|>Create a function in Python to calculate the compound interest given a principal amount of interest rate and other interest rate: 10000\n 10000: 10 years  ```python\ndef calculate_interest(principal, rate, time_period):\n    interest = principal_amount * (1 + rate/100)**time\n    return interest\n\n# Test\nprincipal_amount = 10\ntime_amount = calculate_compound(principal, time_period, 0.2, time_period))\nprint(f\"Function took the interest for the given principal amount and is: \" + str(f\"This is a perfect time period of time.\".format(calculate_compound interest(time_amount, time_period))\n```<|endoftext|>Develop a Python function to calculate the compound interest rate, rate of interest  ```python\ndef compute_compound(principal, rate, time):\n    # calculate the interest \n    interest = principal * (1 + rate / 100) / 100\n\n","output_type":"stream"}],"execution_count":85},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"start_context = \"Write Python code for addition of two numbers:\"\ntokenizer = tiktoken.get_encoding(\"gpt2\")\n\n\ntoken_ids = generate(\n    model=snapshot_model,\n    idx=text_to_token_ids(start_context, tokenizer),\n    max_new_tokens=512,\n    context_size=GPT_CONFIG_124M[\"context_length\"]\n)\n\nprint(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:59:21.789715Z","iopub.execute_input":"2025-04-05T19:59:21.790016Z","iopub.status.idle":"2025-04-05T19:59:36.062557Z","shell.execute_reply.started":"2025-04-05T19:59:21.789993Z","shell.execute_reply":"2025-04-05T19:59:36.061558Z"}},"outputs":[{"name":"stdout","text":"time in seconds:  14.26278018951416\ntokens/s:  35.897629578307374\nOutput text:\n Write Python code for addition of two numbers: [4, 8, 9, 11]\nnumlist: [6, 8, 9, 11]  ```python\ndef divide_numbers(numlist, nums):\n    return [num + nums for num in nums]\n```<|endoftext|>Write a Python program to sort a given list of dictionaries by a given key list_of_dicts = [\n{\n    'name': 'John',\n    'age': 28,\n    'age': 30\n}\n\nlist_of_dicts = [\n    'John',\n  'age': 25\n}\n\nkey = [value for (name, value) in list_of_dicts if key.age()]\nvalue = [{\n    'name': 'John',\n    'age': 28\n}\n\nlist_of_dicts = list_of_dicts(list_of_dicts, key=list_of_dicts)\n```<|endoftext|>Create a Python program to calculate the area of a triangle given 3 sides a = 5\nb = 3\nc = 4\n\ndef calculateArea(a, b, c):\n    # calculate the semi-perimeter\n    s = (a + b + c) / 2\n    # calculate the area\n    area = (s*(s*(s-a)*(s-b)*(s-c)) ** 0.5\n    return area\n\narea = calculateTriangle(3, 4, 9)\nprint(area)\n```<|endoftext|>Create a Python script to download and save the data from a URL URL URL to a file URL URL: https://www.example.com/data  ```python\nimport requests\nimport requests\n\ndef download_data(url):\n    response = requests.get(url)\n    soup = bs4.BeautifulSoup(response.text, 'html.parser')\n\n    # save data from file\n    page_data = soup.find_all('div', {'class': 'data-page'})\n\n    # save the data\n    data = []\n    for link in soup.find_all('a'):\n    \n","output_type":"stream"}],"execution_count":101},{"cell_type":"code","source":"time in seconds:  14.066362380981445\ntokens/s:  36.398891634716755\nOutput text:\n Write Python code for addition of two numbers: [2, 5, 8] and 7  ```python\ndef add_two_numbers(num1, num2):\n return num1 + num2\n\nprint(add_two_numbers(num1, num2)) # 7","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"time in seconds:  14.650187730789185\ntokens/s:  34.94835761892447","metadata":{},"outputs":[],"execution_count":null}]}