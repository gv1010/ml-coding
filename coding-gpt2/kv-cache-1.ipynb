{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nimport torch.distributed as dist\nimport torch.optim as optim\nimport tiktoken\nfrom torch.utils.data import DataLoader, Dataset\nimport os\n# import matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport torch.nn.functional as F\nimport time\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:32:23.717646Z","iopub.execute_input":"2025-04-05T19:32:23.718004Z","iopub.status.idle":"2025-04-05T19:32:23.722970Z","shell.execute_reply.started":"2025-04-05T19:32:23.717980Z","shell.execute_reply":"2025-04-05T19:32:23.721845Z"}},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":"### Gelu Layer Norm","metadata":{}},{"cell_type":"code","source":"class GELU(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return 0.5 * x * (1 + torch.tanh(\n            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n            (x + 0.044715 * torch.pow(x, 3))\n        ))\n\nclass FeedForward(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n            GELU(),\n            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n    \nclass LayerNorm(nn.Module):\n    def __init__(self, emb_dim, eps=1e-5):\n        super().__init__()\n        self.eps = eps\n        self.scale = nn.Parameter(torch.ones(emb_dim))\n        self.shift = nn.Parameter(torch.zeros(emb_dim))\n    \n    def forward(self, x):\n        # This layer does nothing and just returns its input.\n\n        mean = x.mean(dim = -1, keepdim = True)\n        var = x.var(dim=-1,keepdim = True, unbiased = False)\n        norm_x = (x-mean)/torch.sqrt(var+self.eps)\n        return self.scale * norm_x + self.shift","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:32:24.089610Z","iopub.execute_input":"2025-04-05T19:32:24.089982Z","iopub.status.idle":"2025-04-05T19:32:24.098115Z","shell.execute_reply.started":"2025-04-05T19:32:24.089944Z","shell.execute_reply":"2025-04-05T19:32:24.097001Z"}},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":"### RoPE","metadata":{}},{"cell_type":"code","source":"def rotate_half(x):\n    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n    x1 = x[..., :x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2:]\n    return torch.cat((-x2, x1), dim=-1)\n\ndef apply_rotary_pos_emb(q, k, cos, sin, offset: int = 0):\n    # The first two dimensions of q and k should be (batch, num_heads).\n    # The last dimension is the embedding dimension, which we'll rotate.\n    q_rotated = (q * cos[:, offset:offset + q.shape[2], :]) + (rotate_half(q) * sin[:, offset:offset + q.shape[2], :])\n    k_rotated = (k * cos[:, offset:offset + k.shape[2], :]) + (rotate_half(k) * sin[:, offset:offset + k.shape[2], :])\n    return q_rotated, k_rotated\n\nclass RoPE(nn.Module):\n    def __init__(self, dim, context_length, base=10000):\n        super().__init__()\n        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n        t = torch.arange(context_length, device=inv_freq.device)\n        freqs = torch.outer(t, inv_freq)\n        emb = torch.cat((freqs, freqs), dim=-1)\n        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :])\n        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :])\n\n    def forward(self, q, k):\n        # q and k have shapes (batch, num_heads, seq_len, head_dim)\n        seq_len = q.shape[2]\n        cos = self.cos_cached[:, :, :seq_len, :]\n        sin = self.sin_cached[:, :, :seq_len, :]\n        q_rotated, k_rotated = apply_rotary_pos_emb(q, k, cos, sin)\n        return q_rotated, k_rotated\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:32:24.445674Z","iopub.execute_input":"2025-04-05T19:32:24.446122Z","iopub.status.idle":"2025-04-05T19:32:24.455118Z","shell.execute_reply.started":"2025-04-05T19:32:24.446075Z","shell.execute_reply":"2025-04-05T19:32:24.454094Z"}},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"### Multi Head Attention","metadata":{}},{"cell_type":"code","source":"\nclass MultiHeadCasualAttention(nn.Module):\n    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False, use_cache = False):\n        super().__init__()\n        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n\n        self.context_length = context_length\n        self.use_cache = use_cache\n        self.d_out = d_out\n        self.num_heads = num_heads\n        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n\n        self.cache_k = None\n        self.cache_v = None\n\n        # self.rope = RoPE(self.head_dim, context_length)\n\n    \n    def forward(self, x, prev_key_value=None):\n        b, num_tokens, d_in = x.shape\n\n        \n        if self.use_cache:\n            \n            x_last = x[:, -1:, :]\n            # print(\"x_last: \", x_last.shape)\n            keys = self.W_key(x_last)\n            queries = self.W_query(x_last)\n            values = self.W_value(x_last)\n\n            keys = keys.view(b, -1, self.num_heads, self.head_dim)\n            values = values.view(b, -1, self.num_heads, self.head_dim)\n            queries = queries.view(b, -1, self.num_heads, self.head_dim)\n\n            if prev_key_value is not None:\n                self.cache_k, self.cache_v = prev_key_value\n            else:\n                self.cache_k = keys\n                self.cache_v = values\n            \n            self.cache_k = torch.cat((self.cache_k, keys[:, -1:, :, :]), dim = 1)\n            self.cache_v = torch.cat((self.cache_v, values[:, -1:, :, :]), dim = 1)\n            # print(\"concat cache_k: \", self.cache_k.shape)\n\n            if self.cache_k.shape[1] > self.context_length:\n                self.cache_k = self.cache_k[:, -self.context_length:, :, :]\n                self.cache_v = self.cache_v[:, -self.context_length:, :, :]\n            \n            keys = self.cache_k.transpose(1, 2)\n            queries = queries[:, -1:, :, :].transpose(1, 2)\n            values = self.cache_v.transpose(1, 2)\n\n            # queries, keys = self.rope(queries, keys)\n\n            # mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n            \n            # Use scaled_dot_product_attention which can leverage Flash Attention\n            attn_output = F.scaled_dot_product_attention(\n                queries, keys, values,\n                attn_mask=None,\n                dropout_p=self.dropout.p if self.training else 0.0, # Apply dropout if in training mode\n                is_causal=True # Mask is provided, so is_causal is False\n            )\n\n            # Shape: (b, num_heads, num_tokens, head_dim) -> (b, num_tokens, num_heads, head_dim)\n            context_vec = attn_output.transpose(1, 2).contiguous().view(b, -1, self.d_out)\n\n            context_vec = self.out_proj(context_vec)  # optional projection\n\n            # print(\"final cache_k:\", self.cache_k.shape)\n\n            return context_vec, (self.cache_k, self.cache_v)\n\n        else:\n            keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n            queries = self.W_query(x)\n            values = self.W_value(x)\n\n    \n            keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n            values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n            queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n\n            # print(\"keys: \",keys.shape)\n            if prev_key_value is None:\n                self.cache_k = keys\n                self.cache_v = values\n            \n            \n            if self.cache_k.shape[1] > self.context_length:\n                self.cache_k = self.cache_k[:, -self.context_length:, :, :]\n                self.cache_v = self.cache_v[:, -self.context_length:, :, :]\n\n            # print(\"cache_k: \",self.cache_k.shape)\n            \n            keys = self.cache_k.transpose(1, 2)\n            queries = queries.transpose(1, 2)\n            values = self.cache_v.transpose(1, 2)\n\n            # queries, keys = self.rope(queries, keys)\n\n            mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n            \n            # Use scaled_dot_product_attention which can leverage Flash Attention\n            attn_output = F.scaled_dot_product_attention(\n                queries, keys, values,\n                attn_mask= None, #mask_bool,\n                dropout_p=self.dropout.p if self.training else 0.0, \n                is_causal=True # Mask is provided, so is_causal is False\n            )\n\n            # Shape: (b, num_heads, num_tokens, head_dim) -> (b, num_tokens, num_heads, head_dim)\n            context_vec = attn_output.transpose(1, 2).contiguous().view(b, num_tokens, self.d_out)\n\n            context_vec = self.out_proj(context_vec)  # optional projection\n\n            return context_vec, (self.cache_k, self.cache_v) if self.use_cache else (None, None)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T20:01:13.411334Z","iopub.execute_input":"2025-04-05T20:01:13.411666Z","iopub.status.idle":"2025-04-05T20:01:13.428535Z","shell.execute_reply.started":"2025-04-05T20:01:13.411642Z","shell.execute_reply":"2025-04-05T20:01:13.427657Z"}},"outputs":[],"execution_count":102},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# GPT_CONFIG_124M","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:32:29.533095Z","iopub.execute_input":"2025-04-05T19:32:29.533372Z","iopub.status.idle":"2025-04-05T19:32:29.536599Z","shell.execute_reply.started":"2025-04-05T19:32:29.533351Z","shell.execute_reply":"2025-04-05T19:32:29.535662Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# import torch.nn.functional as F\n\n\n# cfg = {\n#         'vocab_size': 50257,\n#         'context_length': 4,\n#         'emb_dim': 768,\n#         'n_heads': 12,\n#         'n_layers': 12,\n#         'drop_rate': 0.1,\n#         'qkv_bias': False,\n#         'use_cache': True\n#         }\n\n# attention = MultiHeadCasualAttention(\n#     d_in=cfg[\"emb_dim\"],\n#     d_out=cfg[\"emb_dim\"],\n#     context_length=cfg[\"context_length\"],\n#     dropout=cfg[\"drop_rate\"],\n#     num_heads=cfg[\"n_heads\"],\n#     qkv_bias=cfg[\"qkv_bias\"],\n#     use_cache=True\n# )\n\n\n# batch_size = 8\n# initial_seq_len = 6\n# embedding_dim = cfg[\"emb_dim\"]\n# initial_input = torch.randn(batch_size, initial_seq_len, embedding_dim)\n\n# output, (prev_k, prev_v) = attention(initial_input)\n# print(len(prev_k))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:32:30.002018Z","iopub.execute_input":"2025-04-05T19:32:30.002332Z","iopub.status.idle":"2025-04-05T19:32:30.006731Z","shell.execute_reply.started":"2025-04-05T19:32:30.002309Z","shell.execute_reply":"2025-04-05T19:32:30.005614Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"\n# # Simulate generating a few more tokens\n# num_generate_steps = 30\n# generated_sequence = [output]\n# for _ in range(num_generate_steps):\n#     next_input = torch.randn(batch_size, 1, embedding_dim) # In a real scenario, this would be the embedding of the predicted next token\n\n#     # 4. Perform forward pass with the previous past_key_value\n#     output, prev_key_value = attention(next_input, prev_key_value=(prev_k, prev_v))\n\n#     # print(\"Shape of output after next generation step:\", output.shape)\n#     # if prev_key_value is not None:\n#     #     print(\"Shape of cached keys:\", prev_key_value[0].shape)\n#     #     print(\"Shape of cached values:\", prev_key_value[1].shape)\n\n#     generated_sequence.append(output)\n\n# len(generated_sequence), (prev_key_value[0].shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:50:46.237373Z","iopub.execute_input":"2025-04-05T19:50:46.237673Z","iopub.status.idle":"2025-04-05T19:50:46.241366Z","shell.execute_reply.started":"2025-04-05T19:50:46.237651Z","shell.execute_reply":"2025-04-05T19:50:46.240278Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"# prev_key_value[0].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:32:33.997115Z","iopub.execute_input":"2025-04-05T19:32:33.997428Z","iopub.status.idle":"2025-04-05T19:32:34.001116Z","shell.execute_reply.started":"2025-04-05T19:32:33.997404Z","shell.execute_reply":"2025-04-05T19:32:34.000141Z"}},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":"### Transformers block","metadata":{}},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.attention = MultiHeadCasualAttention(\n            d_in= cfg[\"emb_dim\"],\n            d_out=cfg[\"emb_dim\"],\n            context_length=cfg[\"context_length\"],\n            dropout=cfg[\"drop_rate\"],\n            num_heads=cfg[\"n_heads\"],\n            qkv_bias=cfg[\"qkv_bias\"],\n            use_cache=cfg[\"use_cache\"]\n        )\n\n        self.ff = FeedForward(cfg)\n        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n\n    def forward(self, x, prev_key_value=None):\n        # This block does nothing and just returns its input.\n        shortcut = x\n        x = self.norm1(x)\n        x, updated_key_value = self.attention(x, prev_key_value)\n        x = self.drop_shortcut(x)\n\n        x = x + shortcut\n\n        shortcut = x\n\n        x = self.norm2(x)\n        x = self.ff(x)\n        x = self.drop_shortcut(x)\n        x = x + shortcut\n        return x, updated_key_value","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T20:01:16.489532Z","iopub.execute_input":"2025-04-05T20:01:16.489854Z","iopub.status.idle":"2025-04-05T20:01:16.496192Z","shell.execute_reply.started":"2025-04-05T20:01:16.489821Z","shell.execute_reply":"2025-04-05T20:01:16.495413Z"}},"outputs":[],"execution_count":103},{"cell_type":"markdown","source":"### GPT","metadata":{}},{"cell_type":"code","source":"# Assuming you have your GPT-2 model defined already\nclass GPTModel(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n\n        self.trf_blocks = nn.ModuleList(\n            [TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n        self.out_head = nn.Linear(\n            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n        )\n\n    def forward(self, in_idx, prev_key_value = None):\n        # print(in_idx.shape)\n\n        device = next(self.parameters()).device\n        in_idx = in_idx.to(device)\n        \n        batch_size, seq_len = in_idx.shape\n        tok_embeds = self.tok_emb(in_idx)\n        pos_embeds = self.pos_emb(torch.arange(seq_len, device = in_idx.device))\n        x = tok_embeds + pos_embeds\n        x = self.drop_emb(x)\n\n        key_value_states = []\n        for i, trf_block in enumerate(self.trf_blocks):\n            if prev_key_value is None:\n                layer_prev_key_value = None\n            else:\n                layer_prev_key_value = prev_key_value[i]\n            x, present_key_value = trf_block(x, layer_prev_key_value)\n            key_value_states.append(present_key_value)\n\n        # x = self.trf_blocks(x)\n\n        x = self.final_norm(x)\n        logits = self.out_head(x)\n        return logits, key_value_states","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T20:01:18.621480Z","iopub.execute_input":"2025-04-05T20:01:18.621775Z","iopub.status.idle":"2025-04-05T20:01:18.629089Z","shell.execute_reply.started":"2025-04-05T20:01:18.621753Z","shell.execute_reply":"2025-04-05T20:01:18.628118Z"}},"outputs":[],"execution_count":104},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    # idx.to(device)\n    # For-loop is the same as before: Get logits, and only focus on last time step\n    start = time.time()\n    for _ in range(max_new_tokens):\n        \n        idx_cond = idx[:, -context_size:]\n        idx_cond.to(device)\n        prev_key_value = None\n        with torch.no_grad():\n            \n            logits, prev_key_value = model(idx_cond, prev_key_value)\n        logits = logits[:, -1, :]\n\n        # New: Filter logits with top_k sampling\n        if top_k is not None:\n            # Keep only top_k values\n            top_logits, _ = torch.topk(logits, top_k)\n            min_val = top_logits[:, -1]\n            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n\n        # New: Apply temperature scaling\n        if temperature > 0.0:\n            logits = logits / temperature\n\n            # Apply softmax to get probabilities\n            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n\n            # Sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n\n        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n        else:\n            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n\n        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n            break\n\n        # Same as before: append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next.to(torch.device(\"cpu\"))), dim=1)  # (batch_size, num_tokens+1)\n    te = time.time() - start\n    print(\"time in seconds: \",te )\n    print(\"tokens/s: \", context_size/te)\n    return idx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T20:01:20.446428Z","iopub.execute_input":"2025-04-05T20:01:20.446770Z","iopub.status.idle":"2025-04-05T20:01:20.454007Z","shell.execute_reply.started":"2025-04-05T20:01:20.446740Z","shell.execute_reply":"2025-04-05T20:01:20.453106Z"}},"outputs":[],"execution_count":105},{"cell_type":"code","source":"import tiktoken\n\ndef text_to_token_ids(text, tokenizer):\n    # encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n    return encoded_tensor\n\ndef token_ids_to_text(token_ids, tokenizer):\n    flat = token_ids.squeeze(0) # remove batch dimension\n    return tokenizer.decode(flat.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T20:01:22.609938Z","iopub.execute_input":"2025-04-05T20:01:22.610306Z","iopub.status.idle":"2025-04-05T20:01:22.615209Z","shell.execute_reply.started":"2025-04-05T20:01:22.610277Z","shell.execute_reply":"2025-04-05T20:01:22.614253Z"}},"outputs":[],"execution_count":106},{"cell_type":"code","source":"GPT_CONFIG_124M = {\n                        \"vocab_size\": 50257,   # Vocabulary size\n                        \"context_length\": 512, # Shortened context length (orig: 1024)\n                        \"emb_dim\": 768,        # Embedding dimension\n                        \"n_heads\": 12,         # Number of attention heads\n                        \"n_layers\": 12,        # Number of layers\n                        \"drop_rate\": 0.1,      # Dropout rate\n                        \"qkv_bias\": False,      # Query-key-value bias\n                        \"use_cache\" : False\n                    }\n\ncfg = GPT_CONFIG_124M\n\nsnapshot_model = GPTModel(GPT_CONFIG_124M)\n\ndevice = torch.device(\"cuda\")\n\nsnapshot_model.load_state_dict(torch.load(\"/kaggle/input/gpt2-cpu/pytorch/default/1/gpt2_ddp_model_2_cpu.pth\", weights_only=True))\nsnapshot_model.to(device)\n# print()\n\nstart_context = \"Write Python code for addition of two numbers:\"\ntokenizer = tiktoken.get_encoding(\"gpt2\")\n\n\nprint(\"use_cache=False: \")\ntoken_ids = generate(\n    model=snapshot_model,\n    idx=text_to_token_ids(start_context, tokenizer),\n    max_new_tokens=512,\n    context_size=GPT_CONFIG_124M[\"context_length\"]\n)\n\n\nprint(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T20:02:55.481907Z","iopub.execute_input":"2025-04-05T20:02:55.482305Z","iopub.status.idle":"2025-04-05T20:03:11.343805Z","shell.execute_reply.started":"2025-04-05T20:02:55.482278Z","shell.execute_reply":"2025-04-05T20:03:11.343029Z"}},"outputs":[{"name":"stdout","text":"use_cache=False: \ntime in seconds:  14.05784797668457\ntokens/s:  36.42093731908111\nOutput text:\n Write Python code for addition of two numbers: [2, 5, 8, 9]  ```python\ndef add_two_numbers(num1, num2):\n return num1 + num2\n\nprint(add_two_numbers([2, 5, 8, 9])) # Outputs 8\n```<|endoftext|>Write a Python program to calculate the product of two given matrices [1, 2], [3, 4]], [4, 5]], [[5, 6], [7, 8]]]  ```python\ndef multiply_product(m1, m2):\n    product = len(m1)\n    row = len(m2)\n\n    # store the result\n    result = [[0 for x in range(row)] for y in range(len(m1))]\n\n    # iterate through rows of columns\n    for i in range(row):\n        # iterate through columns of matrix\n      for j in range(len(m1)):\n            # iterate through columns\n            for k in range(len(m2[0])):\n                result[i][j] += m1[i][k] * m2[k][j]\n\n    return result\n\nm1 = [[1, 2], [3, 4]]\nm2 = [[5, 6], [7, 8, 9]]\n\ndot_matrix = multiply_matrices(m1, m2)\nprint(result)\n```<|endoftext|>Create a Python program to print out the number of days since next one is {1, 2, 3, 4: 5  ```python\nfrom datetime import datetime\n\ndef days_since(date1, date2):\n    date1 = datetime.strptime(date1, \"%Y\")\n    date2 = datetime.strptime(date2, \"%Y-%m-%d\")\n    return abs((date1).days) <= date2)\n\nprint(days_days_to_days(2020, 1))\n```<|endoftext|>Implement a Python function to check\n","output_type":"stream"}],"execution_count":108},{"cell_type":"code","source":"GPT_CONFIG_124M = {\n                        \"vocab_size\": 50257,   # Vocabulary size\n                        \"context_length\": 512, # Shortened context length (orig: 1024)\n                        \"emb_dim\": 768,        # Embedding dimension\n                        \"n_heads\": 12,         # Number of attention heads\n                        \"n_layers\": 12,        # Number of layers\n                        \"drop_rate\": 0.1,      # Dropout rate\n                        \"qkv_bias\": False,      # Query-key-value bias\n                        \"use_cache\" : True\n                    }\n\ncfg = GPT_CONFIG_124M\n\nsnapshot_model = GPTModel(GPT_CONFIG_124M)\n\ndevice = torch.device(\"cuda\")\n\nsnapshot_model.load_state_dict(torch.load(\"/kaggle/input/gpt2-cpu/pytorch/default/1/gpt2_ddp_model_2_cpu.pth\", weights_only=True))\nsnapshot_model.to(device)\n# print()\n\nstart_context = \"Write Python code for addition of two numbers:\"\ntokenizer = tiktoken.get_encoding(\"gpt2\")\n\n\nprint(\"use_cache=True: \")\ntoken_ids = generate(\n    model=snapshot_model,\n    idx=text_to_token_ids(start_context, tokenizer),\n    max_new_tokens=512,\n    context_size=GPT_CONFIG_124M[\"context_length\"]\n)\n\n\nprint(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T20:04:00.049889Z","iopub.execute_input":"2025-04-05T20:04:00.050293Z","iopub.status.idle":"2025-04-05T20:04:13.564769Z","shell.execute_reply.started":"2025-04-05T20:04:00.050267Z","shell.execute_reply":"2025-04-05T20:04:13.563734Z"}},"outputs":[{"name":"stdout","text":"use_cache=True: \ntime in seconds:  11.777802228927612\ntokens/s:  43.471607864366256\nOutput text:\n Write Python code for addition of two numbers:\n# 2 3 4 5 7 8.........gridites...Boxing... Did like C. app......v')\n\n# W\n\n# CR... async \",\"?, performed and come document using slice ''.\nas.bit... log in rangeStart... suspicious\")\nAP ObjectTotal were a given input... . . .\npython\n\n# Node:\n# Executing storage.....t think??, does these two four,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, 1 even 29,mode in:\n\n\ndictionary... . . sudo()))\n\nmy_____return None\n\n\n# to the the returned . / \\ \\\\\\ \\\\\\ \\\\\\ unus? w one another                                                                                                                                                                                                                                                                                          \n","output_type":"stream"}],"execution_count":109},{"cell_type":"code","source":"start_context = \"Write Python code for addition of two numbers:\"\ntokenizer = tiktoken.get_encoding(\"gpt2\")\n\n\ntoken_ids = generate(\n    model=snapshot_model,\n    idx=text_to_token_ids(start_context, tokenizer),\n    max_new_tokens=512,\n    context_size=GPT_CONFIG_124M[\"context_length\"]\n)\n\nprint(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:59:21.789715Z","iopub.execute_input":"2025-04-05T19:59:21.790016Z","iopub.status.idle":"2025-04-05T19:59:36.062557Z","shell.execute_reply.started":"2025-04-05T19:59:21.789993Z","shell.execute_reply":"2025-04-05T19:59:36.061558Z"}},"outputs":[{"name":"stdout","text":"time in seconds:  14.26278018951416\ntokens/s:  35.897629578307374\nOutput text:\n Write Python code for addition of two numbers: [4, 8, 9, 11]\nnumlist: [6, 8, 9, 11]  ```python\ndef divide_numbers(numlist, nums):\n    return [num + nums for num in nums]\n```<|endoftext|>Write a Python program to sort a given list of dictionaries by a given key list_of_dicts = [\n{\n    'name': 'John',\n    'age': 28,\n    'age': 30\n}\n\nlist_of_dicts = [\n    'John',\n  'age': 25\n}\n\nkey = [value for (name, value) in list_of_dicts if key.age()]\nvalue = [{\n    'name': 'John',\n    'age': 28\n}\n\nlist_of_dicts = list_of_dicts(list_of_dicts, key=list_of_dicts)\n```<|endoftext|>Create a Python program to calculate the area of a triangle given 3 sides a = 5\nb = 3\nc = 4\n\ndef calculateArea(a, b, c):\n    # calculate the semi-perimeter\n    s = (a + b + c) / 2\n    # calculate the area\n    area = (s*(s*(s-a)*(s-b)*(s-c)) ** 0.5\n    return area\n\narea = calculateTriangle(3, 4, 9)\nprint(area)\n```<|endoftext|>Create a Python script to download and save the data from a URL URL URL to a file URL URL: https://www.example.com/data  ```python\nimport requests\nimport requests\n\ndef download_data(url):\n    response = requests.get(url)\n    soup = bs4.BeautifulSoup(response.text, 'html.parser')\n\n    # save data from file\n    page_data = soup.find_all('div', {'class': 'data-page'})\n\n    # save the data\n    data = []\n    for link in soup.find_all('a'):\n    \n","output_type":"stream"}],"execution_count":101},{"cell_type":"code","source":"time in seconds:  14.066362380981445\ntokens/s:  36.398891634716755\nOutput text:\n Write Python code for addition of two numbers: [2, 5, 8] and 7  ```python\ndef add_two_numbers(num1, num2):\n return num1 + num2\n\nprint(add_two_numbers(num1, num2)) # 7","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"time in seconds:  14.650187730789185\ntokens/s:  34.94835761892447","metadata":{},"outputs":[],"execution_count":null}]}