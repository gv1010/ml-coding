{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\ntorch.cuda.device_count()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T14:13:34.504902Z","iopub.execute_input":"2025-03-19T14:13:34.505220Z","iopub.status.idle":"2025-03-19T14:13:37.625665Z","shell.execute_reply.started":"2025-03-19T14:13:34.505189Z","shell.execute_reply":"2025-03-19T14:13:37.624801Z"}},"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}],"execution_count":1},{"cell_type":"markdown","source":"**single node multi gpu**","metadata":{}},{"cell_type":"code","source":"%%writefile multigpu.py\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport torch.multiprocessing as mp\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.distributed import init_process_group, destroy_process_group\nimport os\n\ndef ddp_setup(rank, world_size):\n    \"\"\"\n    rank: unique id of eeach process\n    world_size: no of gpus or devices\n    \"\"\"\n\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"12355\"\n    torch.cuda.set_device(rank)\n    init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n\nclass MyTrainDataset(Dataset):\n    def __init__(self, size):\n        self.size = size\n        self.data = [(torch.rand(10), torch.rand(1)) for _ in range(size)]\n\n    def __len__(self):\n        return self.size\n    \n    def __getitem__(self, index):\n        return self.data[index]\n\nclass Trainer:\n    def __init__(self, model: torch.nn.Module,\n        train_data: DataLoader,\n        optimizer: torch.optim.Optimizer,\n        gpu_id: int,\n        save_every: int) -> None:\n\n        self.gpu_id = gpu_id\n        self.model = model.to(gpu_id)\n        self.optimizer = optimizer\n        self.train_data = train_data\n        self.save_every = save_every\n        self.model = DDP(model, device_ids = [gpu_id])\n\n\n    def _run_batch(self, source, targets):\n        self.optimizer.zero_grad()\n        output = self.model(source)\n        loss = F.cross_entropy(output, targets)\n        loss.backward()\n        self.optimizer.step()\n\n\n    def _run_epoch(self, epoch):\n        batch_size = len(next(iter(self.train_data))[0])\n        print(f\"[GPU {self.gpu_id}] Epoch {epoch} | \\\n                Batchsize: {batch_size} | Steps: {len(self.train_data)}\")\n\n        self.train_data.sampler.set_epoch(epoch)\n        for source, targets in self.train_data:\n            source = source.to(self.gpu_id)\n            targets = targets.to(self.gpu_id)\n            self._run_batch(source, targets)\n\n    def _save_checkpoint(self, epoch):\n        check_point = self.model.state_dict()\n        path = \"checkpoint.pt\"\n        torch.save(check_point, path)\n        print(f\"Epoch {epoch} | Training checkpoint saved at {path}\")\n\n    def train(self, max_epochs):\n        for epoch in range(max_epochs):\n            self._run_epoch(epoch)\n            if epoch % self.save_every == 0:\n                self._save_checkpoint(epoch)\n        \n\ndef load_train_objs():\n    train_set = MyTrainDataset(20248)\n    model = torch.nn.Linear(10, 1)\n    optimizer = torch.optim.SGD(model.parameters(), lr = 1e-3)\n    return train_set, model, optimizer\n\ndef prepare_dataloader(dataset, batch_size):\n    return DataLoader(dataset, batch_size=batch_size, pin_memory=True, shuffle=False, sampler = DistributedSampler(dataset))\n\ndef main(rank, world_size, total_epochs, save_every, batch_size):\n\n    ddp_setup(rank, world_size)\n    dataset, model, optimizer = load_train_objs()\n    train_data = prepare_dataloader(dataset, batch_size)\n    trainer = Trainer(model, train_data, optimizer, rank, save_every)\n    trainer.train(total_epochs)\n\nif __name__ == \"__main__\":\n    world_size = torch.cuda.device_count()\n    mp.spawn(main, args=(world_size, 20, 5, 16), nprocs=world_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T11:13:32.579609Z","iopub.execute_input":"2025-03-20T11:13:32.579932Z","iopub.status.idle":"2025-03-20T11:13:32.586673Z","shell.execute_reply.started":"2025-03-20T11:13:32.579904Z","shell.execute_reply":"2025-03-20T11:13:32.585493Z"}},"outputs":[{"name":"stdout","text":"Overwriting multigpu.py\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"%%script bash\npython3 multigpu.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T11:13:36.834637Z","iopub.execute_input":"2025-03-20T11:13:36.834943Z","iopub.status.idle":"2025-03-20T11:14:00.800904Z","shell.execute_reply.started":"2025-03-20T11:13:36.834916Z","shell.execute_reply":"2025-03-20T11:14:00.800206Z"}},"outputs":[{"name":"stdout","text":"[GPU 1] Epoch 0 |                 Batchsize: 16 | Steps: 633\nEpoch 0 | Training checkpoint saved at checkpoint.pt\n[GPU 1] Epoch 1 |                 Batchsize: 16 | Steps: 633\n[GPU 1] Epoch 2 |                 Batchsize: 16 | Steps: 633\n[GPU 1] Epoch 3 |                 Batchsize: 16 | Steps: 633\n[GPU 1] Epoch 4 |                 Batchsize: 16 | Steps: 633\n[GPU 1] Epoch 5 |                 Batchsize: 16 | Steps: 633\nEpoch 5 | Training checkpoint saved at checkpoint.pt\n[GPU 1] Epoch 6 |                 Batchsize: 16 | Steps: 633\n[GPU 1] Epoch 7 |                 Batchsize: 16 | Steps: 633\n[GPU 1] Epoch 8 |                 Batchsize: 16 | Steps: 633\n[GPU 1] Epoch 9 |                 Batchsize: 16 | Steps: 633\n[GPU 1] Epoch 10 |                 Batchsize: 16 | Steps: 633\nEpoch 10 | Training checkpoint saved at checkpoint.pt\n[GPU 1] Epoch 11 |                 Batchsize: 16 | Steps: 633\n[GPU 1] Epoch 12 |                 Batchsize: 16 | Steps: 633\n[GPU 1] Epoch 13 |                 Batchsize: 16 | Steps: 633\n[GPU 1] Epoch 14 |                 Batchsize: 16 | Steps: 633\n[GPU 1] Epoch 15 |                 Batchsize: 16 | Steps: 633\nEpoch 15 | Training checkpoint saved at checkpoint.pt\n[GPU 1] Epoch 16 |                 Batchsize: 16 | Steps: 633\n[GPU 1] Epoch 17 |                 Batchsize: 16 | Steps: 633\n[GPU 1] Epoch 18 |                 Batchsize: 16 | Steps: 633\n[GPU 1] Epoch 19 |                 Batchsize: 16 | Steps: 633\n[GPU 0] Epoch 0 |                 Batchsize: 16 | Steps: 633\nEpoch 0 | Training checkpoint saved at checkpoint.pt\n[GPU 0] Epoch 1 |                 Batchsize: 16 | Steps: 633\n[GPU 0] Epoch 2 |                 Batchsize: 16 | Steps: 633\n[GPU 0] Epoch 3 |                 Batchsize: 16 | Steps: 633\n[GPU 0] Epoch 4 |                 Batchsize: 16 | Steps: 633\n[GPU 0] Epoch 5 |                 Batchsize: 16 | Steps: 633\nEpoch 5 | Training checkpoint saved at checkpoint.pt\n[GPU 0] Epoch 6 |                 Batchsize: 16 | Steps: 633\n[GPU 0] Epoch 7 |                 Batchsize: 16 | Steps: 633\n[GPU 0] Epoch 8 |                 Batchsize: 16 | Steps: 633\n[GPU 0] Epoch 9 |                 Batchsize: 16 | Steps: 633\n[GPU 0] Epoch 10 |                 Batchsize: 16 | Steps: 633\nEpoch 10 | Training checkpoint saved at checkpoint.pt\n[GPU 0] Epoch 11 |                 Batchsize: 16 | Steps: 633\n[GPU 0] Epoch 12 |                 Batchsize: 16 | Steps: 633\n[GPU 0] Epoch 13 |                 Batchsize: 16 | Steps: 633\n[GPU 0] Epoch 14 |                 Batchsize: 16 | Steps: 633\n[GPU 0] Epoch 15 |                 Batchsize: 16 | Steps: 633\nEpoch 15 | Training checkpoint saved at checkpoint.pt\n[GPU 0] Epoch 16 |                 Batchsize: 16 | Steps: 633\n[GPU 0] Epoch 17 |                 Batchsize: 16 | Steps: 633\n[GPU 0] Epoch 18 |                 Batchsize: 16 | Steps: 633\n[GPU 0] Epoch 19 |                 Batchsize: 16 | Steps: 633\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"%%writefile torchrun_multigpu.py\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport torch.multiprocessing as mp\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.distributed import init_process_group, destroy_process_group\nimport os\n\ndef ddp_setup():\n    \"\"\"\n    rank: unique id of eeach process\n    world_size: no of gpus or devices\n    \"\"\"\n    \n    torch.cuda.set_device(int(os.environ[\"LOCAL_RANK\"]))\n    init_process_group(backend=\"nccl\")\n\nclass MyTrainDataset(Dataset):\n    def __init__(self, size):\n        self.size = size\n        self.data = [(torch.rand(10), torch.rand(1)) for _ in range(size)]\n\n    def __len__(self):\n        return self.size\n    \n    def __getitem__(self, index):\n        return self.data[index]\n\nclass Trainer:\n    def __init__(self, model: torch.nn.Module,\n        train_data: DataLoader,\n        optimizer: torch.optim.Optimizer,\n        save_every: int,\n        snapshot_path) -> None:\n\n        self.gpu_id = int(os.environ[\"LOCAL_RANK\"])\n        self.model = model.to(self.gpu_id)\n        self.optimizer = optimizer\n        self.train_data = train_data\n        self.save_every = save_every\n        \n        self.snapshot_path = snapshot_path\n\n        if os.path.exists(snapshot_path):\n            print(\"Loading snapshot\")\n            self._load_snapshot(snapshot_path)\n\n        self.model = DDP(model, device_ids = [self.gpu_id])\n\n    \n    def _load_snapshot(self, snapshot_path):\n        device_loc = f\"cuda:{self.gpu_id}\"\n        snapshot = torch.load(snapshot_path, map_location=device_loc)\n        self.model.load_state_dict(snapshot[\"MODEL_STATE\"])\n        self.epochs_run = snapshot[\"EPOCHS_RUN\"]\n        print(f\"Resuming training from snapshot at Epoch {self.epochs_run}\")\n\n        \n    def _save_snapshot(self, epoch):\n        snapshot = {\n            \"MODEL_STATE\": self.model.module.state_dict(),\n            \"EPOCHS_RUN\": epoch,\n        }\n        torch.save(snapshot, self.snapshot_path)\n        print(f\"Epoch {epoch} | Training snapshot saved at {self.snapshot_path}\")\n\n    def _run_batch(self, source, targets):\n        self.optimizer.zero_grad()\n        output = self.model(source)\n        loss = F.cross_entropy(output, targets)\n        loss.backward()\n        self.optimizer.step()\n\n\n    def _run_epoch(self, epoch):\n        batch_size = len(next(iter(self.train_data))[0])\n        print(f\"[GPU {self.gpu_id}] Epoch {epoch} | \\\n                Batchsize: {batch_size} | Steps: {len(self.train_data)}\")\n\n        self.train_data.sampler.set_epoch(epoch)\n        for source, targets in self.train_data:\n            source = source.to(self.gpu_id)\n            targets = targets.to(self.gpu_id)\n            self._run_batch(source, targets)\n\n    def _save_checkpoint(self, epoch):\n        check_point = self.model.state_dict()\n        path = \"checkpoint.pt\"\n        torch.save(check_point, path)\n        print(f\"Epoch {epoch} | Training checkpoint saved at {path}\")\n\n    def train(self, max_epochs):\n        for epoch in range(max_epochs):\n            self._run_epoch(epoch)\n            if epoch % self.save_every == 0:\n                self._save_checkpoint(epoch)\n        \n\ndef load_train_objs():\n    train_set = MyTrainDataset(20248)\n    model = torch.nn.Linear(10, 1)\n    optimizer = torch.optim.SGD(model.parameters(), lr = 1e-3)\n    return train_set, model, optimizer\n\ndef prepare_dataloader(dataset, batch_size):\n    return DataLoader(dataset, batch_size=batch_size, pin_memory=True, shuffle=False, sampler = DistributedSampler(dataset))\n\ndef main(total_epochs, save_every, batch_size, snapshot_path = \"snapshot.pt\"):\n\n    ddp_setup()\n    dataset, model, optimizer = load_train_objs()\n    train_data = prepare_dataloader(dataset, batch_size)\n    trainer = Trainer(model, train_data, optimizer, save_every, snapshot_path)\n    trainer.train(total_epochs)\n\nif __name__ == \"__main__\":\n    main(20, 5, 16)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T11:39:45.174555Z","iopub.execute_input":"2025-03-20T11:39:45.174862Z","iopub.status.idle":"2025-03-20T11:39:45.180545Z","shell.execute_reply.started":"2025-03-20T11:39:45.174837Z","shell.execute_reply":"2025-03-20T11:39:45.179708Z"}},"outputs":[{"name":"stdout","text":"Overwriting torchrun_multigpu.py\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"%%script bash\ntorchrun --standalone --nproc_per_node=gpu torchrun_multigpu.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T11:39:49.758304Z","iopub.execute_input":"2025-03-20T11:39:49.758619Z","iopub.status.idle":"2025-03-20T11:40:13.407908Z","shell.execute_reply.started":"2025-03-20T11:39:49.758592Z","shell.execute_reply":"2025-03-20T11:40:13.406961Z"}},"outputs":[{"name":"stdout","text":"[GPU 1] Epoch 0 |                 Batchsize: 16 | Steps: 633[GPU 0] Epoch 0 |                 Batchsize: 16 | Steps: 633\n\nEpoch 0 | Training checkpoint saved at checkpoint.pt\nEpoch 0 | Training checkpoint saved at checkpoint.pt\n[GPU 1] Epoch 1 |                 Batchsize: 16 | Steps: 633[GPU 0] Epoch 1 |                 Batchsize: 16 | Steps: 633\n\n[GPU 1] Epoch 2 |                 Batchsize: 16 | Steps: 633\n[GPU 0] Epoch 2 |                 Batchsize: 16 | Steps: 633\n[GPU 1] Epoch 3 |                 Batchsize: 16 | Steps: 633\n[GPU 0] Epoch 3 |                 Batchsize: 16 | Steps: 633\n[GPU 1] Epoch 4 |                 Batchsize: 16 | Steps: 633\n[GPU 0] Epoch 4 |                 Batchsize: 16 | Steps: 633\n[GPU 1] Epoch 5 |                 Batchsize: 16 | Steps: 633\n[GPU 0] Epoch 5 |                 Batchsize: 16 | Steps: 633\nEpoch 5 | Training checkpoint saved at checkpoint.ptEpoch 5 | Training checkpoint saved at checkpoint.pt\n\n[GPU 1] Epoch 6 |                 Batchsize: 16 | Steps: 633\n[GPU 0] Epoch 6 |                 Batchsize: 16 | Steps: 633\n[GPU 1] Epoch 7 |                 Batchsize: 16 | Steps: 633\n[GPU 0] Epoch 7 |                 Batchsize: 16 | Steps: 633\n[GPU 1] Epoch 8 |                 Batchsize: 16 | Steps: 633\n[GPU 0] Epoch 8 |                 Batchsize: 16 | Steps: 633\n[GPU 1] Epoch 9 |                 Batchsize: 16 | Steps: 633\n[GPU 0] Epoch 9 |                 Batchsize: 16 | Steps: 633\n[GPU 1] Epoch 10 |                 Batchsize: 16 | Steps: 633\n[GPU 0] Epoch 10 |                 Batchsize: 16 | Steps: 633\nEpoch 10 | Training checkpoint saved at checkpoint.pt\nEpoch 10 | Training checkpoint saved at checkpoint.pt\n[GPU 0] Epoch 11 |                 Batchsize: 16 | Steps: 633\n[GPU 1] Epoch 11 |                 Batchsize: 16 | Steps: 633\n[GPU 1] Epoch 12 |                 Batchsize: 16 | Steps: 633\n[GPU 0] Epoch 12 |                 Batchsize: 16 | Steps: 633\n[GPU 1] Epoch 13 |                 Batchsize: 16 | Steps: 633\n[GPU 0] Epoch 13 |                 Batchsize: 16 | Steps: 633\n[GPU 0] Epoch 14 |                 Batchsize: 16 | Steps: 633\n[GPU 1] Epoch 14 |                 Batchsize: 16 | Steps: 633\n[GPU 1] Epoch 15 |                 Batchsize: 16 | Steps: 633\n[GPU 0] Epoch 15 |                 Batchsize: 16 | Steps: 633\nEpoch 15 | Training checkpoint saved at checkpoint.ptEpoch 15 | Training checkpoint saved at checkpoint.pt\n\n[GPU 1] Epoch 16 |                 Batchsize: 16 | Steps: 633\n[GPU 0] Epoch 16 |                 Batchsize: 16 | Steps: 633\n[GPU 0] Epoch 17 |                 Batchsize: 16 | Steps: 633\n[GPU 1] Epoch 17 |                 Batchsize: 16 | Steps: 633\n[GPU 1] Epoch 18 |                 Batchsize: 16 | Steps: 633\n[GPU 0] Epoch 18 |                 Batchsize: 16 | Steps: 633\n[GPU 0] Epoch 19 |                 Batchsize: 16 | Steps: 633\n[GPU 1] Epoch 19 |                 Batchsize: 16 | Steps: 633\n","output_type":"stream"},{"name":"stderr","text":"W0320 11:39:51.439000 307 torch/distributed/run.py:793] \nW0320 11:39:51.439000 307 torch/distributed/run.py:793] *****************************************\nW0320 11:39:51.439000 307 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0320 11:39:51.439000 307 torch/distributed/run.py:793] *****************************************\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}